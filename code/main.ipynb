{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TIF360 Project"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main source: https://www.kaggle.com/code/rmonge/predicting-molecule-properties-based-on-its-smiles/notebook"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import rdkit\n",
    "from rdkit import Chem  # To extract information of the molecules\n",
    "from rdkit.Chem import Draw  # To draw the molecules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch_geometric\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch_geometric.utils as utils\n",
    "import networkx as nx\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import global_mean_pool, GraphConv, GATConv, GCNConv\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/smiles_and_targets.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mol_descriptor = np.load(\"../data/mol_descriptors.npy\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigate Neural Networks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Neural Networks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert data to graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "from rdkit.Chem import GetAdjacencyMatrix\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch.utils.data import DataLoader\n",
    "from rdkit.Chem import Descriptors\n",
    "from rdkit.ML.Descriptors import MoleculeDescriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(x, permitted_list):\n",
    "    if x not in permitted_list:\n",
    "        x = permitted_list[-1]  # If the atom is not in the list, get \"Unknown\"\n",
    "        \n",
    "    binary_encoding = [int(boolean) for boolean in list(map(lambda s: x==s, permitted_list))]\n",
    "    \n",
    "    return binary_encoding    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Atom featurisation\\\n",
    "Currently generates ca. 80 node features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_atom_features(atom, use_chirality = True, hydrogens_implicit = True):\n",
    "    # list of permitted atoms\n",
    "    permitted_atom_list = ['C','N','O','S','F','Si','P','Cl','Br','Mg','Na','Ca',\n",
    "                           'Fe','As','Al','I', 'B','V','K','Tl','Yb','Sb','Sn','Ag','Pd','Co',\n",
    "                           'Se','Ti','Zn', 'Li','Ge','Cu','Au','Ni','Cd','In','Mn','Zr','Cr','Pt',\n",
    "                           'Hg','Pb','Unknown']\n",
    "    \n",
    "    atom_type_enc = one_hot_encoding(str(atom.GetSymbol()), permitted_atom_list)\n",
    "    \n",
    "    n_heavy_neighbors = one_hot_encoding(int(atom.GetDegree()), [0,1,2,3,4,\"MoreThanFour\"])\n",
    "    \n",
    "    formal_charge_enc = one_hot_encoding(int(atom.GetFormalCharge()), [-3, -2, -1, 0, 1, 2, 3, 'Extreme'])\n",
    "    \n",
    "    hybridisation_type_enc = one_hot_encoding(str(atom.GetHybridization()), [\"S\", \"SP\", \"SP2\", \"SP3\", \"SP3D\", \"SP3D2\", \"OTHER\"])\n",
    "    \n",
    "    is_in_ring_enc = one_hot_encoding(int(atom.IsInRing()), [0, 1])\n",
    "    \n",
    "    is_aromatic_enc = one_hot_encoding(int(atom.GetIsAromatic()), [0, 1])\n",
    "    \n",
    "    atomic_mass_scaled = [float(atom.GetMass() - 10.812)/116.092] # (?) replace 10.812 with mean the and 116.092 with std\n",
    "    \n",
    "    vdw_radius_scaled = [float((Chem.GetPeriodicTable().GetRvdw(atom.GetAtomicNum()) - 1.5)/0.6)] # (?) replace 1.5 with mean the and 0.6 with std\n",
    "    \n",
    "    covalent_radius_scaled = [float((Chem.GetPeriodicTable().GetRcovalent(atom.GetAtomicNum()) - 0.64)/0.76)] # (?) replace 0.64 with mean the and 0.76 with std\n",
    "                              \n",
    "    atom_feature_vector = atom_type_enc + n_heavy_neighbors + formal_charge_enc + hybridisation_type_enc + is_in_ring_enc + is_aromatic_enc + atomic_mass_scaled + vdw_radius_scaled + covalent_radius_scaled\n",
    "    \n",
    "    if use_chirality:\n",
    "        chirality_type_enc = one_hot_encoding(str(atom.GetChiralTag()), [\"CHI_UNSPECIFIED\", \"CHI_TETRAHEDRAL_CW\", \"CHI_TETRAHEDRAL_CCW\", \"CHI_OTHER\"])\n",
    "        atom_feature_vector += chirality_type_enc\n",
    "        \n",
    "    if hydrogens_implicit:\n",
    "        n_hydrogens_enc = one_hot_encoding(int(atom.GetTotalNumHs()), [0, 1, 2, 3, 4, \"MoreThanFour\"])\n",
    "        atom_feature_vector += n_hydrogens_enc\n",
    "        \n",
    "    return np.array(atom_feature_vector) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bond Featurisation\\\n",
    "Currently generates ca. 10 edge features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bond_features(bond, use_stereochemistry=True):\n",
    "    permitted_bond_types = [Chem.rdchem.BondType.SINGLE, Chem.rdchem.BondType.DOUBLE, \n",
    "                            Chem.rdchem.BondType.TRIPLE, Chem.rdchem.BondType.AROMATIC]\n",
    "    \n",
    "    bond_type_enc = one_hot_encoding(str(bond.GetBondType()), permitted_bond_types)\n",
    "    \n",
    "    bond_is_conjugated_enc = [int(bond.GetIsConjugated())]\n",
    "    \n",
    "    bond_is_in_ring_enc = [int(bond.IsInRing())]\n",
    "    \n",
    "    bond_feature_vector = bond_type_enc + bond_is_conjugated_enc + bond_is_in_ring_enc\n",
    "    \n",
    "    if use_stereochemistry:\n",
    "        stereo_type_enc = one_hot_encoding(str(bond.GetStereo()), [\"STEREOZ\", \"STEREOE\", \"STEREOANY\", \"STEREONONE\"])\n",
    "        bond_feature_vector += stereo_type_enc\n",
    "        \n",
    "    return np.array(bond_feature_vector)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function to generate dataset of labeled Pytorch Geometric Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph_dataset_from_smiles(x_smiles, y):\n",
    "    ## Inputs:\n",
    "    # x_smiles = [smiles_1, smiles_2, ...], smiles representation of molecules\n",
    "    # y = [y_1, y_2, ...] list of numerical labels for each smiles string, here chemical properties\n",
    "    \n",
    "    # Outputs:\n",
    "    # dataset = [data_1, data_2, ...] list of torch_geometric.data.Data objects representing molecular graphs\n",
    "    \n",
    "    dataset = []\n",
    "    \n",
    "    for (smiles, y_val) in zip(x_smiles, y):\n",
    "        # convert smiles to molecular object\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        \n",
    "        # get feature dimensions\n",
    "        n_nodes = mol.GetNumAtoms()\n",
    "        n_edges = 2*mol.GetNumBonds() # each bond is represented twice in the adjacency matrix\n",
    "        n_node_features = len(get_atom_features(mol.GetAtomWithIdx(0)))\n",
    "        if n_nodes > 1:\n",
    "            n_edge_features = len(get_bond_features(mol.GetBondBetweenAtoms(0,1)))\n",
    "        else:\n",
    "            n_edge_features = 0  # for single atom molecules -> no edges\n",
    "        \n",
    "        # construct node feature matrix X \n",
    "        X = np.zeros((n_nodes, n_node_features))\n",
    "        \n",
    "        for atom in mol.GetAtoms():\n",
    "            X[atom.GetIdx(), :] = get_atom_features(atom)\n",
    "        \n",
    "        X = torch.tensor(X, dtype=torch.float)\n",
    "        \n",
    "        # construct edge index array E, shape = (2, n_edges)\n",
    "        (rows, cols) = np.nonzero(GetAdjacencyMatrix(mol))\n",
    "        torch_rows = torch.tensor(rows.astype(np.int64)).to(torch.long)\n",
    "        torch_cols = torch.tensor(cols.astype(np.int64)).to(torch.long)\n",
    "        E = torch.stack([torch_rows, torch_cols], dim=0)\n",
    "        \n",
    "        # construct edge feature matrix EF\n",
    "        EF = np.zeros((n_edges, n_edge_features))       # Note: generates zero matrix if n_edges = n_edge_features = 0\n",
    "        for (k, (i,j)) in enumerate(zip(rows, cols)):\n",
    "            EF[k] = get_bond_features(mol.GetBondBetweenAtoms(int(i),int(j)))\n",
    "        EF = torch.tensor(EF, dtype=torch.float)\n",
    "        \n",
    "        # construct label/y tensor\n",
    "        y_tensor = torch.tensor(np.array([y_val]), dtype=torch.float)\n",
    "        \n",
    "        # construct torch_geometric.data.Data object and append to dataset\n",
    "        dataset.append(Data(x=X, edge_index=E, edge_attr=EF, y=y_tensor))\n",
    "        \n",
    "    return dataset\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the above functions to create a dataset of molecular graphs from the smiles and labels corresponding to chemical properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties_names = ['A', 'B', 'C', 'mu', 'alfa', 'homo', 'lumo', 'gap', 'R²', 'zpve', 'U0', 'U', 'H', 'G', 'Cv']\n",
    "\n",
    "x_smiles = df.smiles.values\n",
    "y = df.loc[:, properties_names].values  # shape = (n_samples, n_properties)\n",
    "\n",
    "dataset = create_graph_dataset_from_smiles(x_smiles, y[0:len(x_smiles), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133796\n",
      "C\n",
      "Data(x=[1, 81], edge_index=[2, 0], edge_attr=[0, 0], y=[1, 15])\n",
      "O\n",
      "Data(x=[1, 81], edge_index=[2, 0], edge_attr=[0, 0], y=[1, 15])\n",
      "N1C=CN=C1\n",
      "Data(x=[5, 81], edge_index=[2, 10], edge_attr=[10, 10], y=[1, 15])\n",
      "tensor([[0., 0., 0., 1., 1., 1., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 1., 1., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 1., 1., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 1., 1., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 1., 1., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 1., 1., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 1., 1., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 1., 1., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 1., 1., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 1., 1., 0., 0., 0., 1.]])\n",
      "tensor([[0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0275, 0.1667, 0.0526, 1.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0103, 0.3333, 0.0526, 1.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0103, 0.3333, 0.0526, 1.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0275, 0.1667, 0.0526, 1.0000,\n",
      "         0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0103, 0.3333, 0.0526, 1.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))\n",
    "\n",
    "# Example entries\n",
    "print(df.smiles.values[0])\n",
    "print(dataset[0])\n",
    "print(df.smiles.values[2])\n",
    "print(dataset[2])\n",
    "print(df.smiles.values[50])\n",
    "print(dataset[50])\n",
    "\n",
    "print(dataset[50].edge_attr)\n",
    "print(dataset[50].x)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information of the graph dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of graphs (molecules): 133796\n",
      "=================================================================================\n",
      "Properties of graph 50, molecule smiles: N1C=CN=C1\n",
      "Number of nodes: 5\n",
      "Number of edges: 10\n",
      "Number of node features: 81\n",
      "Number of edge features: 10\n",
      "Number of properties: 15\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of graphs (molecules): {len(dataset)}')\n",
    "graph = dataset[50]\n",
    "print('=================================================================================')\n",
    "print(f'Properties of graph {50}, molecule smiles: {df.smiles.values[50]}')\n",
    "print(f'Number of nodes: {dataset[50].x.shape[0]}')\n",
    "print(f'Number of edges: {dataset[50].edge_index.shape[1]}')\n",
    "print(f'Number of node features: {dataset[50].x.shape[1]}')\n",
    "print(f'Number of edge features: {dataset[50].edge_attr.shape[1]}')\n",
    "print(f'Number of properties: {dataset[50].y.shape[1]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filterout data with no edge features defined (Like ex: CH4) (These causes problems down the line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of none edge feature molecules:  3\n",
      "Before:  133796\n",
      "Molecule to delete:  C\n",
      "Molecule to delete:  N\n",
      "Molecule to delete:  O\n",
      "After:  133793\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "indexes_to_delete = []\n",
    "for item in range(0,len(dataset)):\n",
    "    if dataset[item].edge_attr.shape[1] == 0:\n",
    "        indexes_to_delete.append(item)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "indexes_to_delete.sort()\n",
    "\n",
    "print(\"Number of none edge feature molecules: \", len(indexes_to_delete))\n",
    "\n",
    "print(\"Before: \", len(dataset))\n",
    "\n",
    "for item in range(0,len(indexes_to_delete)):\n",
    "    print(\"Molecule to delete: \", df.smiles.values[indexes_to_delete[item]])\n",
    "    #del dataset[indexes_to_delete[item] - item] \n",
    "    dataset.pop((indexes_to_delete[item] - item)) # -item since all future data points will have its index reduced by 1 for each deleted previous data point\n",
    "print(\"After: \", len(dataset))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133793\n",
      "107035\n",
      "26758\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "# split the dataset into test and validation:\n",
    "num_samples = len(dataset)\n",
    "\n",
    "# Want to divide data randomly\n",
    "random_indexes = np.array(random.sample(range(num_samples), num_samples)) # random.sample ensures no duplicates\n",
    "\n",
    "train_data = [dataset[index] for index in random_indexes[int(.2 * num_samples ) :]] # 80%\n",
    "test_data = [dataset[index] for index in random_indexes[: int(.2 * num_samples )]] # 20%\n",
    "\n",
    "print(num_samples)\n",
    "print(len(train_data))\n",
    "print(len(test_data))\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main GNN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model for all targets at once"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GNN function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_labels = dataset[50].y.shape[1]\n",
    "data_features = dataset[50].x.shape[1]\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GATConv(data_features, hidden_channels)\n",
    "        self.conv2 = GATConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GATConv(hidden_channels, hidden_channels)\n",
    "        self.conv4 = GATConv(hidden_channels, hidden_channels)\n",
    "        self.conv5 = GATConv(hidden_channels, hidden_channels)\n",
    "        self.lin = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin2 = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin3 = Linear(hidden_channels, data_labels)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch): \n",
    "        x = self.conv1(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv4(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv5(x, edge_index, edge_attr)\n",
    "\n",
    "        #Returns batch-wise graph-level-outputs by averaging node features across the node dimension, so that for a single graph G\n",
    "        #its output is computed by\n",
    "        x = global_mean_pool(x, batch) \n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = F.relu(x)\n",
    "        x = self.lin2(x)\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = F.relu(x)\n",
    "        x = self.lin3(x)\n",
    "        \n",
    " \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 52\u001b[0m\n\u001b[0;32m     49\u001b[0m Test_r2 \u001b[39m=\u001b[39m []\n\u001b[0;32m     51\u001b[0m \u001b[39m# Calculate accuracy before training \u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m Train_r2\u001b[39m.\u001b[39mappend(test(train_loader))\n\u001b[0;32m     53\u001b[0m Test_r2\u001b[39m.\u001b[39mappend(test(test_loader))\n\u001b[0;32m     54\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mInitial training R2: \u001b[39m\u001b[39m\"\u001b[39m, Train_r2[\u001b[39m0\u001b[39m])\n",
      "Cell \u001b[1;32mIn[15], line 25\u001b[0m, in \u001b[0;36mtest\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m     23\u001b[0m all_test_r2 \u001b[39m=\u001b[39m []\n\u001b[0;32m     24\u001b[0m counter \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m    \n\u001b[1;32m---> 25\u001b[0m \u001b[39mfor\u001b[39;00m data_in \u001b[39min\u001b[39;00m data:\n\u001b[0;32m     26\u001b[0m       counter \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     27\u001b[0m       model\u001b[39m.\u001b[39meval()\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    677\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 678\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    679\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    680\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch_geometric\\loader\\dataloader.py:20\u001b[0m, in \u001b[0;36mCollater.__call__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m     18\u001b[0m elem \u001b[39m=\u001b[39m batch[\u001b[39m0\u001b[39m]\n\u001b[0;32m     19\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, BaseData):\n\u001b[1;32m---> 20\u001b[0m     \u001b[39mreturn\u001b[39;00m Batch\u001b[39m.\u001b[39;49mfrom_data_list(batch, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfollow_batch,\n\u001b[0;32m     21\u001b[0m                                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexclude_keys)\n\u001b[0;32m     22\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m     23\u001b[0m     \u001b[39mreturn\u001b[39;00m default_collate(batch)\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch_geometric\\data\\batch.py:76\u001b[0m, in \u001b[0;36mBatch.from_data_list\u001b[1;34m(cls, data_list, follow_batch, exclude_keys)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m     65\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_data_list\u001b[39m(\u001b[39mcls\u001b[39m, data_list: List[BaseData],\n\u001b[0;32m     66\u001b[0m                    follow_batch: Optional[List[\u001b[39mstr\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     67\u001b[0m                    exclude_keys: Optional[List[\u001b[39mstr\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m     68\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"Constructs a :class:`~torch_geometric.data.Batch` object from a\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[39m    Python list of :class:`~torch_geometric.data.Data` or\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[39m    :class:`~torch_geometric.data.HeteroData` objects.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[39m    :obj:`follow_batch`.\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[39m    Will exclude any keys given in :obj:`exclude_keys`.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 76\u001b[0m     batch, slice_dict, inc_dict \u001b[39m=\u001b[39m collate(\n\u001b[0;32m     77\u001b[0m         \u001b[39mcls\u001b[39;49m,\n\u001b[0;32m     78\u001b[0m         data_list\u001b[39m=\u001b[39;49mdata_list,\n\u001b[0;32m     79\u001b[0m         increment\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m     80\u001b[0m         add_batch\u001b[39m=\u001b[39;49m\u001b[39mnot\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(data_list[\u001b[39m0\u001b[39;49m], Batch),\n\u001b[0;32m     81\u001b[0m         follow_batch\u001b[39m=\u001b[39;49mfollow_batch,\n\u001b[0;32m     82\u001b[0m         exclude_keys\u001b[39m=\u001b[39;49mexclude_keys,\n\u001b[0;32m     83\u001b[0m     )\n\u001b[0;32m     85\u001b[0m     batch\u001b[39m.\u001b[39m_num_graphs \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(data_list)\n\u001b[0;32m     86\u001b[0m     batch\u001b[39m.\u001b[39m_slice_dict \u001b[39m=\u001b[39m slice_dict\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch_geometric\\data\\collate.py:85\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(cls, data_list, increment, add_batch, follow_batch, exclude_keys)\u001b[0m\n\u001b[0;32m     82\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[39m# Collate attributes into a unified representation:\u001b[39;00m\n\u001b[1;32m---> 85\u001b[0m value, slices, incs \u001b[39m=\u001b[39m _collate(attr, values, data_list, stores,\n\u001b[0;32m     86\u001b[0m                                increment)\n\u001b[0;32m     88\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(value, Tensor) \u001b[39mand\u001b[39;00m value\u001b[39m.\u001b[39mis_cuda:\n\u001b[0;32m     89\u001b[0m     device \u001b[39m=\u001b[39m value\u001b[39m.\u001b[39mdevice\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch_geometric\\data\\collate.py:134\u001b[0m, in \u001b[0;36m_collate\u001b[1;34m(key, values, data_list, stores, increment)\u001b[0m\n\u001b[0;32m    132\u001b[0m slices \u001b[39m=\u001b[39m cumsum([value\u001b[39m.\u001b[39msize(cat_dim \u001b[39mor\u001b[39;00m \u001b[39m0\u001b[39m) \u001b[39mfor\u001b[39;00m value \u001b[39min\u001b[39;00m values])\n\u001b[0;32m    133\u001b[0m \u001b[39mif\u001b[39;00m increment:\n\u001b[1;32m--> 134\u001b[0m     incs \u001b[39m=\u001b[39m get_incs(key, values, data_list, stores)\n\u001b[0;32m    135\u001b[0m     \u001b[39mif\u001b[39;00m incs\u001b[39m.\u001b[39mdim() \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mint\u001b[39m(incs[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]) \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    136\u001b[0m         values \u001b[39m=\u001b[39m [\n\u001b[0;32m    137\u001b[0m             value \u001b[39m+\u001b[39m inc\u001b[39m.\u001b[39mto(value\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m    138\u001b[0m             \u001b[39mfor\u001b[39;00m value, inc \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(values, incs)\n\u001b[0;32m    139\u001b[0m         ]\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch_geometric\\data\\collate.py:275\u001b[0m, in \u001b[0;36mget_incs\u001b[1;34m(key, values, data_list, stores)\u001b[0m\n\u001b[0;32m    273\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    274\u001b[0m     repeats \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(repeats)\n\u001b[1;32m--> 275\u001b[0m \u001b[39mreturn\u001b[39;00m cumsum(repeats[:\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m])\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch_geometric\\data\\collate.py:261\u001b[0m, in \u001b[0;36mcumsum\u001b[1;34m(value)\u001b[0m\n\u001b[0;32m    259\u001b[0m out \u001b[39m=\u001b[39m value\u001b[39m.\u001b[39mnew_empty((value\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m) \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, ) \u001b[39m+\u001b[39m value\u001b[39m.\u001b[39msize()[\u001b[39m1\u001b[39m:])\n\u001b[0;32m    260\u001b[0m out[\u001b[39m0\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m--> 261\u001b[0m torch\u001b[39m.\u001b[39;49mcumsum(value, \u001b[39m0\u001b[39;49m, out\u001b[39m=\u001b[39;49mout[\u001b[39m1\u001b[39;49m:])\n\u001b[0;32m    262\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = GNN(hidden_channels=256) \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "def train(data_in):\n",
    "      model.train()\n",
    "      optimizer.zero_grad()  # Clear gradients.\n",
    "      out = model(data_in.x, data_in.edge_index, data_in.edge_attr, data_in.batch)  # Perform a single forward pass.\n",
    "\n",
    "      #Alt 1\n",
    "      loss = criterion(out, data_in.y) \n",
    "\n",
    "      #Alt 2\n",
    "      #loss = 0\n",
    "      #for item in range(0,len(data_in.y[0,:])):\n",
    "      #      loss += criterion(out[:,item], data_in.y[:,item]) \n",
    "\n",
    "      loss.backward()  # Derive gradients.\n",
    "      optimizer.step()  # Update parameters based on gradients.\n",
    "      return loss\n",
    "\n",
    "def test(data):\n",
    "      all_test_r2 = []\n",
    "      counter = -1    \n",
    "      for data_in in data:\n",
    "            counter += 1\n",
    "            model.eval()\n",
    "            out = model(data_in.x, data_in.edge_index, data_in.edge_attr, data_in.batch)\n",
    "\n",
    "            # Caculate R2\n",
    "            r2_score_var = []\n",
    "            for item in range(0,data_in.y.shape[1]):\n",
    "                  if item == 0:\n",
    "                        r2_score_var = r2_score(data_in.y[:,item].detach().numpy(), out[:,item].detach().numpy())\n",
    "                  else:\n",
    "                        r2_score_var = np.vstack((r2_score_var,(r2_score(data_in.y[:,item].detach().numpy(), out[:,item].detach().numpy()))))\n",
    "\n",
    "            if counter == 0:\n",
    "                  all_test_r2 = r2_score_var\n",
    "            else:\n",
    "                  all_test_r2 = np.hstack((all_test_r2,r2_score_var))\n",
    "\n",
    "      average_test_r2 = np.sum(all_test_r2,axis=1) / all_test_r2.shape[1]\n",
    "\n",
    "      return average_test_r2\n",
    "\n",
    "# Vectors to append accuracy to:\n",
    "Train_r2 = []\n",
    "Test_r2 = []\n",
    "\n",
    "# Calculate accuracy before training \n",
    "Train_r2.append(test(train_loader))\n",
    "Test_r2.append(test(test_loader))\n",
    "print(\"Initial training R2: \", Train_r2[0])\n",
    "print(\"Initial test R2: \", Test_r2[0])\n",
    "\n",
    "print_r2_option = True\n",
    "for epoch in range(1, 101):\n",
    "      average_loss = []\n",
    "      for data in train_loader:\n",
    "            loss = train(data)\n",
    "            average_loss.append(loss)\n",
    "      print(f'Epoch: {epoch:03d}, Loss: {(sum(average_loss)/len(average_loss)):.4f}')\n",
    "\n",
    "      if print_r2_option:\n",
    "\n",
    "            temp_train_r2 = test(train_loader)\n",
    "            Train_r2.append(temp_train_r2)\n",
    "\n",
    "            temp_test_r2 = test(test_loader)\n",
    "            Test_r2.append(temp_test_r2)\n",
    "\n",
    "            print(f'Average Train R2: {temp_train_r2}')\n",
    "            print(f'Average Test R2: {temp_test_r2:}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model for just one target"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GNN function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_labels = 1\n",
    "data_features = dataset[50].x.shape[1]\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GATConv(data_features, hidden_channels)\n",
    "        self.conv2 = GATConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GATConv(hidden_channels, hidden_channels)\n",
    "        self.conv4 = GATConv(hidden_channels, hidden_channels)\n",
    "        self.conv5 = GATConv(hidden_channels, hidden_channels)\n",
    "        self.lin = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin2 = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin3 = Linear(hidden_channels, data_labels)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch): \n",
    "        x = self.conv1(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv4(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv5(x, edge_index, edge_attr)\n",
    "\n",
    "        #Returns batch-wise graph-level-outputs by averaging node features across the node dimension, so that for a single graph G\n",
    "        #its output is computed by\n",
    "        x = global_mean_pool(x, batch) \n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = F.relu(x)\n",
    "        x = self.lin2(x)\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = F.relu(x)\n",
    "        x = self.lin3(x)\n",
    "        \n",
    " \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial training R2:  -7.49786089020006\n",
      "Initial test R2:  -7.329587586502999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([27])) that is different to the input size (torch.Size([27, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 5.9924\n",
      "Average Train R2: 0.0028777230272099418\n",
      "Average Test R2: 0.002441659361192309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([27])) that is different to the input size (torch.Size([27, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 002, Loss: 5.1882\n",
      "Average Train R2: -0.0045602759162772614\n",
      "Average Test R2: -0.006537056023025651\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([27])) that is different to the input size (torch.Size([27, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 003, Loss: 5.1688\n",
      "Average Train R2: -0.007242981300759531\n",
      "Average Test R2: -0.009304122220448203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([27])) that is different to the input size (torch.Size([27, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 004, Loss: 5.1586\n",
      "Average Train R2: -0.009144920442126211\n",
      "Average Test R2: -0.008319962919350802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([27])) that is different to the input size (torch.Size([27, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 005, Loss: 5.1475\n",
      "Average Train R2: -0.010778084401283363\n",
      "Average Test R2: -0.010981890703993701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([64])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 47\u001b[0m\n\u001b[0;32m     45\u001b[0m average_loss \u001b[39m=\u001b[39m []\n\u001b[0;32m     46\u001b[0m \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m train_loader:\n\u001b[1;32m---> 47\u001b[0m       loss \u001b[39m=\u001b[39m train(data)\n\u001b[0;32m     48\u001b[0m       average_loss\u001b[39m.\u001b[39mappend(loss)\n\u001b[0;32m     49\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m:\u001b[39;00m\u001b[39m03d\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Loss: \u001b[39m\u001b[39m{\u001b[39;00m(\u001b[39msum\u001b[39m(average_loss)\u001b[39m/\u001b[39m\u001b[39mlen\u001b[39m(average_loss))\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[49], line 13\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(data_in)\u001b[0m\n\u001b[0;32m      9\u001b[0m out \u001b[39m=\u001b[39m model(data_in\u001b[39m.\u001b[39mx, data_in\u001b[39m.\u001b[39medge_index, data_in\u001b[39m.\u001b[39medge_attr, data_in\u001b[39m.\u001b[39mbatch)  \u001b[39m# Perform a single forward pass.\u001b[39;00m\n\u001b[0;32m     11\u001b[0m loss \u001b[39m=\u001b[39m criterion(out, data_in\u001b[39m.\u001b[39my[:,target])  \n\u001b[1;32m---> 13\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()  \u001b[39m# Derive gradients.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m optimizer\u001b[39m.\u001b[39mstep()  \u001b[39m# Update parameters based on gradients.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = GNN(hidden_channels=64) \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "def train(data_in):\n",
    "      target = 0 # target index of interest\n",
    "      model.train()\n",
    "      optimizer.zero_grad()  # Clear gradients.\n",
    "      out = model(data_in.x, data_in.edge_index, data_in.edge_attr, data_in.batch)  # Perform a single forward pass.\n",
    "\n",
    "      loss = criterion(out, data_in.y[:,target])  \n",
    "\n",
    "      loss.backward()  # Derive gradients.\n",
    "      optimizer.step()  # Update parameters based on gradients.\n",
    "      return loss\n",
    "\n",
    "def test(data):\n",
    "      target = 0 # target index of interest\n",
    "      all_test_r2 = []  \n",
    "      for data_in in data:\n",
    "            model.eval()\n",
    "            out = model(data_in.x, data_in.edge_index, data_in.edge_attr, data_in.batch)\n",
    "\n",
    "            # Caculate R2\n",
    "            r2_score_var = r2_score(data_in.y[:,target].detach().numpy(), out.detach().numpy())\n",
    "\n",
    "            all_test_r2 .append(r2_score_var)\n",
    "\n",
    "      average_test_r2 = np.sum(all_test_r2) / len(all_test_r2)\n",
    "\n",
    "      return average_test_r2\n",
    "\n",
    "# Vectors to append accuracy to:\n",
    "Train_r2 = []\n",
    "Test_r2 = []\n",
    "\n",
    "# Calculate accuracy before training \n",
    "Train_r2.append(test(train_loader))\n",
    "Test_r2.append(test(test_loader))\n",
    "print(\"Initial training R2: \", Train_r2[0])\n",
    "print(\"Initial test R2: \", Test_r2[0])\n",
    "\n",
    "print_r2_option = True\n",
    "for epoch in range(1, 101):\n",
    "      average_loss = []\n",
    "      for data in train_loader:\n",
    "            loss = train(data)\n",
    "            average_loss.append(loss)\n",
    "      print(f'Epoch: {epoch:03d}, Loss: {(sum(average_loss)/len(average_loss)):.4f}')\n",
    "\n",
    "      if print_r2_option:\n",
    "\n",
    "            temp_train_r2 = test(train_loader)\n",
    "            Train_r2.append(temp_train_r2)\n",
    "\n",
    "            temp_test_r2 = test(test_loader)\n",
    "            Test_r2.append(temp_test_r2)\n",
    "\n",
    "            print(f'Average Train R2: {temp_train_r2}')\n",
    "            print(f'Average Test R2: {temp_test_r2:}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
