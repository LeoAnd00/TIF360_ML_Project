{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TIF360 Project"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main source: https://www.kaggle.com/code/rmonge/predicting-molecule-properties-based-on-its-smiles/notebook"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import rdkit\n",
    "from rdkit import Chem  # To extract information of the molecules\n",
    "from rdkit.Chem import Draw  # To draw the molecules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch_geometric.utils as utils\n",
    "import networkx as nx\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import global_mean_pool, GraphConv, GATConv, GCNConv\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/smiles_and_targets.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigate Neural Networks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Neural Networks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert data to graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "from rdkit.Chem import GetAdjacencyMatrix\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(x, permitted_list):\n",
    "    if x not in permitted_list:\n",
    "        x = permitted_list[-1]  # If the atom is not in the list, get \"Unknown\"\n",
    "        \n",
    "    binary_encoding = [int(boolean) for boolean in list(map(lambda s: x==s, permitted_list))]\n",
    "    \n",
    "    return binary_encoding    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Atom featurisation\\\n",
    "Currently generates ca. 80 node features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_atom_features(atom, use_chirality = True, hydrogens_implicit = True):\n",
    "    # list of permitted atoms\n",
    "    permitted_atom_list = ['C','N','O','S','F','Si','P','Cl','Br','Mg','Na','Ca',\n",
    "                           'Fe','As','Al','I', 'B','V','K','Tl','Yb','Sb','Sn','Ag','Pd','Co',\n",
    "                           'Se','Ti','Zn', 'Li','Ge','Cu','Au','Ni','Cd','In','Mn','Zr','Cr','Pt',\n",
    "                           'Hg','Pb','Unknown']\n",
    "    atom_type_enc = one_hot_encoding(str(atom.GetSymbol()), permitted_atom_list)\n",
    "    \n",
    "    n_heavy_neighbors = one_hot_encoding(int(atom.GetDegree()), [0,1,2,3,4,\"MoreThanFour\"])\n",
    "    \n",
    "    formal_charge_enc = one_hot_encoding(int(atom.GetFormalCharge()), [-3, -2, -1, 0, 1, 2, 3, 'Extreme'])\n",
    "    \n",
    "    hybridisation_type_enc = one_hot_encoding(str(atom.GetHybridization()), [\"S\", \"SP\", \"SP2\", \"SP3\", \"SP3D\", \"SP3D2\", \"OTHER\"])\n",
    "    \n",
    "    is_in_ring_enc = one_hot_encoding(int(atom.IsInRing()), [0, 1])\n",
    "    \n",
    "    is_aromatic_enc = one_hot_encoding(int(atom.GetIsAromatic()), [0, 1])\n",
    "    \n",
    "    atomic_mass_scaled = [float(atom.GetMass() - 10.812)/116.092] # (?) replace 10.812 with mean the and 116.092 with std\n",
    "    \n",
    "    vdw_radius_scaled = [float((Chem.GetPeriodicTable().GetRvdw(atom.GetAtomicNum()) - 1.5)/0.6)] # (?) replace 1.5 with mean the and 0.6 with std\n",
    "    \n",
    "    covalent_radius_scaled = [float((Chem.GetPeriodicTable().GetRcovalent(atom.GetAtomicNum()) - 0.64)/0.76)] # (?) replace 0.64 with mean the and 0.76 with std\n",
    "                              \n",
    "    atom_feature_vector = atom_type_enc + n_heavy_neighbors + formal_charge_enc + hybridisation_type_enc + is_in_ring_enc + is_aromatic_enc + atomic_mass_scaled + vdw_radius_scaled + covalent_radius_scaled\n",
    "    \n",
    "    if use_chirality:\n",
    "        chirality_type_enc = one_hot_encoding(str(atom.GetChiralTag()), [\"CHI_UNSPECIFIED\", \"CHI_TETRAHEDRAL_CW\", \"CHI_TETRAHEDRAL_CCW\", \"CHI_OTHER\"])\n",
    "        atom_feature_vector += chirality_type_enc\n",
    "        \n",
    "    if hydrogens_implicit:\n",
    "        n_hydrogens_enc = one_hot_encoding(int(atom.GetTotalNumHs()), [0, 1, 2, 3, 4, \"MoreThanFour\"])\n",
    "        atom_feature_vector += n_hydrogens_enc\n",
    "        \n",
    "    return np.array(atom_feature_vector) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bond Featurisation\\\n",
    "Currently generates ca. 10 edge features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bond_features(bond, use_stereochemistry=True):\n",
    "    permitted_bond_types = [Chem.rdchem.BondType.SINGLE, Chem.rdchem.BondType.DOUBLE, \n",
    "                            Chem.rdchem.BondType.TRIPLE, Chem.rdchem.BondType.AROMATIC]\n",
    "    \n",
    "    bond_type_enc = one_hot_encoding(str(bond.GetBondType()), permitted_bond_types)\n",
    "    \n",
    "    bond_is_conjugated_enc = [int(bond.GetIsConjugated())]\n",
    "    \n",
    "    bond_is_in_ring_enc = [int(bond.IsInRing())]\n",
    "    \n",
    "    bond_feature_vector = bond_type_enc + bond_is_conjugated_enc + bond_is_in_ring_enc\n",
    "    \n",
    "    if use_stereochemistry:\n",
    "        stereo_type_enc = one_hot_encoding(str(bond.GetStereo()), [\"STEREOZ\", \"STEREOE\", \"STEREOANY\", \"STEREONONE\"])\n",
    "        bond_feature_vector += stereo_type_enc\n",
    "        \n",
    "    return np.array(bond_feature_vector)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function to generate dataset of labeled Pytorch Geometric Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph_dataset_from_smiles(x_smiles, y):\n",
    "    ## Inputs:\n",
    "    # x_smiles = [smiles_1, smiles_2, ...], smiles representation of molecules\n",
    "    # y = [y_1, y_2, ...] list of numerical labels for each smiles string, here chemical properties\n",
    "    \n",
    "    # Outputs:\n",
    "    # dataset = [data_1, data_2, ...] list of torch_geometric.data.Data objects representing molecular graphs\n",
    "    \n",
    "    dataset = []\n",
    "    \n",
    "    for (smiles, y_val) in zip(x_smiles, y):\n",
    "        # convert smiles to molecular object\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        \n",
    "        # get feature dimensions\n",
    "        n_nodes = mol.GetNumAtoms()\n",
    "        n_edges = 2*mol.GetNumBonds() # each bond is represented twice in the adjacency matrix\n",
    "        n_node_features = len(get_atom_features(mol.GetAtomWithIdx(0)))\n",
    "        if n_nodes > 1:\n",
    "            n_edge_features = len(get_bond_features(mol.GetBondBetweenAtoms(0,1)))\n",
    "        else:\n",
    "            n_edge_features = 0  # for single atom molecules -> no edges\n",
    "        \n",
    "        # construct node feature matrix X \n",
    "        X = np.zeros((n_nodes, n_node_features))\n",
    "        \n",
    "        for atom in mol.GetAtoms():\n",
    "            X[atom.GetIdx(), :] = get_atom_features(atom)\n",
    "        \n",
    "        X = torch.tensor(X, dtype=torch.float)\n",
    "        \n",
    "        # construct edge index array E, shape = (2, n_edges)\n",
    "        (rows, cols) = np.nonzero(GetAdjacencyMatrix(mol))\n",
    "        torch_rows = torch.tensor(rows.astype(np.int64)).to(torch.long)\n",
    "        torch_cols = torch.tensor(cols.astype(np.int64)).to(torch.long)\n",
    "        E = torch.stack([torch_rows, torch_cols], dim=0)\n",
    "        \n",
    "        # construct edge feature matrix EF\n",
    "        EF = np.zeros((n_edges, n_edge_features))       # Note: generates zero matrix if n_edges = n_edge_features = 0\n",
    "        for (k, (i,j)) in enumerate(zip(rows, cols)):\n",
    "            EF[k] = get_bond_features(mol.GetBondBetweenAtoms(int(i),int(j)))\n",
    "        EF = torch.tensor(EF, dtype=torch.float)\n",
    "        \n",
    "        # construct label/y tensor\n",
    "        y_tensor = torch.tensor(np.array([y_val]), dtype=torch.float)\n",
    "        \n",
    "        # construct torch_geometric.data.Data object and append to dataset\n",
    "        dataset.append(Data(x=X, edge_index=E, edge_attr=EF, y=y_tensor))\n",
    "        \n",
    "    return dataset\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the above functions to create a dataset of molecular graphs from the smiles and labels corresponding to chemical properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties_names = ['A', 'B', 'C', 'mu', 'alfa', 'homo', 'lumo', 'gap', 'R²', 'zpve', 'U0', 'U', 'H', 'G', 'Cv']\n",
    "\n",
    "x_smiles = df.smiles.values\n",
    "y = df.loc[:, properties_names].values  # shape = (n_samples, n_properties)\n",
    "\n",
    "dataset = create_graph_dataset_from_smiles(x_smiles, y[0:len(x_smiles), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132820\n",
      "C#CC#C\n",
      "Data(x=[4, 81], edge_index=[2, 6], edge_attr=[6, 10], y=[1, 15])\n",
      "N#CC#N\n",
      "Data(x=[4, 81], edge_index=[2, 6], edge_attr=[6, 10], y=[1, 15])\n",
      "CC1=CNC=C1\n",
      "Data(x=[6, 81], edge_index=[2, 12], edge_attr=[12, 10], y=[1, 15])\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))\n",
    "\n",
    "# Example entries\n",
    "print(df.smiles.values[0])\n",
    "print(dataset[0])\n",
    "print(df.smiles.values[2])\n",
    "print(dataset[2])\n",
    "print(df.smiles.values[50])\n",
    "print(dataset[50])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information of the graph dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of graphs (molecules): 132820\n",
      "=================================================================================\n",
      "Properties of graph 50, molecule smiles: CC1=CNC=C1\n",
      "Number of nodes: 6\n",
      "Number of edges: 12\n",
      "Number of node features: 81\n",
      "Number of edge features: 10\n",
      "Number of properties: 15\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of graphs (molecules): {len(dataset)}')\n",
    "graph = dataset[50]\n",
    "print('=================================================================================')\n",
    "print(f'Properties of graph {50}, molecule smiles: {df.smiles.values[50]}')\n",
    "print(f'Number of nodes: {dataset[50].x.shape[0]}')\n",
    "print(f'Number of edges: {dataset[50].edge_index.shape[1]}')\n",
    "print(f'Number of node features: {dataset[50].x.shape[1]}')\n",
    "print(f'Number of edge features: {dataset[50].edge_attr.shape[1]}')\n",
    "print(f'Number of properties: {dataset[50].y.shape[1]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filterout data with no edge features defined (Like ex: CH4) (These causes problems down the line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of none edge feature molecules:  0\n",
      "Before:  132820\n",
      "After:  132820\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "indexes_to_delete = []\n",
    "for item in range(0,len(dataset)):\n",
    "    if dataset[item].edge_attr.shape[1] == 0:\n",
    "        indexes_to_delete.append(item)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "indexes_to_delete.sort()\n",
    "\n",
    "print(\"Number of none edge feature molecules: \", len(indexes_to_delete))\n",
    "\n",
    "print(\"Before: \", len(dataset))\n",
    "\n",
    "for item in range(0,len(indexes_to_delete)):\n",
    "    print(\"Molecule to delete: \", df.smiles.values[indexes_to_delete[item]])\n",
    "    #del dataset[indexes_to_delete[item] - item] \n",
    "    dataset.pop((indexes_to_delete[item] - item)) # -item since all future data points will have its index reduced by 1 for each deleted previous data point\n",
    "print(\"After: \", len(dataset))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example train data target before scaling tensor([[ 2.9693e+00,  1.9777e+00,  1.8423e+00,  1.2093e+00,  8.4060e+01,\n",
      "         -1.8970e-01, -7.0400e-02,  1.1930e-01,  8.7339e+02,  1.5988e-01,\n",
      "         -3.4861e+02, -3.4860e+02, -3.4860e+02, -3.4864e+02,  2.9189e+01]])\n",
      "Example train data target after scaling: tensor([[-0.3350,  1.3300,  2.2841, -1.0264,  1.0990,  2.2975, -1.7445, -2.7966,\n",
      "         -1.1527,  0.3357,  1.6155,  1.6155,  1.6155,  1.6156, -0.6067]])\n",
      "Total data size:  132820\n",
      "Train data size:  106256\n",
      "Test data size:  26564\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "\n",
    "# split the dataset into test and validation:\n",
    "num_samples = len(dataset)\n",
    "\n",
    "# Want to divide data randomly\n",
    "random.seed(42)\n",
    "random_indexes = np.array(random.sample(range(num_samples), num_samples)) # random.sample ensures no duplicates\n",
    "\n",
    "train_data = [dataset[index] for index in random_indexes[int(.2 * num_samples ) :]] # 80%\n",
    "test_data = [dataset[index] for index in random_indexes[: int(.2 * num_samples )]] # 20%\n",
    "\n",
    "print(\"Example train data target before scaling\", train_data[0].y)\n",
    "train_data_targets = [data.y for data in train_data]\n",
    "train_data_targets = torch.concatenate(train_data_targets, axis=0)\n",
    "test_data_targets = [data.y for data in test_data]\n",
    "test_data_targets = torch.concatenate(test_data_targets, axis=0)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_data_targets = scaler.fit_transform(train_data_targets)\n",
    "test_data_targets = scaler.transform(test_data_targets)\n",
    "\n",
    "# print(\"Example train data target\", train_data_targets[0].reshape(1,-1).shape)\n",
    "train_data_targets = torch.tensor(train_data_targets, dtype=torch.float)\n",
    "test_data_targets = torch.tensor(test_data_targets, dtype=torch.float)\n",
    "\n",
    "train_data = [Data(x=data.x, edge_index=data.edge_index, edge_attr=data.edge_attr, y=train_data_targets[index].reshape(1,-1)) for index, data in enumerate(train_data)]\n",
    "test_data = [Data(x=data.x, edge_index=data.edge_index, edge_attr=data.edge_attr, y=test_data_targets[index].reshape(1,-1)) for index, data in enumerate(test_data)]\n",
    "print(\"Example train data target after scaling:\", train_data[0].y)\n",
    "\n",
    "print(\"Total data size: \", len(dataset))\n",
    "print(\"Train data size: \", len(train_data))\n",
    "print(\"Test data size: \", len(test_data))\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main GNN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model for all targets at once"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GNN function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_labels = dataset[50].y.shape[1]\n",
    "data_features = dataset[50].x.shape[1]\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GATConv(data_features, hidden_channels)\n",
    "        self.conv2 = GATConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GATConv(hidden_channels, hidden_channels)\n",
    "        self.conv4 = GATConv(hidden_channels, hidden_channels)\n",
    "        self.conv5 = GATConv(hidden_channels, hidden_channels)\n",
    "        self.lin = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin2 = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin3 = Linear(hidden_channels, data_labels)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch): \n",
    "        x = self.conv1(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv4(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv5(x, edge_index, edge_attr)\n",
    "\n",
    "        #Returns batch-wise graph-level-outputs by averaging node features across the node dimension, so that for a single graph G\n",
    "        #its output is computed by\n",
    "        x = global_mean_pool(x, batch) \n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = F.relu(x)\n",
    "        x = self.lin2(x)\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = F.relu(x)\n",
    "        x = self.lin3(x)\n",
    "        \n",
    " \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial training R2:  [-0.01865857 -0.01742911 -0.01774871 -0.02228407 -0.0201281  -0.02344883\n",
      " -0.01577503 -0.01612567 -0.0229795  -0.01822544 -0.01762917 -0.02177696\n",
      " -0.01757433 -0.02585516 -0.01675634]\n",
      "Initial test R2:  [-0.0195153  -0.01639351 -0.0190408  -0.02689918 -0.01962406 -0.0210199\n",
      " -0.02546815 -0.02056046 -0.02444171 -0.01856565 -0.01779759 -0.02515549\n",
      " -0.01732174 -0.0235655  -0.01742934]\n",
      "Epoch: 001, Loss: 0.53410\n",
      "Average Train R2: [0.24880689 0.36706613 0.41893074 0.31687617 0.62654299 0.25176206\n",
      " 0.69219754 0.64760428 0.59030507 0.81154387 0.62760868 0.6255647\n",
      " 0.62785687 0.62762943 0.64958692]\n",
      "Average Test R2: [0.265577   0.37901875 0.42993328 0.31407711 0.62735961 0.24115116\n",
      " 0.69647084 0.64695372 0.59797961 0.81368156 0.64375137 0.64203859\n",
      " 0.64418709 0.64391944 0.65213049]\n",
      "Epoch: 002, Loss: 0.41042\n",
      "Average Train R2: [0.24942971 0.52258773 0.58967103 0.359278   0.65592439 0.56680182\n",
      " 0.77624317 0.71196212 0.71812648 0.85875174 0.68912926 0.68894879\n",
      " 0.68946904 0.6888237  0.67475607]\n",
      "Average Test R2: [0.26838109 0.52524539 0.59407423 0.35646594 0.6564941  0.57293843\n",
      " 0.77735601 0.71065694 0.71800121 0.85510493 0.69946742 0.69932301\n",
      " 0.69972626 0.69913881 0.66520123]\n",
      "Epoch: 003, Loss: 0.37698\n",
      "Average Train R2: [0.30254268 0.54301447 0.6058944  0.37267861 0.67115126 0.66616843\n",
      " 0.83281625 0.78929642 0.72846938 0.85228406 0.70590741 0.70583937\n",
      " 0.70580119 0.70581968 0.73177603]\n",
      "Average Test R2: [0.31668723 0.54721011 0.61184146 0.37462229 0.6688737  0.67005615\n",
      " 0.8320193  0.78749924 0.73015495 0.85299767 0.71002833 0.71006916\n",
      " 0.70992402 0.70991181 0.72757409]\n",
      "Epoch: 004, Loss: 0.34794\n",
      "Average Train R2: [0.34012237 0.56968707 0.62815108 0.36482948 0.69058056 0.69490005\n",
      " 0.83606731 0.81364432 0.74609261 0.86623705 0.73632276 0.73622132\n",
      " 0.73636173 0.73631394 0.78324665]\n",
      "Average Test R2: [0.35112881 0.57247109 0.63166391 0.36402822 0.68679766 0.69521569\n",
      " 0.83679119 0.81108013 0.74531408 0.86857545 0.74814989 0.74806401\n",
      " 0.7481761  0.74813677 0.78088587]\n",
      "Epoch: 005, Loss: 0.32259\n",
      "Average Train R2: [0.42052807 0.6062563  0.65329142 0.38437175 0.70786944 0.72695005\n",
      " 0.89622498 0.85957208 0.7574063  0.84770999 0.7526946  0.75274012\n",
      " 0.75261478 0.75266522 0.75094521]\n",
      "Average Test R2: [0.43006867 0.6083417  0.65868263 0.37594321 0.70936147 0.7298391\n",
      " 0.89447052 0.85611125 0.75679026 0.84895604 0.76031244 0.76035867\n",
      " 0.76022418 0.76028271 0.75082993]\n",
      "Epoch: 006, Loss: 0.30951\n",
      "Average Train R2: [0.4755827  0.6082844  0.65324474 0.39186645 0.71798249 0.72292432\n",
      " 0.90140482 0.86744114 0.77071181 0.86454903 0.74553334 0.74561954\n",
      " 0.74556224 0.74554549 0.78365113]\n",
      "Average Test R2: [0.48135722 0.6085324  0.64509735 0.38540663 0.70797076 0.72305722\n",
      " 0.90053905 0.86510274 0.76835891 0.86552733 0.75415421 0.75424326\n",
      " 0.75417943 0.75416682 0.78068886]\n",
      "Epoch: 007, Loss: 0.30192\n",
      "Average Train R2: [0.49124018 0.62914389 0.67173555 0.39308988 0.73084494 0.74166707\n",
      " 0.89571898 0.85574023 0.78028656 0.85131991 0.77107078 0.77107828\n",
      " 0.77107947 0.77106063 0.78826144]\n",
      "Average Test R2: [0.50363644 0.63018206 0.6737838  0.38834562 0.72966973 0.74392343\n",
      " 0.8953058  0.85419559 0.77953113 0.85205626 0.77737264 0.77738796\n",
      " 0.77737764 0.77736078 0.78437577]\n",
      "Epoch: 008, Loss: 0.29636\n",
      "Average Train R2: [0.49386658 0.64454653 0.676239   0.39465441 0.74032969 0.74230497\n",
      " 0.90471603 0.86822471 0.79045916 0.8628642  0.76941138 0.76945321\n",
      " 0.76939579 0.76940109 0.78145752]\n",
      "Average Test R2: [0.49933464 0.64392031 0.67667655 0.39416041 0.73916071 0.73999697\n",
      " 0.90328838 0.86429507 0.78885377 0.86378414 0.77707441 0.77711654\n",
      " 0.77705791 0.77706366 0.77718641]\n",
      "Epoch: 009, Loss: 0.29285\n",
      "Average Train R2: [0.52597275 0.6339406  0.67212915 0.39494491 0.74913482 0.72529964\n",
      " 0.90141866 0.87245465 0.78195935 0.86089275 0.76090316 0.76085785\n",
      " 0.76093278 0.7609002  0.78834697]\n",
      "Average Test R2: [0.52646853 0.63601426 0.67456985 0.3923981  0.74852051 0.72357843\n",
      " 0.90011505 0.86998917 0.78032419 0.86222627 0.76744484 0.76740147\n",
      " 0.76747313 0.76744156 0.78629351]\n",
      "Epoch: 010, Loss: 0.28831\n",
      "Average Train R2: [0.53548719 0.65484158 0.67297143 0.40288769 0.76208612 0.73946579\n",
      " 0.89865981 0.87282918 0.78611041 0.85921998 0.77188459 0.77184932\n",
      " 0.77191056 0.77188199 0.78457691]\n",
      "Average Test R2: [0.54367188 0.65612343 0.67407666 0.3791986  0.76135643 0.74159159\n",
      " 0.89872849 0.87048503 0.78587862 0.86179397 0.77854902 0.77851277\n",
      " 0.77857531 0.77854653 0.78294853]\n",
      "Epoch: 011, Loss: 0.28545\n",
      "Average Train R2: [0.54291474 0.65774022 0.69197913 0.40281694 0.75755654 0.74498329\n",
      " 0.9137852  0.87543302 0.79696503 0.85951915 0.7759585  0.77596591\n",
      " 0.77596251 0.77595176 0.80258565]\n",
      "Average Test R2: [0.54813188 0.65828399 0.69428722 0.39832095 0.75818391 0.74537733\n",
      " 0.91359722 0.87344089 0.79648589 0.86112637 0.78409453 0.78410113\n",
      " 0.78409877 0.78408798 0.80057229]\n",
      "Epoch: 012, Loss: 0.28334\n",
      "Average Train R2: [0.5427176  0.65202263 0.67631401 0.40515966 0.77293163 0.7499254\n",
      " 0.89786699 0.85162894 0.7893269  0.86017802 0.78439453 0.78439978\n",
      " 0.78440116 0.7843877  0.80528404]\n",
      "Average Test R2: [0.55110856 0.6538425  0.67869624 0.40566628 0.77356345 0.74036874\n",
      " 0.89834934 0.84971565 0.78979592 0.8634569  0.78772322 0.78772893\n",
      " 0.78772961 0.78771606 0.80402352]\n",
      "Epoch: 013, Loss: 0.28055\n",
      "Average Train R2: [0.55280599 0.64449066 0.68442228 0.40953462 0.7652932  0.72767977\n",
      " 0.90678656 0.87012942 0.77895927 0.86286963 0.78972364 0.78973471\n",
      " 0.78972628 0.78971629 0.80161301]\n",
      "Average Test R2: [0.56172422 0.64430433 0.68498585 0.38971416 0.7662434  0.72931892\n",
      " 0.90615345 0.86721131 0.77868784 0.86273429 0.79690426 0.79691561\n",
      " 0.79690646 0.79689705 0.80035373]\n",
      "Epoch: 014, Loss: 0.27847\n",
      "Average Train R2: [0.56423355 0.64441871 0.67916723 0.4108781  0.77002241 0.74522896\n",
      " 0.90837329 0.86042206 0.76709718 0.86361171 0.78659266 0.78660267\n",
      " 0.78659636 0.78658507 0.81283648]\n",
      "Average Test R2: [0.56810964 0.64621729 0.68411989 0.41234782 0.76956025 0.74160364\n",
      " 0.90617063 0.8574335  0.7702621  0.86518767 0.79140023 0.79140947\n",
      " 0.79140386 0.79139336 0.81279847]\n",
      "Epoch: 015, Loss: 0.27775\n",
      "Average Train R2: [0.51251629 0.65311023 0.67797783 0.4056903  0.74063289 0.75652348\n",
      " 0.91314133 0.88576144 0.7687047  0.85749433 0.76053851 0.76054605\n",
      " 0.76054508 0.76053047 0.78337364]\n",
      "Average Test R2: [0.51841738 0.65263495 0.67940213 0.40316741 0.74294057 0.75741307\n",
      " 0.91115777 0.88050222 0.76723788 0.85931464 0.76850266 0.76850981\n",
      " 0.76850933 0.7684948  0.78198571]\n",
      "Epoch: 016, Loss: 0.27573\n",
      "Average Train R2: [0.56410518 0.67241579 0.68659848 0.41652291 0.7691469  0.75321262\n",
      " 0.91311558 0.88818345 0.79996167 0.86838728 0.76432144 0.76433011\n",
      " 0.76432486 0.76431524 0.82393651]\n",
      "Average Test R2: [0.57422966 0.67364592 0.68871025 0.41693132 0.76949997 0.75220497\n",
      " 0.91265847 0.88552351 0.79883871 0.87058949 0.77284248 0.77285087\n",
      " 0.77284582 0.77283644 0.82338647]\n",
      "Epoch: 017, Loss: 0.27589\n",
      "Average Train R2: [0.54832983 0.66454811 0.68424678 0.41667267 0.77048604 0.76062631\n",
      " 0.91877049 0.8887179  0.78283821 0.867087   0.79193985 0.79194494\n",
      " 0.79194521 0.79193409 0.814849  ]\n",
      "Average Test R2: [0.54993811 0.66213158 0.68492727 0.41364023 0.76752475 0.76283949\n",
      " 0.91797626 0.88665296 0.78001149 0.8676048  0.79822537 0.79823033\n",
      " 0.79823057 0.7982198  0.81313789]\n",
      "Epoch: 018, Loss: 0.27442\n",
      "Average Train R2: [0.56570071 0.65962206 0.68910732 0.41003811 0.76598358 0.75616176\n",
      " 0.90602517 0.88506693 0.79884054 0.86599262 0.76612578 0.76613566\n",
      " 0.76612911 0.76611902 0.82076186]\n",
      "Average Test R2: [0.56863346 0.65620044 0.68689453 0.40560601 0.76045604 0.75796982\n",
      " 0.90599591 0.88253505 0.79508607 0.86501517 0.77356563 0.77357528\n",
      " 0.7735689  0.77355909 0.81765968]\n",
      "Epoch: 019, Loss: 0.27331\n",
      "Average Train R2: [0.57733359 0.67773296 0.70547996 0.42148108 0.78089222 0.75409023\n",
      " 0.91433635 0.8891991  0.81245701 0.86679335 0.79670535 0.79670977\n",
      " 0.7967114  0.79669948 0.81226186]\n",
      "Average Test R2: [0.57852875 0.67142527 0.7053445  0.42011278 0.77871942 0.75674832\n",
      " 0.91340445 0.88706689 0.80702358 0.86871227 0.80290672 0.80291111\n",
      " 0.8029128  0.80290086 0.80948791]\n",
      "Epoch: 020, Loss: 0.27327\n",
      "Average Train R2: [0.52986936 0.68563824 0.70459653 0.41833183 0.78037876 0.76745281\n",
      " 0.9187282  0.88661466 0.81503921 0.859655   0.79835394 0.79835894\n",
      " 0.79836038 0.79834764 0.78342127]\n",
      "Average Test R2: [0.49191142 0.68295213 0.70098162 0.41494752 0.76530836 0.76808074\n",
      " 0.91072449 0.88221035 0.81097463 0.86022369 0.80031796 0.80032263\n",
      " 0.80032495 0.8003111  0.77681599]\n",
      "Epoch: 021, Loss: 0.27083\n",
      "Average Train R2: [0.58225146 0.66824207 0.68932069 0.42012438 0.75923654 0.76196583\n",
      " 0.92168486 0.89033018 0.79064838 0.86460576 0.79051115 0.79051662\n",
      " 0.79051646 0.79050525 0.82010544]\n",
      "Average Test R2: [0.58401509 0.66915971 0.69047122 0.41549516 0.75885283 0.76143729\n",
      " 0.92056986 0.88750037 0.77870939 0.86609278 0.79863349 0.79863886\n",
      " 0.79863877 0.79862766 0.81899518]\n",
      "Epoch: 022, Loss: 0.27188\n",
      "Average Train R2: [0.57777413 0.68656668 0.6924656  0.41518251 0.76970921 0.76561322\n",
      " 0.89417714 0.86049785 0.80631109 0.85923211 0.78818466 0.78818937\n",
      " 0.78819007 0.788179   0.81889584]\n",
      "Average Test R2: [0.58262229 0.68634585 0.69237006 0.39864561 0.76830395 0.763294\n",
      " 0.89280718 0.8574354  0.80640797 0.85954606 0.79292019 0.79292473\n",
      " 0.79292543 0.79291477 0.81752665]\n",
      "Epoch: 023, Loss: 0.26958\n",
      "Average Train R2: [0.58089806 0.65937539 0.68639301 0.41231637 0.78540482 0.73307788\n",
      " 0.92119902 0.88183927 0.79106854 0.86467752 0.7968269  0.79683209\n",
      " 0.79683198 0.79682148 0.81977875]\n",
      "Average Test R2: [0.58930864 0.65793125 0.68646905 0.41299348 0.78246816 0.73349382\n",
      " 0.91968505 0.87922389 0.79029073 0.86435312 0.80263891 0.80264404\n",
      " 0.8026439  0.80263359 0.81715706]\n",
      "Epoch: 024, Loss: 0.26939\n",
      "Average Train R2: [0.5907722  0.68009953 0.69481039 0.41982231 0.78776258 0.76436958\n",
      " 0.91530759 0.88480825 0.79821835 0.86710604 0.80008127 0.80008657\n",
      " 0.80008686 0.800075   0.81357915]\n",
      "Average Test R2: [0.5959326  0.67756528 0.69290965 0.4205614  0.78403363 0.76587302\n",
      " 0.9141306  0.88211963 0.79554082 0.86807706 0.80301261 0.8030179\n",
      " 0.80301821 0.80300627 0.81019016]\n",
      "Epoch: 025, Loss: 0.27039\n",
      "Average Train R2: [0.58154164 0.66871714 0.67123197 0.42171338 0.78200962 0.7585811\n",
      " 0.90780727 0.88313764 0.79171098 0.87645766 0.78120316 0.78120915\n",
      " 0.7812096  0.78119576 0.81174431]\n",
      "Average Test R2: [0.59026146 0.66478663 0.6684493  0.42116858 0.77817142 0.75733453\n",
      " 0.9068244  0.8781677  0.78690374 0.87539864 0.78523759 0.78524369\n",
      " 0.78524412 0.78523005 0.80868115]\n",
      "Epoch: 026, Loss: 0.26885\n",
      "Average Train R2: [0.58271649 0.67961299 0.69697974 0.42046604 0.79042451 0.76107723\n",
      " 0.91392135 0.88829764 0.81697533 0.87585613 0.7956258  0.79563097\n",
      " 0.7956312  0.79562008 0.82027369]\n",
      "Average Test R2: [0.58703543 0.6763135  0.69399245 0.41621961 0.78712954 0.76123398\n",
      " 0.91270134 0.88522948 0.81473963 0.87596812 0.80112616 0.8011313\n",
      " 0.80113152 0.80112046 0.81725855]\n",
      "Epoch: 027, Loss: 0.26832\n",
      "Average Train R2: [0.5924645  0.66719901 0.68631815 0.42034077 0.78742098 0.76529167\n",
      " 0.91012317 0.88507872 0.79949665 0.86556589 0.78813314 0.78813854\n",
      " 0.78813867 0.78812681 0.82503514]\n",
      "Average Test R2: [0.59702059 0.66524425 0.68497799 0.4194932  0.78391566 0.76614492\n",
      " 0.90918662 0.88191819 0.79532325 0.85177147 0.79166971 0.79167501\n",
      " 0.79167514 0.79166353 0.81902785]\n",
      "Epoch: 028, Loss: 0.26697\n",
      "Average Train R2: [0.5676912  0.68498481 0.70625956 0.42434369 0.78324572 0.76807508\n",
      " 0.91898743 0.89201988 0.81774079 0.8603234  0.79772822 0.7977329\n",
      " 0.79773304 0.79772306 0.82424044]\n",
      "Average Test R2: [0.529391   0.67350827 0.69649336 0.42183566 0.7806331  0.76845828\n",
      " 0.91771989 0.88970482 0.80885246 0.86053989 0.80083184 0.80083641\n",
      " 0.80083655 0.80082681 0.82201669]\n",
      "Epoch: 029, Loss: 0.26607\n",
      "Average Train R2: [0.596325   0.66248135 0.64942093 0.42101881 0.78284537 0.76188109\n",
      " 0.86246049 0.81925402 0.79269078 0.86743723 0.8034859  0.80349132\n",
      " 0.8034913  0.80347977 0.82038476]\n",
      "Average Test R2: [0.59461624 0.66001166 0.65091392 0.41948614 0.78074686 0.76298074\n",
      " 0.86057092 0.81469386 0.79010169 0.86026198 0.80575105 0.80575641\n",
      " 0.80575638 0.805745   0.81708276]\n",
      "Epoch: 030, Loss: 0.26598\n",
      "Average Train R2: [0.59518922 0.68653812 0.7042399  0.41493737 0.78617766 0.75829724\n",
      " 0.88722131 0.86579727 0.81688971 0.87568993 0.80171497 0.80172024\n",
      " 0.80172021 0.80170896 0.82218064]\n",
      "Average Test R2: [0.59661326 0.68558729 0.70455917 0.41200407 0.78386098 0.75963712\n",
      " 0.88553335 0.86111517 0.81385668 0.87675062 0.80447125 0.80447628\n",
      " 0.80447624 0.80446557 0.81996912]\n"
     ]
    }
   ],
   "source": [
    "model = GNN(hidden_channels=128) \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005, weight_decay=5e-4)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "def train(data_in):\n",
    "      model.train()\n",
    "      optimizer.zero_grad()  # Clear gradients.\n",
    "      out = model(data_in.x, data_in.edge_index, data_in.edge_attr, data_in.batch)  # Perform a single forward pass.\n",
    "\n",
    "      targets = data_in.y\n",
    "      loss = criterion(out, targets) \n",
    "\n",
    "      loss.backward()  # Derive gradients.\n",
    "      optimizer.step()  # Update parameters based on gradients.\n",
    "      return loss\n",
    "\n",
    "def test(data):\n",
    "      all_test_r2 = []\n",
    "      counter = -1    \n",
    "      for data_in in data:\n",
    "            counter += 1\n",
    "            model.eval()\n",
    "            out = model(data_in.x, data_in.edge_index, data_in.edge_attr, data_in.batch)\n",
    "\n",
    "            # Caculate R2\n",
    "            r2_score_var = []\n",
    "            for item in range(0,data_in.y.shape[1]):\n",
    "                  if item == 0:\n",
    "                        r2_score_var = r2_score(data_in.y[:,item].detach().numpy(), out[:,item].detach().numpy())\n",
    "                  else:\n",
    "                        r2_score_var = np.vstack((r2_score_var,(r2_score(data_in.y[:,item].detach().numpy(), out[:,item].detach().numpy()))))\n",
    "\n",
    "            if counter == 0:\n",
    "                  all_test_r2 = r2_score_var\n",
    "            else:\n",
    "                  all_test_r2 = np.hstack((all_test_r2,r2_score_var))\n",
    "\n",
    "      average_test_r2 = np.sum(all_test_r2,axis=1) / all_test_r2.shape[1]\n",
    "\n",
    "      return average_test_r2\n",
    "\n",
    "# Vectors to append accuracy to:\n",
    "Train_r2 = []\n",
    "Test_r2 = []\n",
    "\n",
    "# Calculate accuracy before training \n",
    "Train_r2.append(test(train_loader))\n",
    "Test_r2.append(test(test_loader))\n",
    "print(\"Initial training R2: \", Train_r2[0])\n",
    "print(\"Initial test R2: \", Test_r2[0])\n",
    "\n",
    "print_r2_option = True\n",
    "for epoch in range(1, 31):\n",
    "      average_loss = []\n",
    "      for data in train_loader:\n",
    "            loss = train(data)\n",
    "            average_loss.append(loss)\n",
    "      print(f'Epoch: {epoch:03d}, Loss: {(sum(average_loss)/len(average_loss)):.5f}')\n",
    "\n",
    "      if print_r2_option:\n",
    "\n",
    "            temp_train_r2 = test(train_loader)\n",
    "            Train_r2.append(temp_train_r2)\n",
    "\n",
    "            temp_test_r2 = test(test_loader)\n",
    "            Test_r2.append(temp_test_r2)\n",
    "\n",
    "            print(f'Average Train R2: {temp_train_r2}')\n",
    "            print(f'Average Test R2: {temp_test_r2:}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final loss:  0.26597893\n",
      "Final training R2:  [0.59518922 0.68653812 0.7042399  0.41493737 0.78617766 0.75829724\n",
      " 0.88722131 0.86579727 0.81688971 0.87568993 0.80171497 0.80172024\n",
      " 0.80172021 0.80170896 0.82218064]\n",
      "Final test R2:  [0.59661326 0.68558729 0.70455917 0.41200407 0.78386098 0.75963712\n",
      " 0.88553335 0.86111517 0.81385668 0.87675062 0.80447125 0.80447628\n",
      " 0.80447624 0.80446557 0.81996912]\n"
     ]
    }
   ],
   "source": [
    "print(\"Final loss: \", (sum(average_loss)/len(average_loss)).detach().numpy())\n",
    "print(\"Final training R2: \", Train_r2[-1])\n",
    "print(\"Final test R2: \", Test_r2[-1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model for just one target"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GNN function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_labels = 1\n",
    "data_features = dataset[50].x.shape[1]\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GATConv(data_features, hidden_channels)\n",
    "        self.conv2 = GATConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GATConv(hidden_channels, hidden_channels)\n",
    "        self.conv4 = GATConv(hidden_channels, hidden_channels)\n",
    "        self.conv5 = GATConv(hidden_channels, hidden_channels)\n",
    "        self.lin = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin2 = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin3 = Linear(hidden_channels, data_labels)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch): \n",
    "        x = self.conv1(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv4(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv5(x, edge_index, edge_attr)\n",
    "\n",
    "        #Returns batch-wise graph-level-outputs by averaging node features across the node dimension, so that for a single graph G\n",
    "        #its output is computed by\n",
    "        x = global_mean_pool(x, batch) \n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = F.relu(x)\n",
    "        x = self.lin2(x)\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = F.relu(x)\n",
    "        x = self.lin3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target index:  0\n",
      "Initial training R2:  -0.02350298644176647\n",
      "Initial test R2:  -0.02277312422248588\n",
      "Epoch: 001, Loss: 0.75478\n",
      "Epoch: 002, Loss: 0.61201\n",
      "Epoch: 003, Loss: 0.54524\n",
      "Epoch: 004, Loss: 0.50621\n",
      "Epoch: 005, Loss: 0.47388\n",
      "Epoch: 006, Loss: 0.45273\n",
      "Epoch: 007, Loss: 0.43445\n",
      "Epoch: 008, Loss: 0.41947\n",
      "Epoch: 009, Loss: 0.40344\n",
      "Epoch: 010, Loss: 0.39646\n",
      "Epoch: 011, Loss: 0.38529\n",
      "Epoch: 012, Loss: 0.37788\n",
      "Epoch: 013, Loss: 0.37201\n",
      "Epoch: 014, Loss: 0.36414\n",
      "Epoch: 015, Loss: 0.35948\n",
      "Epoch: 016, Loss: 0.35402\n",
      "Epoch: 017, Loss: 0.34916\n",
      "Epoch: 018, Loss: 0.34466\n",
      "Epoch: 019, Loss: 0.34472\n",
      "Epoch: 020, Loss: 0.34064\n",
      "Best training R2 for target 0: 0.6636045075432618\n",
      "Best test R2 for target 0: 0.6633040708373179\n",
      "Target index:  1\n",
      "Initial training R2:  -0.019556148457033976\n",
      "Initial test R2:  -0.020751837370101316\n",
      "Epoch: 001, Loss: 0.63477\n",
      "Epoch: 002, Loss: 0.51414\n",
      "Epoch: 003, Loss: 0.45997\n",
      "Epoch: 004, Loss: 0.41721\n",
      "Epoch: 005, Loss: 0.39401\n",
      "Epoch: 006, Loss: 0.37783\n",
      "Epoch: 007, Loss: 0.36235\n",
      "Epoch: 008, Loss: 0.35411\n",
      "Epoch: 009, Loss: 0.34534\n",
      "Epoch: 010, Loss: 0.34089\n",
      "Epoch: 011, Loss: 0.33671\n",
      "Epoch: 012, Loss: 0.33286\n",
      "Epoch: 013, Loss: 0.32874\n",
      "Epoch: 014, Loss: 0.32577\n",
      "Epoch: 015, Loss: 0.32382\n",
      "Epoch: 016, Loss: 0.31910\n",
      "Epoch: 017, Loss: 0.31706\n",
      "Epoch: 018, Loss: 0.31479\n",
      "Epoch: 019, Loss: 0.31168\n",
      "Epoch: 020, Loss: 0.31004\n",
      "Best training R2 for target 1: 0.710362291730381\n",
      "Best test R2 for target 1: 0.7092732408649669\n",
      "Target index:  2\n",
      "Initial training R2:  -0.021824641442938792\n",
      "Initial test R2:  -0.02061272224855382\n",
      "Epoch: 001, Loss: 0.57500\n",
      "Epoch: 002, Loss: 0.45826\n",
      "Epoch: 003, Loss: 0.43073\n",
      "Epoch: 004, Loss: 0.41353\n",
      "Epoch: 005, Loss: 0.39910\n",
      "Epoch: 006, Loss: 0.38831\n",
      "Epoch: 007, Loss: 0.37356\n",
      "Epoch: 008, Loss: 0.36167\n",
      "Epoch: 009, Loss: 0.34915\n",
      "Epoch: 010, Loss: 0.34099\n",
      "Epoch: 011, Loss: 0.33713\n",
      "Epoch: 012, Loss: 0.33379\n",
      "Epoch: 013, Loss: 0.32956\n",
      "Epoch: 014, Loss: 0.32639\n",
      "Epoch: 015, Loss: 0.32514\n",
      "Epoch: 016, Loss: 0.32283\n",
      "Epoch: 017, Loss: 0.32175\n",
      "Epoch: 018, Loss: 0.32020\n",
      "Epoch: 019, Loss: 0.31697\n",
      "Epoch: 020, Loss: 0.31718\n",
      "Best training R2 for target 2: 0.7008766990285974\n",
      "Best test R2 for target 2: 0.7010124687446676\n",
      "Target index:  3\n",
      "Initial training R2:  -0.018710509113034592\n",
      "Initial test R2:  -0.022724398850784068\n",
      "Epoch: 001, Loss: 0.61810\n",
      "Epoch: 002, Loss: 0.49544\n",
      "Epoch: 003, Loss: 0.47623\n",
      "Epoch: 004, Loss: 0.46633\n",
      "Epoch: 005, Loss: 0.45944\n",
      "Epoch: 006, Loss: 0.45410\n",
      "Epoch: 007, Loss: 0.45004\n",
      "Epoch: 008, Loss: 0.44626\n",
      "Epoch: 009, Loss: 0.44306\n",
      "Epoch: 010, Loss: 0.43983\n",
      "Epoch: 011, Loss: 0.43711\n",
      "Epoch: 012, Loss: 0.43509\n",
      "Epoch: 013, Loss: 0.43333\n",
      "Epoch: 014, Loss: 0.43097\n",
      "Epoch: 015, Loss: 0.43028\n",
      "Epoch: 016, Loss: 0.42788\n",
      "Epoch: 017, Loss: 0.42682\n",
      "Epoch: 018, Loss: 0.42539\n",
      "Epoch: 019, Loss: 0.42272\n",
      "Epoch: 020, Loss: 0.42126\n",
      "Best training R2 for target 3: 0.5816224007314774\n",
      "Best test R2 for target 3: 0.5791630189408662\n",
      "Target index:  4\n",
      "Initial training R2:  -0.017957503345227645\n",
      "Initial test R2:  -0.018826389038767075\n",
      "Epoch: 001, Loss: 0.46417\n",
      "Epoch: 002, Loss: 0.32335\n",
      "Epoch: 003, Loss: 0.30916\n",
      "Epoch: 004, Loss: 0.30213\n",
      "Epoch: 005, Loss: 0.29541\n",
      "Epoch: 006, Loss: 0.28687\n",
      "Epoch: 007, Loss: 0.27883\n",
      "Epoch: 008, Loss: 0.27022\n",
      "Epoch: 009, Loss: 0.26005\n",
      "Epoch: 010, Loss: 0.24980\n",
      "Epoch: 011, Loss: 0.24266\n",
      "Epoch: 012, Loss: 0.23694\n",
      "Epoch: 013, Loss: 0.23295\n",
      "Epoch: 014, Loss: 0.22987\n",
      "Epoch: 015, Loss: 0.22709\n",
      "Epoch: 016, Loss: 0.22480\n",
      "Epoch: 017, Loss: 0.22265\n",
      "Epoch: 018, Loss: 0.22084\n",
      "Epoch: 019, Loss: 0.21950\n",
      "Epoch: 020, Loss: 0.21777\n",
      "Best training R2 for target 4: 0.7927752196619405\n",
      "Best test R2 for target 4: 0.7924222418738837\n",
      "Target index:  5\n",
      "Initial training R2:  -0.01755465465413163\n",
      "Initial test R2:  -0.023018471160525807\n",
      "Epoch: 001, Loss: 0.53032\n",
      "Epoch: 002, Loss: 0.29399\n",
      "Epoch: 003, Loss: 0.24357\n",
      "Epoch: 004, Loss: 0.21905\n",
      "Epoch: 005, Loss: 0.20451\n",
      "Epoch: 006, Loss: 0.19665\n",
      "Epoch: 007, Loss: 0.19081\n",
      "Epoch: 008, Loss: 0.18457\n",
      "Epoch: 009, Loss: 0.17966\n",
      "Epoch: 010, Loss: 0.17690\n",
      "Epoch: 011, Loss: 0.17405\n",
      "Epoch: 012, Loss: 0.17119\n",
      "Epoch: 013, Loss: 0.16916\n",
      "Epoch: 014, Loss: 0.16848\n",
      "Epoch: 015, Loss: 0.16562\n",
      "Epoch: 016, Loss: 0.16453\n",
      "Epoch: 017, Loss: 0.16276\n",
      "Epoch: 018, Loss: 0.16151\n",
      "Epoch: 019, Loss: 0.16054\n",
      "Epoch: 020, Loss: 0.15932\n",
      "Best training R2 for target 5: 0.8511877798122606\n",
      "Best test R2 for target 5: 0.8473284678635936\n",
      "Target index:  6\n",
      "Initial training R2:  -0.017693073355085884\n",
      "Initial test R2:  -0.029767501941036573\n",
      "Epoch: 001, Loss: 0.34436\n",
      "Epoch: 002, Loss: 0.12944\n",
      "Epoch: 003, Loss: 0.08840\n",
      "Epoch: 004, Loss: 0.07860\n",
      "Epoch: 005, Loss: 0.07378\n",
      "Epoch: 006, Loss: 0.07152\n",
      "Epoch: 007, Loss: 0.06811\n",
      "Epoch: 008, Loss: 0.06611\n",
      "Epoch: 009, Loss: 0.06532\n",
      "Epoch: 010, Loss: 0.06388\n",
      "Epoch: 011, Loss: 0.06312\n",
      "Epoch: 012, Loss: 0.06176\n",
      "Epoch: 013, Loss: 0.06153\n",
      "Epoch: 014, Loss: 0.06104\n",
      "Epoch: 015, Loss: 0.06069\n",
      "Epoch: 016, Loss: 0.05960\n",
      "Epoch: 017, Loss: 0.05948\n",
      "Epoch: 018, Loss: 0.05878\n",
      "Epoch: 019, Loss: 0.05823\n",
      "Epoch: 020, Loss: 0.05815\n",
      "Best training R2 for target 6: 0.9559410397677668\n",
      "Best test R2 for target 6: 0.9537264209407537\n",
      "Target index:  7\n",
      "Initial training R2:  -0.018820415131039225\n",
      "Initial test R2:  -0.022009049036801006\n",
      "Epoch: 001, Loss: 0.36314\n",
      "Epoch: 002, Loss: 0.16107\n",
      "Epoch: 003, Loss: 0.12904\n",
      "Epoch: 004, Loss: 0.11872\n",
      "Epoch: 005, Loss: 0.11278\n",
      "Epoch: 006, Loss: 0.10945\n",
      "Epoch: 007, Loss: 0.10647\n",
      "Epoch: 008, Loss: 0.10392\n",
      "Epoch: 009, Loss: 0.10232\n",
      "Epoch: 010, Loss: 0.10107\n",
      "Epoch: 011, Loss: 0.10026\n",
      "Epoch: 012, Loss: 0.09859\n",
      "Epoch: 013, Loss: 0.09830\n",
      "Epoch: 014, Loss: 0.09739\n",
      "Epoch: 015, Loss: 0.09691\n",
      "Epoch: 016, Loss: 0.09564\n",
      "Epoch: 017, Loss: 0.09554\n",
      "Epoch: 018, Loss: 0.09498\n",
      "Epoch: 019, Loss: 0.09364\n",
      "Epoch: 020, Loss: 0.09369\n",
      "Best training R2 for target 7: 0.9202312796092724\n",
      "Best test R2 for target 7: 0.9169470184386495\n",
      "Target index:  8\n",
      "Initial training R2:  -0.020948827012665677\n",
      "Initial test R2:  -0.02090283884078734\n",
      "Epoch: 001, Loss: 0.48213\n",
      "Epoch: 002, Loss: 0.32532\n",
      "Epoch: 003, Loss: 0.29950\n",
      "Epoch: 004, Loss: 0.28426\n",
      "Epoch: 005, Loss: 0.27154\n",
      "Epoch: 006, Loss: 0.26100\n",
      "Epoch: 007, Loss: 0.25214\n",
      "Epoch: 008, Loss: 0.24312\n",
      "Epoch: 009, Loss: 0.23665\n",
      "Epoch: 010, Loss: 0.22948\n",
      "Epoch: 011, Loss: 0.22364\n",
      "Epoch: 012, Loss: 0.22178\n",
      "Epoch: 013, Loss: 0.21673\n",
      "Epoch: 014, Loss: 0.21423\n",
      "Epoch: 015, Loss: 0.21156\n",
      "Epoch: 016, Loss: 0.20910\n",
      "Epoch: 017, Loss: 0.20662\n",
      "Epoch: 018, Loss: 0.20556\n",
      "Epoch: 019, Loss: 0.20380\n",
      "Epoch: 020, Loss: 0.20272\n",
      "Best training R2 for target 8: 0.8025759432284453\n",
      "Best test R2 for target 8: 0.8032926968266698\n",
      "Target index:  9\n",
      "Initial training R2:  -0.018263924220073856\n",
      "Initial test R2:  -0.022171209111824814\n",
      "Epoch: 001, Loss: 0.21317\n",
      "Epoch: 002, Loss: 0.09035\n",
      "Epoch: 003, Loss: 0.08700\n",
      "Epoch: 004, Loss: 0.08580\n",
      "Epoch: 005, Loss: 0.08435\n",
      "Epoch: 006, Loss: 0.08302\n",
      "Epoch: 007, Loss: 0.08181\n",
      "Epoch: 008, Loss: 0.08136\n",
      "Epoch: 009, Loss: 0.08078\n",
      "Epoch: 010, Loss: 0.08028\n",
      "Epoch: 011, Loss: 0.08021\n",
      "Epoch: 012, Loss: 0.07950\n",
      "Epoch: 013, Loss: 0.07899\n",
      "Epoch: 014, Loss: 0.07926\n",
      "Epoch: 015, Loss: 0.07801\n",
      "Epoch: 016, Loss: 0.07760\n",
      "Epoch: 017, Loss: 0.07733\n",
      "Epoch: 018, Loss: 0.07753\n",
      "Epoch: 019, Loss: 0.07727\n",
      "Epoch: 020, Loss: 0.07688\n",
      "Best training R2 for target 9: 0.936628410369879\n",
      "Best test R2 for target 9: 0.9373772248321128\n",
      "Target index:  10\n",
      "Initial training R2:  -0.018265002867474192\n",
      "Initial test R2:  -0.019224871491389576\n",
      "Epoch: 001, Loss: 0.46594\n",
      "Epoch: 002, Loss: 0.33912\n",
      "Epoch: 003, Loss: 0.32996\n",
      "Epoch: 004, Loss: 0.32415\n",
      "Epoch: 005, Loss: 0.31935\n",
      "Epoch: 006, Loss: 0.31353\n",
      "Epoch: 007, Loss: 0.30456\n",
      "Epoch: 008, Loss: 0.29546\n",
      "Epoch: 009, Loss: 0.28375\n",
      "Epoch: 010, Loss: 0.27180\n",
      "Epoch: 011, Loss: 0.26105\n",
      "Epoch: 012, Loss: 0.25143\n",
      "Epoch: 013, Loss: 0.24669\n",
      "Epoch: 014, Loss: 0.24145\n",
      "Epoch: 015, Loss: 0.23915\n",
      "Epoch: 016, Loss: 0.23547\n",
      "Epoch: 017, Loss: 0.23311\n",
      "Epoch: 018, Loss: 0.23100\n",
      "Epoch: 019, Loss: 0.23059\n",
      "Epoch: 020, Loss: 0.22881\n",
      "Best training R2 for target 10: 0.7861361496541605\n",
      "Best test R2 for target 10: 0.7931090209326379\n",
      "Target index:  11\n",
      "Initial training R2:  -0.01826497098020484\n",
      "Initial test R2:  -0.01922484354631408\n",
      "Epoch: 001, Loss: 0.46593\n",
      "Epoch: 002, Loss: 0.33911\n",
      "Epoch: 003, Loss: 0.32995\n",
      "Epoch: 004, Loss: 0.32416\n",
      "Epoch: 005, Loss: 0.31933\n",
      "Epoch: 006, Loss: 0.31352\n",
      "Epoch: 007, Loss: 0.30457\n",
      "Epoch: 008, Loss: 0.29560\n",
      "Epoch: 009, Loss: 0.28393\n",
      "Epoch: 010, Loss: 0.27248\n",
      "Epoch: 011, Loss: 0.26204\n",
      "Epoch: 012, Loss: 0.25232\n",
      "Epoch: 013, Loss: 0.24732\n",
      "Epoch: 014, Loss: 0.24183\n",
      "Epoch: 015, Loss: 0.23928\n",
      "Epoch: 016, Loss: 0.23557\n",
      "Epoch: 017, Loss: 0.23297\n",
      "Epoch: 018, Loss: 0.23100\n",
      "Epoch: 019, Loss: 0.23035\n",
      "Epoch: 020, Loss: 0.22858\n",
      "Best training R2 for target 11: 0.7864241810548465\n",
      "Best test R2 for target 11: 0.7934040176933929\n",
      "Target index:  12\n",
      "Initial training R2:  -0.018264971775211955\n",
      "Initial test R2:  -0.01922484317582709\n",
      "Epoch: 001, Loss: 0.46593\n",
      "Epoch: 002, Loss: 0.33908\n",
      "Epoch: 003, Loss: 0.32990\n",
      "Epoch: 004, Loss: 0.32411\n",
      "Epoch: 005, Loss: 0.31922\n",
      "Epoch: 006, Loss: 0.31341\n",
      "Epoch: 007, Loss: 0.30430\n",
      "Epoch: 008, Loss: 0.29517\n",
      "Epoch: 009, Loss: 0.28335\n",
      "Epoch: 010, Loss: 0.27212\n",
      "Epoch: 011, Loss: 0.26227\n",
      "Epoch: 012, Loss: 0.25312\n",
      "Epoch: 013, Loss: 0.24844\n",
      "Epoch: 014, Loss: 0.24313\n",
      "Epoch: 015, Loss: 0.24054\n",
      "Epoch: 016, Loss: 0.23677\n",
      "Epoch: 017, Loss: 0.23433\n",
      "Epoch: 018, Loss: 0.23194\n",
      "Epoch: 019, Loss: 0.23176\n",
      "Epoch: 020, Loss: 0.22997\n",
      "Best training R2 for target 12: 0.7852140869439007\n",
      "Best test R2 for target 12: 0.7924508923844775\n",
      "Target index:  13\n",
      "Initial training R2:  -0.018265033190665834\n",
      "Initial test R2:  -0.01922489711406268\n",
      "Epoch: 001, Loss: 0.46593\n",
      "Epoch: 002, Loss: 0.33905\n",
      "Epoch: 003, Loss: 0.32990\n",
      "Epoch: 004, Loss: 0.32412\n",
      "Epoch: 005, Loss: 0.31930\n",
      "Epoch: 006, Loss: 0.31347\n",
      "Epoch: 007, Loss: 0.30455\n",
      "Epoch: 008, Loss: 0.29551\n",
      "Epoch: 009, Loss: 0.28370\n",
      "Epoch: 010, Loss: 0.27224\n",
      "Epoch: 011, Loss: 0.26231\n",
      "Epoch: 012, Loss: 0.25317\n",
      "Epoch: 013, Loss: 0.24864\n",
      "Epoch: 014, Loss: 0.24346\n",
      "Epoch: 015, Loss: 0.24090\n",
      "Epoch: 016, Loss: 0.23691\n",
      "Epoch: 017, Loss: 0.23411\n",
      "Epoch: 018, Loss: 0.23162\n",
      "Epoch: 019, Loss: 0.23119\n",
      "Epoch: 020, Loss: 0.22964\n",
      "Best training R2 for target 13: 0.7856129193654282\n",
      "Best test R2 for target 13: 0.7928930646215756\n",
      "Target index:  14\n",
      "Initial training R2:  -0.018903465486760643\n",
      "Initial test R2:  -0.02098441479975519\n",
      "Epoch: 001, Loss: 0.37149\n",
      "Epoch: 002, Loss: 0.26077\n",
      "Epoch: 003, Loss: 0.25360\n",
      "Epoch: 004, Loss: 0.24857\n",
      "Epoch: 005, Loss: 0.24409\n",
      "Epoch: 006, Loss: 0.23853\n",
      "Epoch: 007, Loss: 0.23384\n",
      "Epoch: 008, Loss: 0.22966\n",
      "Epoch: 009, Loss: 0.22405\n",
      "Epoch: 010, Loss: 0.21772\n",
      "Epoch: 011, Loss: 0.21222\n",
      "Epoch: 012, Loss: 0.20485\n",
      "Epoch: 013, Loss: 0.19908\n",
      "Epoch: 014, Loss: 0.19533\n",
      "Epoch: 015, Loss: 0.19160\n",
      "Epoch: 016, Loss: 0.18766\n",
      "Epoch: 017, Loss: 0.18527\n",
      "Epoch: 018, Loss: 0.18402\n",
      "Epoch: 019, Loss: 0.18232\n",
      "Epoch: 020, Loss: 0.18151\n",
      "Best training R2 for target 14: 0.8330250216457179\n",
      "Best test R2 for target 14: 0.8348749141832345\n"
     ]
    }
   ],
   "source": [
    "def train(data_in, target):\n",
    "      model.train()\n",
    "      optimizer.zero_grad()  # Clear gradients.\n",
    "      out = model(data_in.x, data_in.edge_index, data_in.edge_attr, data_in.batch)\n",
    "      targets = data_in.y[:,target].reshape(-1,1)\n",
    "      \n",
    "      #Alt 1\n",
    "      loss = criterion(out, targets)   \n",
    "\n",
    "      loss.backward()  # Derive gradients.\n",
    "      optimizer.step()  # Update parameters based on gradients.\n",
    "      return loss\n",
    "\n",
    "def test(data, target):\n",
    "      all_test_r2 = []\n",
    "      counter = -1    \n",
    "      for data_in in data:\n",
    "            counter += 1\n",
    "            model.eval()\n",
    "            out = model(data_in.x, data_in.edge_index, data_in.edge_attr, data_in.batch)\n",
    "            targets = data_in.y[:,target].reshape(-1,1)\n",
    "            \n",
    "            # Caculate R2\n",
    "            r2_score_var = r2_score(targets.detach().numpy(), out.detach().numpy())\n",
    "\n",
    "            all_test_r2 .append(r2_score_var)\n",
    "\n",
    "      average_test_r2 = np.sum(all_test_r2) / len(all_test_r2)\n",
    "\n",
    "      return average_test_r2\n",
    "\n",
    "num_targets = dataset[50].y.shape[1]\n",
    "for target_index in range(num_targets):\n",
    "      print(\"Target index: \", target_index)\n",
    "\n",
    "      model = GNN(hidden_channels=64) \n",
    "      optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=5e-4)\n",
    "      criterion = torch.nn.MSELoss()\n",
    "\n",
    "      # Vectors to append accuracy to:\n",
    "      Train_r2 = []\n",
    "      Test_r2 = []\n",
    "\n",
    "      # Calculate accuracy before training \n",
    "      Train_r2.append(test(train_loader, target_index))\n",
    "      Test_r2.append(test(test_loader, target_index))\n",
    "      print(\"Initial training R2: \", Train_r2[0])\n",
    "      print(\"Initial test R2: \", Test_r2[0])\n",
    "\n",
    "      print_r2_option = True\n",
    "      for epoch in range(1, 21):\n",
    "            average_loss = []\n",
    "            for data in train_loader:\n",
    "                  loss = train(data, target_index)\n",
    "                  average_loss.append(loss)\n",
    "            print(f'Epoch: {epoch:03d}, Loss: {(sum(average_loss)/len(average_loss)):.5f}')\n",
    "\n",
    "            if print_r2_option:\n",
    "                  temp_train_r2 = test(train_loader, target_index)\n",
    "                  Train_r2.append(temp_train_r2)\n",
    "\n",
    "                  temp_test_r2 = test(test_loader, target_index)\n",
    "                  Test_r2.append(temp_test_r2)\n",
    "\n",
    "                  # print(f'Average Train R2: {temp_train_r2}')\n",
    "                  # print(f'Average Test R2: {temp_test_r2:}')\n",
    "\n",
    "      print(f\"Best training R2 for target {target_index}: {np.max(Train_r2)}\")\n",
    "      print(f\"Best test R2 for target {target_index}: {np.max(Test_r2)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
