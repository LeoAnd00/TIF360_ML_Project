{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TIF360 Project"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main source: https://www.kaggle.com/code/rmonge/predicting-molecule-properties-based-on-its-smiles/notebook"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch_geometric.utils as utils\n",
    "import networkx as nx\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import global_mean_pool, GATConv\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(132820, 21)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/smiles_and_targets.csv\")\n",
    "print(np.shape(df))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigate Neural Networks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Neural Networks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert data to graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph_dataset_functions import create_graph_dataset_from_smiles\n",
    "\n",
    "properties_names = ['A', 'B', 'C', 'mu', 'alfa', 'homo', 'lumo', 'gap', 'RÂ²', 'zpve', 'U0', 'U', 'H', 'G', 'Cv']\n",
    "\n",
    "x_smiles = df.smiles.values\n",
    "y = df.loc[:, properties_names].values  # shape = (n_samples, n_properties)\n",
    "\n",
    "dataset = create_graph_dataset_from_smiles(x_smiles, y[0:len(x_smiles), :])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information of the graph dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of graphs (molecules): 132820\n",
      "=================================================================================\n",
      "Properties of graph 50, molecule smiles: CC1=CNC=C1\n",
      "Number of nodes: 6\n",
      "Number of edges: 12\n",
      "Number of node features: 78\n",
      "Number of edge features: 10\n",
      "Number of target properties: 15\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of graphs (molecules): {len(dataset)}')\n",
    "graph = dataset[50]\n",
    "print('=================================================================================')\n",
    "print(f'Properties of graph {50}, molecule smiles: {df.smiles.values[50]}')\n",
    "print(f'Number of nodes: {dataset[50].x.shape[0]}')\n",
    "print(f'Number of edges: {dataset[50].edge_index.shape[1]}')\n",
    "print(f'Number of node features: {dataset[50].x.shape[1]}')\n",
    "print(f'Number of edge features: {dataset[50].edge_attr.shape[1]}')\n",
    "print(f'Number of target properties: {dataset[50].y.shape[1]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filterout data with no edge features defined (Like ex: CH4) (These causes problems down the line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of none edge feature molecules:  0\n",
      "Before:  132820\n",
      "After:  132820\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "indexes_to_delete = []\n",
    "for item in range(0,len(dataset)):\n",
    "    if dataset[item].edge_attr.shape[1] == 0:\n",
    "        indexes_to_delete.append(item)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "indexes_to_delete.sort()\n",
    "\n",
    "print(\"Number of none edge feature molecules: \", len(indexes_to_delete))\n",
    "\n",
    "print(\"Before: \", len(dataset))\n",
    "\n",
    "for item in range(0,len(indexes_to_delete)):\n",
    "    print(\"Molecule to delete: \", df.smiles.values[indexes_to_delete[item]])\n",
    "    #del dataset[indexes_to_delete[item] - item] \n",
    "    dataset.pop((indexes_to_delete[item] - item)) # -item since all future data points will have its index reduced by 1 for each deleted previous data point\n",
    "print(\"After: \", len(dataset))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example train data target before scaling tensor([[ 2.9693e+00,  1.9777e+00,  1.8423e+00,  1.2093e+00,  8.4060e+01,\n",
      "         -1.8970e-01, -7.0400e-02,  1.1930e-01,  8.7339e+02,  1.5988e-01,\n",
      "         -3.4861e+02, -3.4860e+02, -3.4860e+02, -3.4864e+02,  2.9189e+01]])\n",
      "Example train data target after scaling: tensor([[-0.3350,  1.3300,  2.2841, -1.0264,  1.0990,  2.2975, -1.7445, -2.7966,\n",
      "         -1.1527,  0.3357,  1.6155,  1.6155,  1.6155,  1.6156, -0.6067]])\n",
      "Total data size:  132820\n",
      "Train data size:  106256\n",
      "Test data size:  26564\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "\n",
    "# split the dataset into test and validation:\n",
    "num_samples = len(dataset)\n",
    "\n",
    "# Want to divide data randomly\n",
    "random.seed(42)\n",
    "random_indexes = np.array(random.sample(range(num_samples), num_samples)) # random.sample ensures no duplicates\n",
    "\n",
    "train_data = [dataset[index] for index in random_indexes[int(.2 * num_samples ) :]] # 80%\n",
    "test_data = [dataset[index] for index in random_indexes[: int(.2 * num_samples )]] # 20%\n",
    "\n",
    "print(\"Example train data target before scaling\", train_data[0].y)\n",
    "train_data_targets = [data.y for data in train_data]\n",
    "train_data_targets = torch.concatenate(train_data_targets, axis=0)\n",
    "test_data_targets = [data.y for data in test_data]\n",
    "test_data_targets = torch.concatenate(test_data_targets, axis=0)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_data_targets = scaler.fit_transform(train_data_targets)\n",
    "test_data_targets = scaler.transform(test_data_targets)\n",
    "\n",
    "# print(\"Example train data target\", train_data_targets[0].reshape(1,-1).shape)\n",
    "train_data_targets = torch.tensor(train_data_targets, dtype=torch.float)\n",
    "test_data_targets = torch.tensor(test_data_targets, dtype=torch.float)\n",
    "\n",
    "train_data = [Data(x=data.x, edge_index=data.edge_index, edge_attr=data.edge_attr, y=train_data_targets[index].reshape(1,-1)) for index, data in enumerate(train_data)]\n",
    "test_data = [Data(x=data.x, edge_index=data.edge_index, edge_attr=data.edge_attr, y=test_data_targets[index].reshape(1,-1)) for index, data in enumerate(test_data)]\n",
    "print(\"Example train data target after scaling:\", train_data[0].y)\n",
    "\n",
    "print(\"Total data size: \", len(dataset))\n",
    "print(\"Train data size: \", len(train_data))\n",
    "print(\"Test data size: \", len(test_data))\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main GNN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model for all targets at once"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GNN function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_labels = dataset[50].y.shape[1]\n",
    "data_features = dataset[50].x.shape[1]\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GATConv(data_features, hidden_channels)\n",
    "        self.conv2 = GATConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GATConv(hidden_channels, hidden_channels)\n",
    "        self.conv4 = GATConv(hidden_channels, hidden_channels)\n",
    "        self.conv5 = GATConv(hidden_channels, hidden_channels)\n",
    "        self.lin = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin2 = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin3 = Linear(hidden_channels, data_labels)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch): \n",
    "        x = self.conv1(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv4(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv5(x, edge_index, edge_attr)\n",
    "\n",
    "        #Returns batch-wise graph-level-outputs by averaging node features across the node dimension, so that for a single graph G\n",
    "        #its output is computed by\n",
    "        x = global_mean_pool(x, batch) \n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = F.relu(x)\n",
    "        x = self.lin2(x)\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = F.relu(x)\n",
    "        x = self.lin3(x)\n",
    "        \n",
    " \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 47\u001b[0m\n\u001b[0;32m     44\u001b[0m Test_r2 \u001b[39m=\u001b[39m []\n\u001b[0;32m     46\u001b[0m \u001b[39m# Calculate accuracy before training \u001b[39;00m\n\u001b[1;32m---> 47\u001b[0m Train_r2\u001b[39m.\u001b[39mappend(test(train_loader))\n\u001b[0;32m     48\u001b[0m Test_r2\u001b[39m.\u001b[39mappend(test(test_loader))\n\u001b[0;32m     49\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mInitial training R2: \u001b[39m\u001b[39m\"\u001b[39m, Train_r2[\u001b[39m0\u001b[39m])\n",
      "Cell \u001b[1;32mIn[16], line 23\u001b[0m, in \u001b[0;36mtest\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m     21\u001b[0m counter \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     22\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[1;32m---> 23\u001b[0m out \u001b[39m=\u001b[39m model(data_in\u001b[39m.\u001b[39;49mx, data_in\u001b[39m.\u001b[39;49medge_index, data_in\u001b[39m.\u001b[39;49medge_attr, data_in\u001b[39m.\u001b[39;49mbatch)\n\u001b[0;32m     25\u001b[0m \u001b[39m# Caculate R2\u001b[39;00m\n\u001b[0;32m     26\u001b[0m r2_score_var \u001b[39m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[14], line 18\u001b[0m, in \u001b[0;36mGNN.forward\u001b[1;34m(self, x, edge_index, edge_attr, batch)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, edge_index, edge_attr, batch): \n\u001b[1;32m---> 18\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x, edge_index, edge_attr)\n\u001b[0;32m     19\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(x)\n\u001b[0;32m     20\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(x, edge_index, edge_attr)\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch_geometric\\nn\\conv\\gat_conv.py:255\u001b[0m, in \u001b[0;36mGATConv.forward\u001b[1;34m(self, x, edge_index, edge_attr, size, return_attention_weights)\u001b[0m\n\u001b[0;32m    252\u001b[0m alpha \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39medge_updater(edge_index, alpha\u001b[39m=\u001b[39malpha, edge_attr\u001b[39m=\u001b[39medge_attr)\n\u001b[0;32m    254\u001b[0m \u001b[39m# propagate_type: (x: OptPairTensor, alpha: Tensor)\u001b[39;00m\n\u001b[1;32m--> 255\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpropagate(edge_index, x\u001b[39m=\u001b[39;49mx, alpha\u001b[39m=\u001b[39;49malpha, size\u001b[39m=\u001b[39;49msize)\n\u001b[0;32m    257\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconcat:\n\u001b[0;32m    258\u001b[0m     out \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheads \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout_channels)\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch_geometric\\nn\\conv\\message_passing.py:467\u001b[0m, in \u001b[0;36mMessagePassing.propagate\u001b[1;34m(self, edge_index, size, **kwargs)\u001b[0m\n\u001b[0;32m    465\u001b[0m     \u001b[39mif\u001b[39;00m res \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    466\u001b[0m         msg_kwargs \u001b[39m=\u001b[39m res[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(res, \u001b[39mtuple\u001b[39m) \u001b[39melse\u001b[39;00m res\n\u001b[1;32m--> 467\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmessage(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmsg_kwargs)\n\u001b[0;32m    468\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_message_forward_hooks\u001b[39m.\u001b[39mvalues():\n\u001b[0;32m    469\u001b[0m     res \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, (msg_kwargs, ), out)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = GNN(hidden_channels=128) \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005, weight_decay=5e-4)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "def train(data_in):\n",
    "      model.train()\n",
    "      optimizer.zero_grad()  # Clear gradients.\n",
    "      out = model(data_in.x, data_in.edge_index, data_in.edge_attr, data_in.batch)  # Perform a single forward pass.\n",
    "\n",
    "      targets = data_in.y\n",
    "      loss = criterion(out, targets) \n",
    "\n",
    "      loss.backward()  # Derive gradients.\n",
    "      optimizer.step()  # Update parameters based on gradients.\n",
    "      return loss\n",
    "\n",
    "def test(data):\n",
    "      all_test_r2 = []\n",
    "      counter = -1    \n",
    "      for data_in in data:\n",
    "            counter += 1\n",
    "            model.eval()\n",
    "            out = model(data_in.x, data_in.edge_index, data_in.edge_attr, data_in.batch)\n",
    "\n",
    "            # Caculate R2\n",
    "            r2_score_var = []\n",
    "            for item in range(0,data_in.y.shape[1]):\n",
    "                  if item == 0:\n",
    "                        r2_score_var = r2_score(data_in.y[:,item].detach().numpy(), out[:,item].detach().numpy())\n",
    "                  else:\n",
    "                        r2_score_var = np.vstack((r2_score_var,(r2_score(data_in.y[:,item].detach().numpy(), out[:,item].detach().numpy()))))\n",
    "\n",
    "            if counter == 0:\n",
    "                  all_test_r2 = r2_score_var\n",
    "            else:\n",
    "                  all_test_r2 = np.hstack((all_test_r2,r2_score_var))\n",
    "\n",
    "      average_test_r2 = np.sum(all_test_r2,axis=1) / all_test_r2.shape[1]\n",
    "\n",
    "      return average_test_r2\n",
    "\n",
    "# Vectors to append accuracy to:\n",
    "Train_r2 = []\n",
    "Test_r2 = []\n",
    "\n",
    "# Calculate accuracy before training \n",
    "Train_r2.append(test(train_loader))\n",
    "Test_r2.append(test(test_loader))\n",
    "print(\"Initial training R2: \", Train_r2[0])\n",
    "print(\"Initial test R2: \", Test_r2[0])\n",
    "\n",
    "print_r2_option = True\n",
    "for epoch in range(1, 31):\n",
    "      average_loss = []\n",
    "      for data in train_loader:\n",
    "            loss = train(data)\n",
    "            average_loss.append(loss)\n",
    "      print(f'Epoch: {epoch:03d}, Loss: {(sum(average_loss)/len(average_loss)):.5f}')\n",
    "\n",
    "      if print_r2_option:\n",
    "\n",
    "            temp_train_r2 = test(train_loader)\n",
    "            Train_r2.append(temp_train_r2)\n",
    "\n",
    "            temp_test_r2 = test(test_loader)\n",
    "            Test_r2.append(temp_test_r2)\n",
    "\n",
    "            print(f'Average Train R2: {temp_train_r2}')\n",
    "            print(f'Average Test R2: {temp_test_r2:}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final loss:  0.26597893\n",
      "Final training R2:  [0.59518922 0.68653812 0.7042399  0.41493737 0.78617766 0.75829724\n",
      " 0.88722131 0.86579727 0.81688971 0.87568993 0.80171497 0.80172024\n",
      " 0.80172021 0.80170896 0.82218064]\n",
      "Final test R2:  [0.59661326 0.68558729 0.70455917 0.41200407 0.78386098 0.75963712\n",
      " 0.88553335 0.86111517 0.81385668 0.87675062 0.80447125 0.80447628\n",
      " 0.80447624 0.80446557 0.81996912]\n"
     ]
    }
   ],
   "source": [
    "print(\"Final loss: \", (sum(average_loss)/len(average_loss)).detach().numpy())\n",
    "print(\"Final training R2: \", Train_r2[-1])\n",
    "print(\"Final test R2: \", Test_r2[-1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model for just one target"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GNN function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_labels = 1\n",
    "data_features = dataset[50].x.shape[1]\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GATConv(data_features, hidden_channels)\n",
    "        self.conv2 = GATConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GATConv(hidden_channels, hidden_channels)\n",
    "        self.conv4 = GATConv(hidden_channels, hidden_channels)\n",
    "        self.conv5 = GATConv(hidden_channels, hidden_channels)\n",
    "        self.lin = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin2 = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin3 = Linear(hidden_channels, data_labels)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch): \n",
    "        x = self.conv1(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv4(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv5(x, edge_index, edge_attr)\n",
    "\n",
    "        #Returns batch-wise graph-level-outputs by averaging node features across the node dimension, so that for a single graph G\n",
    "        #its output is computed by\n",
    "        x = global_mean_pool(x, batch) \n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = F.relu(x)\n",
    "        x = self.lin2(x)\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = F.relu(x)\n",
    "        x = self.lin3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target index:  0\n",
      "Initial training R2:  -0.02350298644176647\n",
      "Initial test R2:  -0.02277312422248588\n",
      "Epoch: 001, Loss: 0.75478\n",
      "Epoch: 002, Loss: 0.61201\n",
      "Epoch: 003, Loss: 0.54524\n",
      "Epoch: 004, Loss: 0.50621\n",
      "Epoch: 005, Loss: 0.47388\n",
      "Epoch: 006, Loss: 0.45273\n",
      "Epoch: 007, Loss: 0.43445\n",
      "Epoch: 008, Loss: 0.41947\n",
      "Epoch: 009, Loss: 0.40344\n",
      "Epoch: 010, Loss: 0.39646\n",
      "Epoch: 011, Loss: 0.38529\n",
      "Epoch: 012, Loss: 0.37788\n",
      "Epoch: 013, Loss: 0.37201\n",
      "Epoch: 014, Loss: 0.36414\n",
      "Epoch: 015, Loss: 0.35948\n",
      "Epoch: 016, Loss: 0.35402\n",
      "Epoch: 017, Loss: 0.34916\n",
      "Epoch: 018, Loss: 0.34466\n",
      "Epoch: 019, Loss: 0.34472\n",
      "Epoch: 020, Loss: 0.34064\n",
      "Best training R2 for target 0: 0.6636045075432618\n",
      "Best test R2 for target 0: 0.6633040708373179\n",
      "Target index:  1\n",
      "Initial training R2:  -0.019556148457033976\n",
      "Initial test R2:  -0.020751837370101316\n",
      "Epoch: 001, Loss: 0.63477\n",
      "Epoch: 002, Loss: 0.51414\n",
      "Epoch: 003, Loss: 0.45997\n",
      "Epoch: 004, Loss: 0.41721\n",
      "Epoch: 005, Loss: 0.39401\n",
      "Epoch: 006, Loss: 0.37783\n",
      "Epoch: 007, Loss: 0.36235\n",
      "Epoch: 008, Loss: 0.35411\n",
      "Epoch: 009, Loss: 0.34534\n",
      "Epoch: 010, Loss: 0.34089\n",
      "Epoch: 011, Loss: 0.33671\n",
      "Epoch: 012, Loss: 0.33286\n",
      "Epoch: 013, Loss: 0.32874\n",
      "Epoch: 014, Loss: 0.32577\n",
      "Epoch: 015, Loss: 0.32382\n",
      "Epoch: 016, Loss: 0.31910\n",
      "Epoch: 017, Loss: 0.31706\n",
      "Epoch: 018, Loss: 0.31479\n",
      "Epoch: 019, Loss: 0.31168\n",
      "Epoch: 020, Loss: 0.31004\n",
      "Best training R2 for target 1: 0.710362291730381\n",
      "Best test R2 for target 1: 0.7092732408649669\n",
      "Target index:  2\n",
      "Initial training R2:  -0.021824641442938792\n",
      "Initial test R2:  -0.02061272224855382\n",
      "Epoch: 001, Loss: 0.57500\n",
      "Epoch: 002, Loss: 0.45826\n",
      "Epoch: 003, Loss: 0.43073\n",
      "Epoch: 004, Loss: 0.41353\n",
      "Epoch: 005, Loss: 0.39910\n",
      "Epoch: 006, Loss: 0.38831\n",
      "Epoch: 007, Loss: 0.37356\n",
      "Epoch: 008, Loss: 0.36167\n",
      "Epoch: 009, Loss: 0.34915\n",
      "Epoch: 010, Loss: 0.34099\n",
      "Epoch: 011, Loss: 0.33713\n",
      "Epoch: 012, Loss: 0.33379\n",
      "Epoch: 013, Loss: 0.32956\n",
      "Epoch: 014, Loss: 0.32639\n",
      "Epoch: 015, Loss: 0.32514\n",
      "Epoch: 016, Loss: 0.32283\n",
      "Epoch: 017, Loss: 0.32175\n",
      "Epoch: 018, Loss: 0.32020\n",
      "Epoch: 019, Loss: 0.31697\n",
      "Epoch: 020, Loss: 0.31718\n",
      "Best training R2 for target 2: 0.7008766990285974\n",
      "Best test R2 for target 2: 0.7010124687446676\n",
      "Target index:  3\n",
      "Initial training R2:  -0.018710509113034592\n",
      "Initial test R2:  -0.022724398850784068\n",
      "Epoch: 001, Loss: 0.61810\n",
      "Epoch: 002, Loss: 0.49544\n",
      "Epoch: 003, Loss: 0.47623\n",
      "Epoch: 004, Loss: 0.46633\n",
      "Epoch: 005, Loss: 0.45944\n",
      "Epoch: 006, Loss: 0.45410\n",
      "Epoch: 007, Loss: 0.45004\n",
      "Epoch: 008, Loss: 0.44626\n",
      "Epoch: 009, Loss: 0.44306\n",
      "Epoch: 010, Loss: 0.43983\n",
      "Epoch: 011, Loss: 0.43711\n",
      "Epoch: 012, Loss: 0.43509\n",
      "Epoch: 013, Loss: 0.43333\n",
      "Epoch: 014, Loss: 0.43097\n",
      "Epoch: 015, Loss: 0.43028\n",
      "Epoch: 016, Loss: 0.42788\n",
      "Epoch: 017, Loss: 0.42682\n",
      "Epoch: 018, Loss: 0.42539\n",
      "Epoch: 019, Loss: 0.42272\n",
      "Epoch: 020, Loss: 0.42126\n",
      "Best training R2 for target 3: 0.5816224007314774\n",
      "Best test R2 for target 3: 0.5791630189408662\n",
      "Target index:  4\n",
      "Initial training R2:  -0.017957503345227645\n",
      "Initial test R2:  -0.018826389038767075\n",
      "Epoch: 001, Loss: 0.46417\n",
      "Epoch: 002, Loss: 0.32335\n",
      "Epoch: 003, Loss: 0.30916\n",
      "Epoch: 004, Loss: 0.30213\n",
      "Epoch: 005, Loss: 0.29541\n",
      "Epoch: 006, Loss: 0.28687\n",
      "Epoch: 007, Loss: 0.27883\n",
      "Epoch: 008, Loss: 0.27022\n",
      "Epoch: 009, Loss: 0.26005\n",
      "Epoch: 010, Loss: 0.24980\n",
      "Epoch: 011, Loss: 0.24266\n",
      "Epoch: 012, Loss: 0.23694\n",
      "Epoch: 013, Loss: 0.23295\n",
      "Epoch: 014, Loss: 0.22987\n",
      "Epoch: 015, Loss: 0.22709\n",
      "Epoch: 016, Loss: 0.22480\n",
      "Epoch: 017, Loss: 0.22265\n",
      "Epoch: 018, Loss: 0.22084\n",
      "Epoch: 019, Loss: 0.21950\n",
      "Epoch: 020, Loss: 0.21777\n",
      "Best training R2 for target 4: 0.7927752196619405\n",
      "Best test R2 for target 4: 0.7924222418738837\n",
      "Target index:  5\n",
      "Initial training R2:  -0.01755465465413163\n",
      "Initial test R2:  -0.023018471160525807\n",
      "Epoch: 001, Loss: 0.53032\n",
      "Epoch: 002, Loss: 0.29399\n",
      "Epoch: 003, Loss: 0.24357\n",
      "Epoch: 004, Loss: 0.21905\n",
      "Epoch: 005, Loss: 0.20451\n",
      "Epoch: 006, Loss: 0.19665\n",
      "Epoch: 007, Loss: 0.19081\n",
      "Epoch: 008, Loss: 0.18457\n",
      "Epoch: 009, Loss: 0.17966\n",
      "Epoch: 010, Loss: 0.17690\n",
      "Epoch: 011, Loss: 0.17405\n",
      "Epoch: 012, Loss: 0.17119\n",
      "Epoch: 013, Loss: 0.16916\n",
      "Epoch: 014, Loss: 0.16848\n",
      "Epoch: 015, Loss: 0.16562\n",
      "Epoch: 016, Loss: 0.16453\n",
      "Epoch: 017, Loss: 0.16276\n",
      "Epoch: 018, Loss: 0.16151\n",
      "Epoch: 019, Loss: 0.16054\n",
      "Epoch: 020, Loss: 0.15932\n",
      "Best training R2 for target 5: 0.8511877798122606\n",
      "Best test R2 for target 5: 0.8473284678635936\n",
      "Target index:  6\n",
      "Initial training R2:  -0.017693073355085884\n",
      "Initial test R2:  -0.029767501941036573\n",
      "Epoch: 001, Loss: 0.34436\n",
      "Epoch: 002, Loss: 0.12944\n",
      "Epoch: 003, Loss: 0.08840\n",
      "Epoch: 004, Loss: 0.07860\n",
      "Epoch: 005, Loss: 0.07378\n",
      "Epoch: 006, Loss: 0.07152\n",
      "Epoch: 007, Loss: 0.06811\n",
      "Epoch: 008, Loss: 0.06611\n",
      "Epoch: 009, Loss: 0.06532\n",
      "Epoch: 010, Loss: 0.06388\n",
      "Epoch: 011, Loss: 0.06312\n",
      "Epoch: 012, Loss: 0.06176\n",
      "Epoch: 013, Loss: 0.06153\n",
      "Epoch: 014, Loss: 0.06104\n",
      "Epoch: 015, Loss: 0.06069\n",
      "Epoch: 016, Loss: 0.05960\n",
      "Epoch: 017, Loss: 0.05948\n",
      "Epoch: 018, Loss: 0.05878\n",
      "Epoch: 019, Loss: 0.05823\n",
      "Epoch: 020, Loss: 0.05815\n",
      "Best training R2 for target 6: 0.9559410397677668\n",
      "Best test R2 for target 6: 0.9537264209407537\n",
      "Target index:  7\n",
      "Initial training R2:  -0.018820415131039225\n",
      "Initial test R2:  -0.022009049036801006\n",
      "Epoch: 001, Loss: 0.36314\n",
      "Epoch: 002, Loss: 0.16107\n",
      "Epoch: 003, Loss: 0.12904\n",
      "Epoch: 004, Loss: 0.11872\n",
      "Epoch: 005, Loss: 0.11278\n",
      "Epoch: 006, Loss: 0.10945\n",
      "Epoch: 007, Loss: 0.10647\n",
      "Epoch: 008, Loss: 0.10392\n",
      "Epoch: 009, Loss: 0.10232\n",
      "Epoch: 010, Loss: 0.10107\n",
      "Epoch: 011, Loss: 0.10026\n",
      "Epoch: 012, Loss: 0.09859\n",
      "Epoch: 013, Loss: 0.09830\n",
      "Epoch: 014, Loss: 0.09739\n",
      "Epoch: 015, Loss: 0.09691\n",
      "Epoch: 016, Loss: 0.09564\n",
      "Epoch: 017, Loss: 0.09554\n",
      "Epoch: 018, Loss: 0.09498\n",
      "Epoch: 019, Loss: 0.09364\n",
      "Epoch: 020, Loss: 0.09369\n",
      "Best training R2 for target 7: 0.9202312796092724\n",
      "Best test R2 for target 7: 0.9169470184386495\n",
      "Target index:  8\n",
      "Initial training R2:  -0.020948827012665677\n",
      "Initial test R2:  -0.02090283884078734\n",
      "Epoch: 001, Loss: 0.48213\n",
      "Epoch: 002, Loss: 0.32532\n",
      "Epoch: 003, Loss: 0.29950\n",
      "Epoch: 004, Loss: 0.28426\n",
      "Epoch: 005, Loss: 0.27154\n",
      "Epoch: 006, Loss: 0.26100\n",
      "Epoch: 007, Loss: 0.25214\n",
      "Epoch: 008, Loss: 0.24312\n",
      "Epoch: 009, Loss: 0.23665\n",
      "Epoch: 010, Loss: 0.22948\n",
      "Epoch: 011, Loss: 0.22364\n",
      "Epoch: 012, Loss: 0.22178\n",
      "Epoch: 013, Loss: 0.21673\n",
      "Epoch: 014, Loss: 0.21423\n",
      "Epoch: 015, Loss: 0.21156\n",
      "Epoch: 016, Loss: 0.20910\n",
      "Epoch: 017, Loss: 0.20662\n",
      "Epoch: 018, Loss: 0.20556\n",
      "Epoch: 019, Loss: 0.20380\n",
      "Epoch: 020, Loss: 0.20272\n",
      "Best training R2 for target 8: 0.8025759432284453\n",
      "Best test R2 for target 8: 0.8032926968266698\n",
      "Target index:  9\n",
      "Initial training R2:  -0.018263924220073856\n",
      "Initial test R2:  -0.022171209111824814\n",
      "Epoch: 001, Loss: 0.21317\n",
      "Epoch: 002, Loss: 0.09035\n",
      "Epoch: 003, Loss: 0.08700\n",
      "Epoch: 004, Loss: 0.08580\n",
      "Epoch: 005, Loss: 0.08435\n",
      "Epoch: 006, Loss: 0.08302\n",
      "Epoch: 007, Loss: 0.08181\n",
      "Epoch: 008, Loss: 0.08136\n",
      "Epoch: 009, Loss: 0.08078\n",
      "Epoch: 010, Loss: 0.08028\n",
      "Epoch: 011, Loss: 0.08021\n",
      "Epoch: 012, Loss: 0.07950\n",
      "Epoch: 013, Loss: 0.07899\n",
      "Epoch: 014, Loss: 0.07926\n",
      "Epoch: 015, Loss: 0.07801\n",
      "Epoch: 016, Loss: 0.07760\n",
      "Epoch: 017, Loss: 0.07733\n",
      "Epoch: 018, Loss: 0.07753\n",
      "Epoch: 019, Loss: 0.07727\n",
      "Epoch: 020, Loss: 0.07688\n",
      "Best training R2 for target 9: 0.936628410369879\n",
      "Best test R2 for target 9: 0.9373772248321128\n",
      "Target index:  10\n",
      "Initial training R2:  -0.018265002867474192\n",
      "Initial test R2:  -0.019224871491389576\n",
      "Epoch: 001, Loss: 0.46594\n",
      "Epoch: 002, Loss: 0.33912\n",
      "Epoch: 003, Loss: 0.32996\n",
      "Epoch: 004, Loss: 0.32415\n",
      "Epoch: 005, Loss: 0.31935\n",
      "Epoch: 006, Loss: 0.31353\n",
      "Epoch: 007, Loss: 0.30456\n",
      "Epoch: 008, Loss: 0.29546\n",
      "Epoch: 009, Loss: 0.28375\n",
      "Epoch: 010, Loss: 0.27180\n",
      "Epoch: 011, Loss: 0.26105\n",
      "Epoch: 012, Loss: 0.25143\n",
      "Epoch: 013, Loss: 0.24669\n",
      "Epoch: 014, Loss: 0.24145\n",
      "Epoch: 015, Loss: 0.23915\n",
      "Epoch: 016, Loss: 0.23547\n",
      "Epoch: 017, Loss: 0.23311\n",
      "Epoch: 018, Loss: 0.23100\n",
      "Epoch: 019, Loss: 0.23059\n",
      "Epoch: 020, Loss: 0.22881\n",
      "Best training R2 for target 10: 0.7861361496541605\n",
      "Best test R2 for target 10: 0.7931090209326379\n",
      "Target index:  11\n",
      "Initial training R2:  -0.01826497098020484\n",
      "Initial test R2:  -0.01922484354631408\n",
      "Epoch: 001, Loss: 0.46593\n",
      "Epoch: 002, Loss: 0.33911\n",
      "Epoch: 003, Loss: 0.32995\n",
      "Epoch: 004, Loss: 0.32416\n",
      "Epoch: 005, Loss: 0.31933\n",
      "Epoch: 006, Loss: 0.31352\n",
      "Epoch: 007, Loss: 0.30457\n",
      "Epoch: 008, Loss: 0.29560\n",
      "Epoch: 009, Loss: 0.28393\n",
      "Epoch: 010, Loss: 0.27248\n",
      "Epoch: 011, Loss: 0.26204\n",
      "Epoch: 012, Loss: 0.25232\n",
      "Epoch: 013, Loss: 0.24732\n",
      "Epoch: 014, Loss: 0.24183\n",
      "Epoch: 015, Loss: 0.23928\n",
      "Epoch: 016, Loss: 0.23557\n",
      "Epoch: 017, Loss: 0.23297\n",
      "Epoch: 018, Loss: 0.23100\n",
      "Epoch: 019, Loss: 0.23035\n",
      "Epoch: 020, Loss: 0.22858\n",
      "Best training R2 for target 11: 0.7864241810548465\n",
      "Best test R2 for target 11: 0.7934040176933929\n",
      "Target index:  12\n",
      "Initial training R2:  -0.018264971775211955\n",
      "Initial test R2:  -0.01922484317582709\n",
      "Epoch: 001, Loss: 0.46593\n",
      "Epoch: 002, Loss: 0.33908\n",
      "Epoch: 003, Loss: 0.32990\n",
      "Epoch: 004, Loss: 0.32411\n",
      "Epoch: 005, Loss: 0.31922\n",
      "Epoch: 006, Loss: 0.31341\n",
      "Epoch: 007, Loss: 0.30430\n",
      "Epoch: 008, Loss: 0.29517\n",
      "Epoch: 009, Loss: 0.28335\n",
      "Epoch: 010, Loss: 0.27212\n",
      "Epoch: 011, Loss: 0.26227\n",
      "Epoch: 012, Loss: 0.25312\n",
      "Epoch: 013, Loss: 0.24844\n",
      "Epoch: 014, Loss: 0.24313\n",
      "Epoch: 015, Loss: 0.24054\n",
      "Epoch: 016, Loss: 0.23677\n",
      "Epoch: 017, Loss: 0.23433\n",
      "Epoch: 018, Loss: 0.23194\n",
      "Epoch: 019, Loss: 0.23176\n",
      "Epoch: 020, Loss: 0.22997\n",
      "Best training R2 for target 12: 0.7852140869439007\n",
      "Best test R2 for target 12: 0.7924508923844775\n",
      "Target index:  13\n",
      "Initial training R2:  -0.018265033190665834\n",
      "Initial test R2:  -0.01922489711406268\n",
      "Epoch: 001, Loss: 0.46593\n",
      "Epoch: 002, Loss: 0.33905\n",
      "Epoch: 003, Loss: 0.32990\n",
      "Epoch: 004, Loss: 0.32412\n",
      "Epoch: 005, Loss: 0.31930\n",
      "Epoch: 006, Loss: 0.31347\n",
      "Epoch: 007, Loss: 0.30455\n",
      "Epoch: 008, Loss: 0.29551\n",
      "Epoch: 009, Loss: 0.28370\n",
      "Epoch: 010, Loss: 0.27224\n",
      "Epoch: 011, Loss: 0.26231\n",
      "Epoch: 012, Loss: 0.25317\n",
      "Epoch: 013, Loss: 0.24864\n",
      "Epoch: 014, Loss: 0.24346\n",
      "Epoch: 015, Loss: 0.24090\n",
      "Epoch: 016, Loss: 0.23691\n",
      "Epoch: 017, Loss: 0.23411\n",
      "Epoch: 018, Loss: 0.23162\n",
      "Epoch: 019, Loss: 0.23119\n",
      "Epoch: 020, Loss: 0.22964\n",
      "Best training R2 for target 13: 0.7856129193654282\n",
      "Best test R2 for target 13: 0.7928930646215756\n",
      "Target index:  14\n",
      "Initial training R2:  -0.018903465486760643\n",
      "Initial test R2:  -0.02098441479975519\n",
      "Epoch: 001, Loss: 0.37149\n",
      "Epoch: 002, Loss: 0.26077\n",
      "Epoch: 003, Loss: 0.25360\n",
      "Epoch: 004, Loss: 0.24857\n",
      "Epoch: 005, Loss: 0.24409\n",
      "Epoch: 006, Loss: 0.23853\n",
      "Epoch: 007, Loss: 0.23384\n",
      "Epoch: 008, Loss: 0.22966\n",
      "Epoch: 009, Loss: 0.22405\n",
      "Epoch: 010, Loss: 0.21772\n",
      "Epoch: 011, Loss: 0.21222\n",
      "Epoch: 012, Loss: 0.20485\n",
      "Epoch: 013, Loss: 0.19908\n",
      "Epoch: 014, Loss: 0.19533\n",
      "Epoch: 015, Loss: 0.19160\n",
      "Epoch: 016, Loss: 0.18766\n",
      "Epoch: 017, Loss: 0.18527\n",
      "Epoch: 018, Loss: 0.18402\n",
      "Epoch: 019, Loss: 0.18232\n",
      "Epoch: 020, Loss: 0.18151\n",
      "Best training R2 for target 14: 0.8330250216457179\n",
      "Best test R2 for target 14: 0.8348749141832345\n"
     ]
    }
   ],
   "source": [
    "def train(data_in, target):\n",
    "      model.train()\n",
    "      optimizer.zero_grad()  # Clear gradients.\n",
    "      out = model(data_in.x, data_in.edge_index, data_in.edge_attr, data_in.batch)\n",
    "      targets = data_in.y[:,target].reshape(-1,1)\n",
    "      \n",
    "      #Alt 1\n",
    "      loss = criterion(out, targets)   \n",
    "\n",
    "      loss.backward()  # Derive gradients.\n",
    "      optimizer.step()  # Update parameters based on gradients.\n",
    "      return loss\n",
    "\n",
    "def test(data, target):\n",
    "      all_test_r2 = []\n",
    "      counter = -1    \n",
    "      for data_in in data:\n",
    "            counter += 1\n",
    "            model.eval()\n",
    "            out = model(data_in.x, data_in.edge_index, data_in.edge_attr, data_in.batch)\n",
    "            targets = data_in.y[:,target].reshape(-1,1)\n",
    "            \n",
    "            # Caculate R2\n",
    "            r2_score_var = r2_score(targets.detach().numpy(), out.detach().numpy())\n",
    "\n",
    "            all_test_r2 .append(r2_score_var)\n",
    "\n",
    "      average_test_r2 = np.sum(all_test_r2) / len(all_test_r2)\n",
    "\n",
    "      return average_test_r2\n",
    "\n",
    "num_targets = dataset[50].y.shape[1]\n",
    "for target_index in range(num_targets):\n",
    "      print(\"Target index: \", target_index)\n",
    "\n",
    "      model = GNN(hidden_channels=64) \n",
    "      optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=5e-4)\n",
    "      criterion = torch.nn.MSELoss()\n",
    "\n",
    "      # Vectors to append accuracy to:\n",
    "      Train_r2 = []\n",
    "      Test_r2 = []\n",
    "\n",
    "      # Calculate accuracy before training \n",
    "      Train_r2.append(test(train_loader, target_index))\n",
    "      Test_r2.append(test(test_loader, target_index))\n",
    "      print(\"Initial training R2: \", Train_r2[0])\n",
    "      print(\"Initial test R2: \", Test_r2[0])\n",
    "\n",
    "      print_r2_option = True\n",
    "      for epoch in range(1, 21):\n",
    "            average_loss = []\n",
    "            for data in train_loader:\n",
    "                  loss = train(data, target_index)\n",
    "                  average_loss.append(loss)\n",
    "            print(f'Epoch: {epoch:03d}, Loss: {(sum(average_loss)/len(average_loss)):.5f}')\n",
    "\n",
    "            if print_r2_option:\n",
    "                  temp_train_r2 = test(train_loader, target_index)\n",
    "                  Train_r2.append(temp_train_r2)\n",
    "\n",
    "                  temp_test_r2 = test(test_loader, target_index)\n",
    "                  Test_r2.append(temp_test_r2)\n",
    "\n",
    "                  # print(f'Average Train R2: {temp_train_r2}')\n",
    "                  # print(f'Average Test R2: {temp_test_r2:}')\n",
    "\n",
    "      print(f\"Best training R2 for target {target_index}: {np.max(Train_r2)}\")\n",
    "      print(f\"Best test R2 for target {target_index}: {np.max(Test_r2)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
