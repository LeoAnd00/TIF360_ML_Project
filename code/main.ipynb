{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TIF360 Project"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main source: https://www.kaggle.com/code/rmonge/predicting-molecule-properties-based-on-its-smiles/notebook"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import rdkit\n",
    "from rdkit import Chem  # To extract information of the molecules\n",
    "from rdkit.Chem import Draw  # To draw the molecules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch_geometric.utils as utils\n",
    "import networkx as nx\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import global_mean_pool, GraphConv, GATConv, GCNConv\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/smiles_and_targets.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mol_descriptor = np.load(\"../data/mol_descriptors.npy\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigate Neural Networks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Neural Networks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert data to graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "from rdkit.Chem import GetAdjacencyMatrix\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(x, permitted_list):\n",
    "    if x not in permitted_list:\n",
    "        x = permitted_list[-1]  # If the atom is not in the list, get \"Unknown\"\n",
    "        \n",
    "    binary_encoding = [int(boolean) for boolean in list(map(lambda s: x==s, permitted_list))]\n",
    "    \n",
    "    return binary_encoding    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Atom featurisation\\\n",
    "Currently generates ca. 80 node features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_atom_features(atom, use_chirality = True, hydrogens_implicit = True):\n",
    "    # list of permitted atoms\n",
    "    permitted_atom_list = ['C','N','O','S','F','Si','P','Cl','Br','Mg','Na','Ca',\n",
    "                           'Fe','As','Al','I', 'B','V','K','Tl','Yb','Sb','Sn','Ag','Pd','Co',\n",
    "                           'Se','Ti','Zn', 'Li','Ge','Cu','Au','Ni','Cd','In','Mn','Zr','Cr','Pt',\n",
    "                           'Hg','Pb','Unknown']\n",
    "    atom_type_enc = one_hot_encoding(str(atom.GetSymbol()), permitted_atom_list)\n",
    "    \n",
    "    n_heavy_neighbors = one_hot_encoding(int(atom.GetDegree()), [0,1,2,3,4,\"MoreThanFour\"])\n",
    "    \n",
    "    formal_charge_enc = one_hot_encoding(int(atom.GetFormalCharge()), [-3, -2, -1, 0, 1, 2, 3, 'Extreme'])\n",
    "    \n",
    "    hybridisation_type_enc = one_hot_encoding(str(atom.GetHybridization()), [\"S\", \"SP\", \"SP2\", \"SP3\", \"SP3D\", \"SP3D2\", \"OTHER\"])\n",
    "    \n",
    "    is_in_ring_enc = one_hot_encoding(int(atom.IsInRing()), [0, 1])\n",
    "    \n",
    "    is_aromatic_enc = one_hot_encoding(int(atom.GetIsAromatic()), [0, 1])\n",
    "    \n",
    "    atomic_mass_scaled = [float(atom.GetMass() - 10.812)/116.092] # (?) replace 10.812 with mean the and 116.092 with std\n",
    "    \n",
    "    vdw_radius_scaled = [float((Chem.GetPeriodicTable().GetRvdw(atom.GetAtomicNum()) - 1.5)/0.6)] # (?) replace 1.5 with mean the and 0.6 with std\n",
    "    \n",
    "    covalent_radius_scaled = [float((Chem.GetPeriodicTable().GetRcovalent(atom.GetAtomicNum()) - 0.64)/0.76)] # (?) replace 0.64 with mean the and 0.76 with std\n",
    "                              \n",
    "    atom_feature_vector = atom_type_enc + n_heavy_neighbors + formal_charge_enc + hybridisation_type_enc + is_in_ring_enc + is_aromatic_enc + atomic_mass_scaled + vdw_radius_scaled + covalent_radius_scaled\n",
    "    \n",
    "    if use_chirality:\n",
    "        chirality_type_enc = one_hot_encoding(str(atom.GetChiralTag()), [\"CHI_UNSPECIFIED\", \"CHI_TETRAHEDRAL_CW\", \"CHI_TETRAHEDRAL_CCW\", \"CHI_OTHER\"])\n",
    "        atom_feature_vector += chirality_type_enc\n",
    "        \n",
    "    if hydrogens_implicit:\n",
    "        n_hydrogens_enc = one_hot_encoding(int(atom.GetTotalNumHs()), [0, 1, 2, 3, 4, \"MoreThanFour\"])\n",
    "        atom_feature_vector += n_hydrogens_enc\n",
    "        \n",
    "    return np.array(atom_feature_vector) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bond Featurisation\\\n",
    "Currently generates ca. 10 edge features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bond_features(bond, use_stereochemistry=True):\n",
    "    permitted_bond_types = [Chem.rdchem.BondType.SINGLE, Chem.rdchem.BondType.DOUBLE, \n",
    "                            Chem.rdchem.BondType.TRIPLE, Chem.rdchem.BondType.AROMATIC]\n",
    "    \n",
    "    bond_type_enc = one_hot_encoding(str(bond.GetBondType()), permitted_bond_types)\n",
    "    \n",
    "    bond_is_conjugated_enc = [int(bond.GetIsConjugated())]\n",
    "    \n",
    "    bond_is_in_ring_enc = [int(bond.IsInRing())]\n",
    "    \n",
    "    bond_feature_vector = bond_type_enc + bond_is_conjugated_enc + bond_is_in_ring_enc\n",
    "    \n",
    "    if use_stereochemistry:\n",
    "        stereo_type_enc = one_hot_encoding(str(bond.GetStereo()), [\"STEREOZ\", \"STEREOE\", \"STEREOANY\", \"STEREONONE\"])\n",
    "        bond_feature_vector += stereo_type_enc\n",
    "        \n",
    "    return np.array(bond_feature_vector)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function to generate dataset of labeled Pytorch Geometric Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph_dataset_from_smiles(x_smiles, y):\n",
    "    ## Inputs:\n",
    "    # x_smiles = [smiles_1, smiles_2, ...], smiles representation of molecules\n",
    "    # y = [y_1, y_2, ...] list of numerical labels for each smiles string, here chemical properties\n",
    "    \n",
    "    # Outputs:\n",
    "    # dataset = [data_1, data_2, ...] list of torch_geometric.data.Data objects representing molecular graphs\n",
    "    \n",
    "    dataset = []\n",
    "    \n",
    "    for (smiles, y_val) in zip(x_smiles, y):\n",
    "        # convert smiles to molecular object\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        \n",
    "        # get feature dimensions\n",
    "        n_nodes = mol.GetNumAtoms()\n",
    "        n_edges = 2*mol.GetNumBonds() # each bond is represented twice in the adjacency matrix\n",
    "        n_node_features = len(get_atom_features(mol.GetAtomWithIdx(0)))\n",
    "        if n_nodes > 1:\n",
    "            n_edge_features = len(get_bond_features(mol.GetBondBetweenAtoms(0,1)))\n",
    "        else:\n",
    "            n_edge_features = 0  # for single atom molecules -> no edges\n",
    "        \n",
    "        # construct node feature matrix X \n",
    "        X = np.zeros((n_nodes, n_node_features))\n",
    "        \n",
    "        for atom in mol.GetAtoms():\n",
    "            X[atom.GetIdx(), :] = get_atom_features(atom)\n",
    "        \n",
    "        X = torch.tensor(X, dtype=torch.float)\n",
    "        \n",
    "        # construct edge index array E, shape = (2, n_edges)\n",
    "        (rows, cols) = np.nonzero(GetAdjacencyMatrix(mol))\n",
    "        torch_rows = torch.tensor(rows.astype(np.int64)).to(torch.long)\n",
    "        torch_cols = torch.tensor(cols.astype(np.int64)).to(torch.long)\n",
    "        E = torch.stack([torch_rows, torch_cols], dim=0)\n",
    "        \n",
    "        # construct edge feature matrix EF\n",
    "        EF = np.zeros((n_edges, n_edge_features))       # Note: generates zero matrix if n_edges = n_edge_features = 0\n",
    "        for (k, (i,j)) in enumerate(zip(rows, cols)):\n",
    "            EF[k] = get_bond_features(mol.GetBondBetweenAtoms(int(i),int(j)))\n",
    "        EF = torch.tensor(EF, dtype=torch.float)\n",
    "        \n",
    "        # construct label/y tensor\n",
    "        y_tensor = torch.tensor(np.array([y_val]), dtype=torch.float)\n",
    "        \n",
    "        # construct torch_geometric.data.Data object and append to dataset\n",
    "        dataset.append(Data(x=X, edge_index=E, edge_attr=EF, y=y_tensor))\n",
    "        \n",
    "    return dataset\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the above functions to create a dataset of molecular graphs from the smiles and labels corresponding to chemical properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties_names = ['A', 'B', 'C', 'mu', 'alfa', 'homo', 'lumo', 'gap', 'R²', 'zpve', 'U0', 'U', 'H', 'G', 'Cv']\n",
    "\n",
    "x_smiles = df.smiles.values\n",
    "y = df.loc[:, properties_names].values  # shape = (n_samples, n_properties)\n",
    "\n",
    "dataset = create_graph_dataset_from_smiles(x_smiles, y[0:len(x_smiles), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133794\n",
      "C\n",
      "Data(x=[1, 81], edge_index=[2, 0], edge_attr=[0, 0], y=[1, 15])\n",
      "O\n",
      "Data(x=[1, 81], edge_index=[2, 0], edge_attr=[0, 0], y=[1, 15])\n",
      "N1C=CN=C1\n",
      "Data(x=[5, 81], edge_index=[2, 10], edge_attr=[10, 10], y=[1, 15])\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))\n",
    "\n",
    "# Example entries\n",
    "print(df.smiles.values[0])\n",
    "print(dataset[0])\n",
    "print(df.smiles.values[2])\n",
    "print(dataset[2])\n",
    "print(df.smiles.values[50])\n",
    "print(dataset[50])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information of the graph dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of graphs (molecules): 133794\n",
      "=================================================================================\n",
      "Properties of graph 50, molecule smiles: N1C=CN=C1\n",
      "Number of nodes: 5\n",
      "Number of edges: 10\n",
      "Number of node features: 81\n",
      "Number of edge features: 10\n",
      "Number of properties: 15\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of graphs (molecules): {len(dataset)}')\n",
    "graph = dataset[50]\n",
    "print('=================================================================================')\n",
    "print(f'Properties of graph {50}, molecule smiles: {df.smiles.values[50]}')\n",
    "print(f'Number of nodes: {dataset[50].x.shape[0]}')\n",
    "print(f'Number of edges: {dataset[50].edge_index.shape[1]}')\n",
    "print(f'Number of node features: {dataset[50].x.shape[1]}')\n",
    "print(f'Number of edge features: {dataset[50].edge_attr.shape[1]}')\n",
    "print(f'Number of properties: {dataset[50].y.shape[1]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filterout data with no edge features defined (Like ex: CH4) (These causes problems down the line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of none edge feature molecules:  3\n",
      "Before:  133794\n",
      "Molecule to delete:  C\n",
      "Molecule to delete:  N\n",
      "Molecule to delete:  O\n",
      "After:  133791\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "indexes_to_delete = []\n",
    "for item in range(0,len(dataset)):\n",
    "    if dataset[item].edge_attr.shape[1] == 0:\n",
    "        indexes_to_delete.append(item)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "indexes_to_delete.sort()\n",
    "\n",
    "print(\"Number of none edge feature molecules: \", len(indexes_to_delete))\n",
    "\n",
    "print(\"Before: \", len(dataset))\n",
    "\n",
    "for item in range(0,len(indexes_to_delete)):\n",
    "    print(\"Molecule to delete: \", df.smiles.values[indexes_to_delete[item]])\n",
    "    #del dataset[indexes_to_delete[item] - item] \n",
    "    dataset.pop((indexes_to_delete[item] - item)) # -item since all future data points will have its index reduced by 1 for each deleted previous data point\n",
    "print(\"After: \", len(dataset))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example train data target before scaling tensor([[ 2.9447e+00,  2.6734e+00,  1.6819e+00,  7.0850e-01,  7.0340e+01,\n",
      "         -2.4000e-01,  5.5900e-02,  2.9590e-01,  8.0710e+02,  1.4671e-01,\n",
      "         -2.7254e+02, -2.7254e+02, -2.7254e+02, -2.7258e+02,  2.9530e+01]])\n",
      "Example train data target after scaling: tensor([[-2.2742e-01,  2.5325e+00,  1.4954e+00, -1.3102e+00, -5.9178e-01,\n",
      "         -1.2331e-03,  9.5616e-01,  9.4529e-01, -1.3664e+00, -5.2366e-02,\n",
      "          3.4719e+00,  3.4719e+00,  3.4719e+00,  3.4719e+00, -5.0999e-01]])\n",
      "Total data size:  133791\n",
      "Train data size:  107033\n",
      "Test data size:  26758\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "\n",
    "# split the dataset into test and validation:\n",
    "num_samples = len(dataset)\n",
    "\n",
    "# Want to divide data randomly\n",
    "random.seed(42)\n",
    "random_indexes = np.array(random.sample(range(num_samples), num_samples)) # random.sample ensures no duplicates\n",
    "\n",
    "train_data = [dataset[index] for index in random_indexes[int(.2 * num_samples ) :]] # 80%\n",
    "test_data = [dataset[index] for index in random_indexes[: int(.2 * num_samples )]] # 20%\n",
    "\n",
    "print(\"Example train data target before scaling\", train_data[0].y)\n",
    "train_data_targets = [data.y for data in train_data]\n",
    "train_data_targets = torch.concatenate(train_data_targets, axis=0)\n",
    "test_data_targets = [data.y for data in test_data]\n",
    "test_data_targets = torch.concatenate(test_data_targets, axis=0)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_data_targets = scaler.fit_transform(train_data_targets)\n",
    "test_data_targets = scaler.transform(test_data_targets)\n",
    "\n",
    "# print(\"Example train data target\", train_data_targets[0].reshape(1,-1).shape)\n",
    "train_data_targets = torch.tensor(train_data_targets, dtype=torch.float)\n",
    "test_data_targets = torch.tensor(test_data_targets, dtype=torch.float)\n",
    "\n",
    "train_data = [Data(x=data.x, edge_index=data.edge_index, edge_attr=data.edge_attr, y=train_data_targets[index].reshape(1,-1)) for index, data in enumerate(train_data)]\n",
    "test_data = [Data(x=data.x, edge_index=data.edge_index, edge_attr=data.edge_attr, y=test_data_targets[index].reshape(1,-1)) for index, data in enumerate(test_data)]\n",
    "print(\"Example train data target after scaling:\", train_data[0].y)\n",
    "\n",
    "print(\"Total data size: \", len(dataset))\n",
    "print(\"Train data size: \", len(train_data))\n",
    "print(\"Test data size: \", len(test_data))\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main GNN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model for all targets at once"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GNN function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_labels = dataset[50].y.shape[1]\n",
    "data_features = dataset[50].x.shape[1]\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GATConv(data_features, hidden_channels)\n",
    "        self.conv2 = GATConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GATConv(hidden_channels, hidden_channels)\n",
    "        self.conv4 = GATConv(hidden_channels, hidden_channels)\n",
    "        self.conv5 = GATConv(hidden_channels, hidden_channels)\n",
    "        self.lin = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin2 = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin3 = Linear(hidden_channels, data_labels)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch): \n",
    "        x = self.conv1(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv4(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv5(x, edge_index, edge_attr)\n",
    "\n",
    "        #Returns batch-wise graph-level-outputs by averaging node features across the node dimension, so that for a single graph G\n",
    "        #its output is computed by\n",
    "        x = global_mean_pool(x, batch) \n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = F.relu(x)\n",
    "        x = self.lin2(x)\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = F.relu(x)\n",
    "        x = self.lin3(x)\n",
    "        \n",
    " \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial training R2:  [-0.02034011 -0.0172521  -0.01965901 -0.02143906 -0.02006519 -0.02162258\n",
      " -0.01593567 -0.0154436  -0.02292025 -0.01775128 -0.01767568 -0.02199647\n",
      " -0.01765895 -0.02587157 -0.0158652 ]\n",
      "Initial test R2:  [-0.02112639 -0.0189632  -0.02089278 -0.01996619 -0.02216537 -0.02919588\n",
      " -0.01552276 -0.01521761 -0.02419008 -0.01688866 -0.02127208 -0.02363355\n",
      " -0.02153127 -0.03111836 -0.0177583 ]\n",
      "Epoch: 001, Loss: 0.55871\n",
      "Average Train R2: [0.1951949  0.40794601 0.49197687 0.27392301 0.61148099 0.24410877\n",
      " 0.70684006 0.67683428 0.65217574 0.8434309  0.64427631 0.64414196\n",
      " 0.64437309 0.64420164 0.64811555]\n",
      "Average Test R2: [0.19599081 0.40957752 0.50026938 0.2730022  0.62078261 0.23422642\n",
      " 0.70818947 0.6783536  0.6576467  0.84765732 0.64565344 0.64572373\n",
      " 0.64576428 0.64557926 0.65612875]\n",
      "Epoch: 002, Loss: 0.43956\n",
      "Average Train R2: [0.18418352 0.46900893 0.53259671 0.33391267 0.63810736 0.59475037\n",
      " 0.77239994 0.74607857 0.67022887 0.85018634 0.7083865  0.70832622\n",
      " 0.70853644 0.70843014 0.62045859]\n",
      "Average Test R2: [0.18126944 0.47029062 0.53646615 0.32769921 0.64299543 0.58717954\n",
      " 0.7749469  0.74531422 0.66986764 0.85488087 0.71228885 0.7122734\n",
      " 0.71243754 0.71233193 0.62620385]\n",
      "Epoch: 003, Loss: 0.39309\n",
      "Average Train R2: [0.17235503 0.53744957 0.60059462 0.33553109 0.691923   0.63097385\n",
      " 0.80295955 0.78288922 0.73842313 0.86487903 0.73657593 0.73632371\n",
      " 0.7363538  0.7367     0.71513588]\n",
      "Average Test R2: [0.16271782 0.54047792 0.60457821 0.32942635 0.69028732 0.62474927\n",
      " 0.80353516 0.78166622 0.73882398 0.86860685 0.73919149 0.73901221\n",
      " 0.7389644  0.73931259 0.71974859]\n",
      "Epoch: 004, Loss: 0.36165\n",
      "Average Train R2: [0.272419   0.56864963 0.62975884 0.34990921 0.69018926 0.68055298\n",
      " 0.86015534 0.82700691 0.74672949 0.84751741 0.74064112 0.74060794\n",
      " 0.7406297  0.7405664  0.76327449]\n",
      "Average Test R2: [0.18939784 0.57447459 0.63441742 0.34451737 0.69145895 0.67662817\n",
      " 0.85969057 0.82434021 0.74086337 0.85101941 0.74360928 0.74362369\n",
      " 0.74363706 0.74356348 0.7668364 ]\n",
      "Epoch: 005, Loss: 0.34319\n",
      "Average Train R2: [0.31130304 0.58960181 0.64881946 0.35151644 0.69019019 0.70388823\n",
      " 0.83249269 0.78720331 0.75039682 0.85497551 0.75385112 0.75381037\n",
      " 0.75385232 0.75384036 0.77935661]\n",
      "Average Test R2: [0.28692436 0.59106569 0.654961   0.3473475  0.69362223 0.69810289\n",
      " 0.83337666 0.78756695 0.74995791 0.85974472 0.75476804 0.75471671\n",
      " 0.75477503 0.75475938 0.78274044]\n",
      "Epoch: 006, Loss: 0.33626\n",
      "Average Train R2: [0.33552524 0.59645383 0.65909093 0.35980137 0.70514855 0.72110741\n",
      " 0.89574264 0.85433893 0.75494406 0.86678433 0.74146257 0.74144566\n",
      " 0.74141135 0.74137926 0.78492953]\n",
      "Average Test R2: [0.24641108 0.60272086 0.6650988  0.3499707  0.70783318 0.71365295\n",
      " 0.89361955 0.85041866 0.75383069 0.86991616 0.74149687 0.7414528\n",
      " 0.74146516 0.74143726 0.78490244]\n",
      "Epoch: 007, Loss: 0.32995\n",
      "Average Train R2: [0.36414067 0.60989695 0.66590346 0.36930788 0.70616558 0.6922669\n",
      " 0.90090011 0.85920511 0.77529296 0.86917055 0.76362175 0.76358241\n",
      " 0.76364321 0.76361265 0.76391127]\n",
      "Average Test R2: [0.37121407 0.61447235 0.67290106 0.36488064 0.70933636 0.68125258\n",
      " 0.90025473 0.85582371 0.77112218 0.87097644 0.76671948 0.76668733\n",
      " 0.76674144 0.76670639 0.76528848]\n",
      "Epoch: 008, Loss: 0.31993\n",
      "Average Train R2: [0.16852363 0.60452345 0.65906001 0.3711694  0.7164115  0.72901622\n",
      " 0.90184124 0.86392394 0.77744763 0.87380208 0.76245164 0.7623912\n",
      " 0.76244668 0.76242288 0.78528739]\n",
      "Average Test R2: [0.12388105 0.60910016 0.66782    0.36576675 0.71362578 0.72420828\n",
      " 0.90102375 0.86110631 0.7754713  0.87541523 0.76171834 0.76165359\n",
      " 0.76170709 0.76168649 0.78707963]\n",
      "Epoch: 009, Loss: 0.31977\n",
      "Average Train R2: [0.37987858 0.600437   0.64717692 0.38825346 0.72459781 0.74102384\n",
      " 0.91022575 0.86969974 0.75912496 0.86018836 0.76029288 0.76033604\n",
      " 0.76026538 0.76025112 0.79630482]\n",
      "Average Test R2: [0.35289458 0.58738106 0.6398183  0.38397284 0.72885803 0.73495655\n",
      " 0.91008953 0.86728151 0.7480815  0.86348243 0.76356875 0.76360591\n",
      " 0.76354339 0.76352878 0.7979758 ]\n",
      "Epoch: 010, Loss: 0.31171\n",
      "Average Train R2: [0.39474132 0.59705599 0.66779695 0.39436692 0.73617019 0.73721435\n",
      " 0.89322328 0.83959997 0.77220765 0.85051054 0.7497209  0.7497544\n",
      " 0.74965606 0.74968376 0.78759599]\n",
      "Average Test R2: [0.40537541 0.60085523 0.67400877 0.38691685 0.73859951 0.73243357\n",
      " 0.89334588 0.83860963 0.76989249 0.85426426 0.74956262 0.74958585\n",
      " 0.74949414 0.74952738 0.78932113]\n",
      "Epoch: 011, Loss: 0.31355\n",
      "Average Train R2: [0.38561867 0.59623429 0.65331499 0.39582121 0.75194388 0.74404393\n",
      " 0.90814149 0.86714724 0.7614591  0.87687388 0.7661169  0.76609641\n",
      " 0.766122   0.7661125  0.79070763]\n",
      "Average Test R2: [0.37076525 0.60179738 0.65826368 0.38528253 0.75361126 0.7383757\n",
      " 0.90674452 0.863355   0.76201728 0.87947067 0.76595907 0.76593547\n",
      " 0.76596176 0.76595533 0.79189389]\n",
      "Epoch: 012, Loss: 0.30618\n",
      "Average Train R2: [0.39185507 0.61783778 0.66156941 0.3985699  0.7549713  0.75317164\n",
      " 0.9135343  0.87246957 0.76834033 0.86798699 0.76644122 0.7664279\n",
      " 0.76644365 0.76642151 0.80398203]\n",
      "Average Test R2: [0.23924958 0.62408006 0.66575362 0.39246281 0.75557694 0.74814418\n",
      " 0.91336347 0.87033874 0.76255184 0.86990374 0.76419531 0.76418023\n",
      " 0.76419638 0.76417669 0.80491138]\n",
      "Epoch: 013, Loss: 0.31052\n",
      "Average Train R2: [0.31100176 0.61672656 0.67214949 0.39779735 0.75036117 0.73523186\n",
      " 0.91440905 0.87347605 0.77688378 0.87411148 0.77750314 0.77751127\n",
      " 0.77750254 0.7774925  0.80410374]\n",
      "Average Test R2: [0.13390224 0.62177965 0.67764637 0.39003431 0.75137123 0.73123546\n",
      " 0.91329512 0.87097604 0.7754187  0.8765417  0.77809874 0.77810301\n",
      " 0.77809669 0.7780915  0.80391107]\n",
      "Epoch: 014, Loss: 0.29978\n",
      "Average Train R2: [0.36094622 0.60972095 0.6684617  0.38601604 0.75510466 0.72031417\n",
      " 0.87474196 0.86222938 0.75680738 0.8575053  0.78059686 0.78060265\n",
      " 0.78060048 0.78058688 0.8049102 ]\n",
      "Average Test R2: [0.18049201 0.61957529 0.67748588 0.38375895 0.75916387 0.71358216\n",
      " 0.87650157 0.86092679 0.75609601 0.86129475 0.78095801 0.78096425\n",
      " 0.78096055 0.7809503  0.80745649]\n",
      "Epoch: 015, Loss: 0.30545\n",
      "Average Train R2: [0.4422222  0.58722661 0.62147595 0.38885555 0.71937131 0.72916014\n",
      " 0.90109476 0.86462895 0.76667642 0.86358098 0.7140018  0.71398642\n",
      " 0.71401384 0.71400145 0.77892915]\n",
      "Average Test R2: [0.43550359 0.59243963 0.62415788 0.38167336 0.72252079 0.71946256\n",
      " 0.89993396 0.86292062 0.76772833 0.86509136 0.7145854  0.71456991\n",
      " 0.71459681 0.71458566 0.78077017]\n",
      "Epoch: 016, Loss: 0.30425\n",
      "Average Train R2: [0.38550164 0.62479904 0.66761786 0.40917191 0.75854696 0.7541142\n",
      " 0.90856344 0.86618988 0.78540888 0.87541805 0.76685508 0.76686139\n",
      " 0.7668696  0.76685688 0.79124212]\n",
      "Average Test R2: [0.35443065 0.6269892  0.67273239 0.39318457 0.76114634 0.74833407\n",
      " 0.90774843 0.86382995 0.78203106 0.87636437 0.76723375 0.76723851\n",
      " 0.76724787 0.76723618 0.79462203]\n",
      "Epoch: 017, Loss: 0.29823\n",
      "Average Train R2: [0.37766399 0.52793822 0.56180364 0.3991757  0.69165598 0.74575275\n",
      " 0.89964429 0.85086529 0.665796   0.84311849 0.72446687 0.72446568\n",
      " 0.72447529 0.72446559 0.73567865]\n",
      "Average Test R2: [0.17443869 0.52466679 0.56366918 0.39201099 0.69652699 0.73679543\n",
      " 0.89901531 0.84691357 0.66560754 0.84528452 0.72584896 0.72584854\n",
      " 0.72585645 0.72584767 0.73824187]\n",
      "Epoch: 018, Loss: 0.29680\n",
      "Average Train R2: [0.37865536 0.62609982 0.68331473 0.41282748 0.76624579 0.7573666\n",
      " 0.91097786 0.86973025 0.78333948 0.86502287 0.78051739 0.78051792\n",
      " 0.78052176 0.78052155 0.79751691]\n",
      "Average Test R2: [0.24552869 0.62617756 0.68557182 0.40824652 0.76912758 0.75042913\n",
      " 0.91023058 0.86713631 0.7818406  0.86657858 0.77650294 0.77650311\n",
      " 0.77650653 0.77650622 0.80078796]\n",
      "Epoch: 019, Loss: 0.30190\n",
      "Average Train R2: [0.41309595 0.64380869 0.68017405 0.42230178 0.75547908 0.74398552\n",
      " 0.91343322 0.87684541 0.79325782 0.87554297 0.7904693  0.79047122\n",
      " 0.79047231 0.7904658  0.81197023]\n",
      "Average Test R2: [0.34968076 0.64734947 0.68660601 0.41470272 0.76025614 0.73886156\n",
      " 0.91151545 0.87401737 0.79044735 0.87894769 0.79144761 0.79144949\n",
      " 0.79145054 0.79144394 0.81449268]\n",
      "Epoch: 020, Loss: 0.29712\n",
      "Average Train R2: [0.3999089  0.62819317 0.67781284 0.43002768 0.75409627 0.75429647\n",
      " 0.90870212 0.87594714 0.77577451 0.8705636  0.78100635 0.78101252\n",
      " 0.78101126 0.78100341 0.79763211]\n",
      "Average Test R2: [0.27602682 0.63121419 0.68415385 0.42577835 0.7577867  0.74768206\n",
      " 0.90883951 0.87381353 0.77354484 0.87083714 0.78292876 0.78293512\n",
      " 0.78293374 0.78292552 0.79993372]\n",
      "Epoch: 021, Loss: 0.29504\n",
      "Average Train R2: [0.44096276 0.6291001  0.66926705 0.42152919 0.744942   0.76405057\n",
      " 0.91336433 0.86921273 0.77208207 0.87602366 0.7661566  0.76616777\n",
      " 0.76615985 0.76615043 0.80565124]\n",
      "Average Test R2: [0.34440596 0.63505339 0.67693186 0.42005114 0.74924101 0.75753988\n",
      " 0.91240061 0.86661616 0.77420898 0.87816556 0.7673666  0.76737759\n",
      " 0.76736996 0.76736011 0.80944663]\n",
      "Epoch: 022, Loss: 0.29676\n",
      "Average Train R2: [0.34585975 0.60700232 0.65420054 0.42990641 0.74011747 0.76124897\n",
      " 0.9186882  0.88009379 0.77084179 0.87255446 0.77415648 0.774159\n",
      " 0.77416332 0.77415138 0.80033977]\n",
      "Average Test R2: [0.3049349  0.60937786 0.65489908 0.42334678 0.73995264 0.75565337\n",
      " 0.91687485 0.87759351 0.75800686 0.87339507 0.773614   0.77361583\n",
      " 0.77362133 0.77360872 0.80224219]\n",
      "Epoch: 023, Loss: 0.28885\n",
      "Average Train R2: [0.16465218 0.61813768 0.67695965 0.40908961 0.74065978 0.74158845\n",
      " 0.90858876 0.86777775 0.76667912 0.87558623 0.77377527 0.77378279\n",
      " 0.77377744 0.77376608 0.8114468 ]\n",
      "Average Test R2: [-0.07363874  0.62625793  0.68528951  0.4066271   0.74463497  0.73523998\n",
      "  0.90759857  0.86488728  0.76739576  0.87789329  0.7736915   0.77369909\n",
      "  0.77369394  0.77368049  0.81267731]\n",
      "Epoch: 024, Loss: 0.30170\n",
      "Average Train R2: [0.43619154 0.62618203 0.67146011 0.44099088 0.76351232 0.74358558\n",
      " 0.92132466 0.87527743 0.78317418 0.87232937 0.78154395 0.78154973\n",
      " 0.78154765 0.7815402  0.80147354]\n",
      "Average Test R2: [0.38943793 0.63032684 0.67952034 0.43477344 0.76699586 0.73809626\n",
      " 0.92070398 0.87389949 0.78234754 0.8756943  0.78448192 0.78448746\n",
      " 0.78448565 0.78447756 0.80353472]\n",
      "Epoch: 025, Loss: 0.29011\n",
      "Average Train R2: [0.31650769 0.64092239 0.68995331 0.44099197 0.76691138 0.76152706\n",
      " 0.91616053 0.87401329 0.79775135 0.88711775 0.79219481 0.79220063\n",
      " 0.79220012 0.79218868 0.79991899]\n",
      "Average Test R2: [0.3280609  0.64649145 0.69753179 0.43649564 0.77003053 0.75650424\n",
      " 0.91578413 0.87172126 0.78381209 0.88878009 0.79063936 0.79064499\n",
      " 0.79064495 0.79063282 0.8019729 ]\n",
      "Epoch: 026, Loss: 0.28732\n",
      "Average Train R2: [0.38168536 0.64334527 0.66524052 0.4385852  0.76755382 0.76317536\n",
      " 0.92039412 0.87828888 0.79379767 0.87493353 0.79423466 0.79424008\n",
      " 0.79424046 0.79422824 0.81105414]\n",
      "Average Test R2: [0.18862699 0.64814562 0.66944803 0.43770097 0.77059291 0.75605186\n",
      " 0.91951957 0.87605221 0.79204014 0.87685214 0.7938654  0.79387105\n",
      " 0.79387137 0.79385875 0.81397811]\n",
      "Epoch: 027, Loss: 0.28973\n",
      "Average Train R2: [0.44187884 0.6428594  0.67713278 0.43774204 0.7673613  0.75564245\n",
      " 0.91808253 0.87909797 0.79075316 0.88459781 0.76998924 0.76999508\n",
      " 0.7699942  0.76998562 0.80299771]\n",
      "Average Test R2: [0.3251549  0.64280741 0.67906275 0.4323354  0.76994781 0.74992432\n",
      " 0.9175153  0.87705795 0.78970602 0.88618362 0.7696892  0.76969472\n",
      " 0.76969443 0.76968602 0.80477811]\n",
      "Epoch: 028, Loss: 0.29058\n",
      "Average Train R2: [0.39481776 0.65456177 0.69156188 0.44018317 0.77037279 0.76845053\n",
      " 0.92002603 0.87850326 0.8016149  0.8751466  0.79605864 0.79606478\n",
      " 0.79606343 0.79605281 0.81851544]\n",
      "Average Test R2: [0.21638475 0.65885521 0.69717014 0.43644912 0.77551474 0.76245747\n",
      " 0.92024607 0.87738186 0.79806213 0.87879597 0.79577091 0.79577714\n",
      " 0.79577581 0.79576478 0.82215245]\n",
      "Epoch: 029, Loss: 0.29198\n",
      "Average Train R2: [0.39887842 0.6450246  0.69497572 0.44399421 0.76419021 0.7518301\n",
      " 0.90586772 0.84981896 0.80148284 0.87799873 0.79660265 0.79660761\n",
      " 0.79660846 0.7965974  0.80990455]\n",
      "Average Test R2: [0.30494724 0.64900973 0.70260636 0.43978811 0.76673102 0.74622938\n",
      " 0.90550736 0.84878112 0.79917866 0.87941078 0.79702197 0.7970268\n",
      " 0.79702779 0.79701665 0.81166051]\n",
      "Epoch: 030, Loss: 0.28812\n",
      "Average Train R2: [0.44592872 0.64500714 0.69411709 0.44092666 0.76660806 0.75943554\n",
      " 0.91623289 0.87238605 0.8016457  0.88027497 0.79126537 0.791271\n",
      " 0.79127136 0.79125927 0.804672  ]\n",
      "Average Test R2: [0.35186088 0.64974584 0.7004525  0.43579885 0.76991663 0.75247342\n",
      " 0.91545619 0.86910568 0.79832137 0.88293133 0.79166443 0.79166999\n",
      " 0.79167037 0.79165834 0.80760725]\n",
      "Epoch: 031, Loss: 0.29356\n",
      "Average Train R2: [0.43837923 0.64376393 0.6960623  0.44258696 0.76358925 0.76489574\n",
      " 0.92244094 0.88497807 0.796435   0.87454055 0.77946174 0.77946625\n",
      " 0.77946674 0.77945687 0.80484581]\n",
      "Average Test R2: [0.42103969 0.64768939 0.70084966 0.43823154 0.76659129 0.75925546\n",
      " 0.92122332 0.88226199 0.79558381 0.87719203 0.77999013 0.77999462\n",
      " 0.77999522 0.77998518 0.80761944]\n",
      "Epoch: 032, Loss: 0.28666\n",
      "Average Train R2: [0.2900868  0.61496383 0.66631185 0.43735693 0.75908431 0.75811118\n",
      " 0.91907315 0.87398604 0.7803867  0.87131453 0.79568665 0.79569184\n",
      " 0.79569176 0.79568101 0.78154268]\n",
      "Average Test R2: [0.20137805 0.61567886 0.66896022 0.43427955 0.76095288 0.75183425\n",
      " 0.91797602 0.87151277 0.77930407 0.87087667 0.79577805 0.79578319\n",
      " 0.79578318 0.79577243 0.7844417 ]\n",
      "Epoch: 033, Loss: 0.29151\n",
      "Average Train R2: [0.30755864 0.63209144 0.66896039 0.43041039 0.74223618 0.71993673\n",
      " 0.89272216 0.86677999 0.78506418 0.84844592 0.7744178  0.77442157\n",
      " 0.77442282 0.77441176 0.78985145]\n",
      "Average Test R2: [0.24681045 0.63514956 0.67627724 0.423059   0.74361595 0.71434986\n",
      " 0.8901485  0.86399388 0.78369449 0.85155103 0.77460741 0.77461097\n",
      " 0.77461232 0.77460178 0.79123655]\n",
      "Epoch: 034, Loss: 0.28761\n",
      "Average Train R2: [0.35195307 0.59284139 0.64363584 0.43354427 0.70961742 0.72379136\n",
      " 0.91565739 0.86977991 0.73360605 0.87218421 0.76205126 0.7620575\n",
      " 0.7620576  0.76204425 0.76914616]\n",
      "Average Test R2: [0.28942216 0.59730269 0.65084098 0.4259415  0.71595777 0.71981688\n",
      " 0.91571235 0.8685786  0.73673743 0.87620235 0.76454829 0.76455436\n",
      " 0.76455463 0.76454128 0.77264055]\n",
      "Epoch: 035, Loss: 0.28081\n",
      "Average Train R2: [-0.15721649  0.55099619  0.58739403  0.41630189  0.74906794  0.71480923\n",
      "  0.91452384  0.87682412  0.70624231  0.87379709  0.78045049  0.78045633\n",
      "  0.78045595  0.78044482  0.80026068]\n",
      "Average Test R2: [-0.33821472  0.55454802  0.59528488  0.40774171  0.7534421   0.70776923\n",
      "  0.91415101  0.87462232  0.7049946   0.87674633  0.78093442  0.78094024\n",
      "  0.78093981  0.78092892  0.80210013]\n",
      "Epoch: 036, Loss: 0.28772\n",
      "Average Train R2: [0.28078329 0.49681078 0.55517165 0.43436056 0.6170879  0.71607843\n",
      " 0.90421825 0.86370588 0.65062282 0.82733752 0.68307716 0.6830861\n",
      " 0.68308653 0.68306554 0.68583472]\n",
      "Average Test R2: [0.16552995 0.50220881 0.56362632 0.42103894 0.62864691 0.70768586\n",
      " 0.90435055 0.86180578 0.65569978 0.8318544  0.68257573 0.6825844\n",
      " 0.68258498 0.68256425 0.69369655]\n",
      "Epoch: 037, Loss: 0.28332\n",
      "Average Train R2: [-0.06944302  0.54285339  0.56589603  0.41940168  0.69065333  0.74661968\n",
      "  0.91056466  0.86679657  0.7290263   0.83735134  0.71536581  0.71537217\n",
      "  0.71537468  0.71535514  0.73137087]\n",
      "Average Test R2: [-0.41463106  0.54317882  0.57712225  0.41120819  0.69555344  0.73592741\n",
      "  0.90958116  0.86440713  0.7296363   0.84013312  0.71565175  0.71565775\n",
      "  0.71566095  0.71564093  0.73336684]\n",
      "Epoch: 038, Loss: 0.28496\n",
      "Average Train R2: [0.4392632  0.62481976 0.6614388  0.43726388 0.76094412 0.75461767\n",
      " 0.91618546 0.88057522 0.79409025 0.87409347 0.79064699 0.79065257\n",
      " 0.79065219 0.79064096 0.81704829]\n",
      "Average Test R2: [0.30344273 0.6294508  0.66600112 0.42917481 0.76529105 0.74790507\n",
      " 0.91586517 0.87736503 0.79495613 0.87656154 0.79175724 0.79176281\n",
      " 0.79176255 0.79175098 0.81996635]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 56\u001b[0m\n\u001b[0;32m     54\u001b[0m average_loss \u001b[39m=\u001b[39m []\n\u001b[0;32m     55\u001b[0m \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m train_loader:\n\u001b[1;32m---> 56\u001b[0m       loss \u001b[39m=\u001b[39m train(data)\n\u001b[0;32m     57\u001b[0m       average_loss\u001b[39m.\u001b[39mappend(loss)\n\u001b[0;32m     58\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m:\u001b[39;00m\u001b[39m03d\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Loss: \u001b[39m\u001b[39m{\u001b[39;00m(\u001b[39msum\u001b[39m(average_loss)\u001b[39m/\u001b[39m\u001b[39mlen\u001b[39m(average_loss))\u001b[39m:\u001b[39;00m\u001b[39m.5f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[52], line 13\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(data_in)\u001b[0m\n\u001b[0;32m     10\u001b[0m targets \u001b[39m=\u001b[39m data_in\u001b[39m.\u001b[39my\n\u001b[0;32m     11\u001b[0m loss \u001b[39m=\u001b[39m criterion(out, targets) \n\u001b[1;32m---> 13\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()  \u001b[39m# Derive gradients.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m optimizer\u001b[39m.\u001b[39mstep()  \u001b[39m# Update parameters based on gradients.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\jespe\\anaconda3\\envs\\proj_env\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\jespe\\anaconda3\\envs\\proj_env\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = GNN(hidden_channels=128) \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005, weight_decay=5e-4)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "def train(data_in):\n",
    "      model.train()\n",
    "      optimizer.zero_grad()  # Clear gradients.\n",
    "      out = model(data_in.x, data_in.edge_index, data_in.edge_attr, data_in.batch)  # Perform a single forward pass.\n",
    "\n",
    "      targets = data_in.y\n",
    "      loss = criterion(out, targets) \n",
    "\n",
    "      loss.backward()  # Derive gradients.\n",
    "      optimizer.step()  # Update parameters based on gradients.\n",
    "      return loss\n",
    "\n",
    "def test(data):\n",
    "      all_test_r2 = []\n",
    "      counter = -1    \n",
    "      for data_in in data:\n",
    "            counter += 1\n",
    "            model.eval()\n",
    "            out = model(data_in.x, data_in.edge_index, data_in.edge_attr, data_in.batch)\n",
    "\n",
    "            # Caculate R2\n",
    "            r2_score_var = []\n",
    "            for item in range(0,data_in.y.shape[1]):\n",
    "                  if item == 0:\n",
    "                        r2_score_var = r2_score(data_in.y[:,item].detach().numpy(), out[:,item].detach().numpy())\n",
    "                  else:\n",
    "                        r2_score_var = np.vstack((r2_score_var,(r2_score(data_in.y[:,item].detach().numpy(), out[:,item].detach().numpy()))))\n",
    "\n",
    "            if counter == 0:\n",
    "                  all_test_r2 = r2_score_var\n",
    "            else:\n",
    "                  all_test_r2 = np.hstack((all_test_r2,r2_score_var))\n",
    "\n",
    "      average_test_r2 = np.sum(all_test_r2,axis=1) / all_test_r2.shape[1]\n",
    "\n",
    "      return average_test_r2\n",
    "\n",
    "# Vectors to append accuracy to:\n",
    "Train_r2 = []\n",
    "Test_r2 = []\n",
    "\n",
    "# Calculate accuracy before training \n",
    "Train_r2.append(test(train_loader))\n",
    "Test_r2.append(test(test_loader))\n",
    "print(\"Initial training R2: \", Train_r2[0])\n",
    "print(\"Initial test R2: \", Test_r2[0])\n",
    "\n",
    "print_r2_option = True\n",
    "for epoch in range(1, 31):\n",
    "      average_loss = []\n",
    "      for data in train_loader:\n",
    "            loss = train(data)\n",
    "            average_loss.append(loss)\n",
    "      print(f'Epoch: {epoch:03d}, Loss: {(sum(average_loss)/len(average_loss)):.5f}')\n",
    "\n",
    "      if print_r2_option:\n",
    "\n",
    "            temp_train_r2 = test(train_loader)\n",
    "            Train_r2.append(temp_train_r2)\n",
    "\n",
    "            temp_test_r2 = test(test_loader)\n",
    "            Test_r2.append(temp_test_r2)\n",
    "\n",
    "            print(f'Average Train R2: {temp_train_r2}')\n",
    "            print(f'Average Test R2: {temp_test_r2:}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final loss:  0.27030855\n",
      "Final training R2:  [0.4392632  0.62481976 0.6614388  0.43726388 0.76094412 0.75461767\n",
      " 0.91618546 0.88057522 0.79409025 0.87409347 0.79064699 0.79065257\n",
      " 0.79065219 0.79064096 0.81704829]\n",
      "Final test R2:  [0.30344273 0.6294508  0.66600112 0.42917481 0.76529105 0.74790507\n",
      " 0.91586517 0.87736503 0.79495613 0.87656154 0.79175724 0.79176281\n",
      " 0.79176255 0.79175098 0.81996635]\n"
     ]
    }
   ],
   "source": [
    "print(\"Final loss: \", (sum(average_loss)/len(average_loss)).detach().numpy())\n",
    "print(\"Final training R2: \", Train_r2[-1])\n",
    "print(\"Final test R2: \", Test_r2[-1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model for just one target"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GNN function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_labels = 1\n",
    "data_features = dataset[50].x.shape[1]\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GATConv(data_features, hidden_channels)\n",
    "        self.conv2 = GATConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GATConv(hidden_channels, hidden_channels)\n",
    "        self.conv4 = GATConv(hidden_channels, hidden_channels)\n",
    "        self.conv5 = GATConv(hidden_channels, hidden_channels)\n",
    "        self.lin = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin2 = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin3 = Linear(hidden_channels, data_labels)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch): \n",
    "        x = self.conv1(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv4(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv5(x, edge_index, edge_attr)\n",
    "\n",
    "        #Returns batch-wise graph-level-outputs by averaging node features across the node dimension, so that for a single graph G\n",
    "        #its output is computed by\n",
    "        x = global_mean_pool(x, batch) \n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = F.relu(x)\n",
    "        x = self.lin2(x)\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = F.relu(x)\n",
    "        x = self.lin3(x)\n",
    "        \n",
    " \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial training R2:  -0.0186155677613906\n",
      "Initial test R2:  -0.015978314784087073\n",
      "Epoch: 001, Loss: 0.32682\n",
      "Average Train R2: 0.8672998000584929\n",
      "Average Test R2: 0.8681060938673704\n",
      "Epoch: 002, Loss: 0.10999\n",
      "Average Train R2: 0.928836275969301\n",
      "Average Test R2: 0.9266400583640536\n",
      "Epoch: 003, Loss: 0.08343\n",
      "Average Train R2: 0.93881720928776\n",
      "Average Test R2: 0.9378613671032885\n",
      "Epoch: 004, Loss: 0.07667\n",
      "Average Train R2: 0.9416217515269708\n",
      "Average Test R2: 0.9410020391316778\n",
      "Epoch: 005, Loss: 0.07255\n",
      "Average Train R2: 0.9460813636748321\n",
      "Average Test R2: 0.9455932715116921\n",
      "Epoch: 006, Loss: 0.07021\n",
      "Average Train R2: 0.9463242108634822\n",
      "Average Test R2: 0.9453247671856768\n",
      "Epoch: 007, Loss: 0.06855\n",
      "Average Train R2: 0.915514749894574\n",
      "Average Test R2: 0.9139418518749833\n",
      "Epoch: 008, Loss: 0.06688\n",
      "Average Train R2: 0.9492874816496003\n",
      "Average Test R2: 0.9481633532384031\n",
      "Epoch: 009, Loss: 0.06580\n",
      "Average Train R2: 0.9502914239926807\n",
      "Average Test R2: 0.9490051368253422\n",
      "Epoch: 010, Loss: 0.06516\n",
      "Average Train R2: 0.9501059577332617\n",
      "Average Test R2: 0.9491539607420908\n",
      "Epoch: 011, Loss: 0.06446\n",
      "Average Train R2: 0.9385364944666847\n",
      "Average Test R2: 0.9376097626197034\n",
      "Epoch: 012, Loss: 0.06319\n",
      "Average Train R2: 0.9494618824113242\n",
      "Average Test R2: 0.9483624668801787\n",
      "Epoch: 013, Loss: 0.06251\n",
      "Average Train R2: 0.9508069067255801\n",
      "Average Test R2: 0.949841047545027\n",
      "Epoch: 014, Loss: 0.06172\n",
      "Average Train R2: 0.9543321279358812\n",
      "Average Test R2: 0.9527922051735405\n",
      "Epoch: 015, Loss: 0.06155\n",
      "Average Train R2: 0.9492625022520974\n",
      "Average Test R2: 0.9477185738185646\n",
      "Best training R2:  0.9543321279358812\n",
      "Best test R2:  0.9527922051735405\n"
     ]
    }
   ],
   "source": [
    "model = GNN(hidden_channels=64) \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=5e-4)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "def train(data_in):\n",
    "      target = 6 # target index of interest\n",
    "      model.train()\n",
    "      optimizer.zero_grad()  # Clear gradients.\n",
    "      out = model(data_in.x, data_in.edge_index, data_in.edge_attr, data_in.batch)[:,target].reshape(-1,1)\n",
    "      targets = data_in.y[:,target].reshape(-1,1)\n",
    "      \n",
    "      #Alt 1\n",
    "      loss = criterion(out, targets)   \n",
    "\n",
    "      loss.backward()  # Derive gradients.\n",
    "      optimizer.step()  # Update parameters based on gradients.\n",
    "      return loss\n",
    "\n",
    "def test(data):\n",
    "      target = 6\n",
    "      all_test_r2 = []\n",
    "      counter = -1    \n",
    "      for data_in in data:\n",
    "            counter += 1\n",
    "            model.eval()\n",
    "            out = model(data_in.x, data_in.edge_index, data_in.edge_attr, data_in.batch)[:,target].reshape(-1,1)\n",
    "            targets = data_in.y[:,target].reshape(-1,1)\n",
    "            \n",
    "            # Caculate R2\n",
    "            r2_score_var = r2_score(targets.detach().numpy(), out.detach().numpy())\n",
    "\n",
    "            all_test_r2 .append(r2_score_var)\n",
    "\n",
    "      average_test_r2 = np.sum(all_test_r2) / len(all_test_r2)\n",
    "\n",
    "      return average_test_r2\n",
    "\n",
    "# Vectors to append accuracy to:\n",
    "Train_r2 = []\n",
    "Test_r2 = []\n",
    "\n",
    "# Calculate accuracy before training \n",
    "Train_r2.append(test(train_loader))\n",
    "Test_r2.append(test(test_loader))\n",
    "print(\"Initial training R2: \", Train_r2[0])\n",
    "print(\"Initial test R2: \", Test_r2[0])\n",
    "\n",
    "print_r2_option = True\n",
    "for epoch in range(1, 16):\n",
    "      average_loss = []\n",
    "      for data in train_loader:\n",
    "            loss = train(data)\n",
    "            average_loss.append(loss)\n",
    "      print(f'Epoch: {epoch:03d}, Loss: {(sum(average_loss)/len(average_loss)):.5f}')\n",
    "\n",
    "      if print_r2_option:\n",
    "\n",
    "            temp_train_r2 = test(train_loader)\n",
    "            Train_r2.append(temp_train_r2)\n",
    "\n",
    "            temp_test_r2 = test(test_loader)\n",
    "            Test_r2.append(temp_test_r2)\n",
    "\n",
    "            print(f'Average Train R2: {temp_train_r2}')\n",
    "            print(f'Average Test R2: {temp_test_r2:}')\n",
    "\n",
    "print(\"Best training R2: \", np.max(Train_r2))\n",
    "print(\"Best test R2: \", np.max(Test_r2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
