{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TIF360 Project"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main source: https://www.kaggle.com/code/rmonge/predicting-molecule-properties-based-on-its-smiles/notebook"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import rdkit\n",
    "from rdkit import Chem  # To extract information of the molecules\n",
    "from rdkit.Chem import Draw  # To draw the molecules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch_geometric\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch_geometric.utils as utils\n",
    "import networkx as nx\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import global_mean_pool, GraphConv, GATConv, GCNConv, BatchNorm, GraphNorm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(132820, 21)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/smiles_and_targets.csv\")\n",
    "print(np.shape(df))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(132820, 987)\n"
     ]
    }
   ],
   "source": [
    "mol_descriptor = np.load(\"../data/Mordred_mol_descriptors.npy\")\n",
    "print(mol_descriptor.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Morgan fingerprints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(132820, 2048)\n"
     ]
    }
   ],
   "source": [
    "mol_fingerprints = np.load(\"../data/mol_morgan_fingerprints.npy\")\n",
    "print(mol_fingerprints.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigate Neural Networks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Neural Networks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert data to graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph_dataset_functions import create_graph_dataset_from_smiles\n",
    "\n",
    "properties_names = ['A', 'B', 'C', 'mu', 'alfa', 'homo', 'lumo', 'gap', 'RÂ²', 'zpve', 'U0', 'U', 'H', 'G', 'Cv']\n",
    "\n",
    "x_smiles = df.smiles.values\n",
    "y = df.loc[:, properties_names].values  # shape = (n_samples, n_properties)\n",
    "\n",
    "dataset = create_graph_dataset_from_smiles(x_smiles, y[0:len(x_smiles), :])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information of the graph dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of graphs (molecules): 132820\n",
      "=================================================================================\n",
      "Properties of graph 50, molecule smiles: CC1=CNC=C1\n",
      "Number of nodes: 6\n",
      "Number of edges: 12\n",
      "Number of node features: 78\n",
      "Number of edge features: 10\n",
      "Number of target properties: 15\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of graphs (molecules): {len(dataset)}')\n",
    "graph = dataset[50]\n",
    "print('=================================================================================')\n",
    "print(f'Properties of graph {50}, molecule smiles: {df.smiles.values[50]}')\n",
    "print(f'Number of nodes: {dataset[50].x.shape[0]}')\n",
    "print(f'Number of edges: {dataset[50].edge_index.shape[1]}')\n",
    "print(f'Number of node features: {dataset[50].x.shape[1]}')\n",
    "print(f'Number of edge features: {dataset[50].edge_attr.shape[1]}')\n",
    "print(f'Number of target properties: {dataset[50].y.shape[1]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filterout data with no edge features defined (Like ex: CH4) (These causes problems down the line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of none edge feature molecules:  0\n",
      "Before:  132820\n",
      "After:  132820\n"
     ]
    }
   ],
   "source": [
    "indexes_to_delete = []\n",
    "for item in range(0,len(dataset)):\n",
    "    if dataset[item].edge_attr.shape[1] == 0:\n",
    "        indexes_to_delete.append(item)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "indexes_to_delete.sort()\n",
    "\n",
    "print(\"Number of none edge feature molecules: \", len(indexes_to_delete))\n",
    "\n",
    "print(\"Before: \", len(dataset))\n",
    "\n",
    "for item in range(0,len(indexes_to_delete)):\n",
    "    print(\"Molecule to delete: \", df.smiles.values[indexes_to_delete[item]])\n",
    "    #del dataset[indexes_to_delete[item] - item] \n",
    "    dataset.pop((indexes_to_delete[item] - item)) # -item since all future data points will have its index reduced by 1 for each deleted previous data point\n",
    "print(\"After: \", len(dataset))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example train data target before scaling tensor([[ 2.9693e+00,  1.9777e+00,  1.8423e+00,  1.2093e+00,  8.4060e+01,\n",
      "         -1.8970e-01, -7.0400e-02,  1.1930e-01,  8.7339e+02,  1.5988e-01,\n",
      "         -3.4861e+02, -3.4860e+02, -3.4860e+02, -3.4864e+02,  2.9189e+01]])\n",
      "Example train data target after scaling: tensor([[-0.3350,  1.3300,  2.2841, -1.0264,  1.0990,  2.2975, -1.7445, -2.7966,\n",
      "         -1.1527,  0.3357,  1.6155,  1.6155,  1.6155,  1.6156, -0.6067]])\n",
      "Total data size:  132820\n",
      "Train data size:  106256\n",
      "Test data size:  26564\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# split the dataset into test and validation:\n",
    "num_samples = len(dataset)\n",
    "\n",
    "# Want to divide data randomly\n",
    "random.seed(42)\n",
    "random_indexes = np.array(random.sample(range(num_samples), num_samples)) # random.sample ensures no duplicates\n",
    "\n",
    "train_data = [dataset[index] for index in random_indexes[int(.2 * num_samples ) :]] # 80%\n",
    "test_data = [dataset[index] for index in random_indexes[: int(.2 * num_samples )]] # 20%\n",
    "\n",
    "print(\"Example train data target before scaling\", train_data[0].y)\n",
    "train_data_targets = [data.y for data in train_data]\n",
    "train_data_targets = torch.concatenate(train_data_targets, axis=0)\n",
    "test_data_targets = [data.y for data in test_data]\n",
    "test_data_targets = torch.concatenate(test_data_targets, axis=0)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_data_targets = scaler.fit_transform(train_data_targets)\n",
    "test_data_targets = scaler.transform(test_data_targets)\n",
    "\n",
    "# print(\"Example train data target\", train_data_targets[0].reshape(1,-1).shape)\n",
    "train_data_targets = torch.tensor(train_data_targets, dtype=torch.float)\n",
    "test_data_targets = torch.tensor(test_data_targets, dtype=torch.float)\n",
    "\n",
    "train_data = [Data(x=data.x, edge_index=data.edge_index, edge_attr=data.edge_attr, y=train_data_targets[index].reshape(1,-1)) for index, data in enumerate(train_data)]\n",
    "test_data = [Data(x=data.x, edge_index=data.edge_index, edge_attr=data.edge_attr, y=test_data_targets[index].reshape(1,-1)) for index, data in enumerate(test_data)]\n",
    "print(\"Example train data target after scaling:\", train_data[0].y)\n",
    "\n",
    "print(\"Total data size: \", len(dataset))\n",
    "print(\"Train data size: \", len(train_data))\n",
    "print(\"Test data size: \", len(test_data))\n",
    "\n",
    "#train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "#test_loader = DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide descriptors and fingerprints into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_descriptor = [mol_descriptor[index,:] for index in random_indexes[int(.2 * num_samples ) :]] # 80%\n",
    "test_data_desriptors = [mol_descriptor[index,:] for index in random_indexes[: int(.2 * num_samples )]] # 20%\n",
    "\n",
    "train_data_fingerprints = [mol_fingerprints[index,:] for index in random_indexes[int(.2 * num_samples ) :]] # 80%\n",
    "test_data_fingerprints = [mol_fingerprints[index,:] for index in random_indexes[: int(.2 * num_samples )]] # 20%\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize using MinMax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "minmax_scaler = MinMaxScaler()\n",
    "\n",
    "train_data_descriptor = minmax_scaler.fit_transform(train_data_descriptor)\n",
    "\n",
    "test_data_desriptors = minmax_scaler.transform(test_data_desriptors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "minmax_scaler = MinMaxScaler()\n",
    "\n",
    "train_data_fingerprints = minmax_scaler.fit_transform(train_data_fingerprints)\n",
    "\n",
    "test_data_fingerprints = minmax_scaler.transform(test_data_fingerprints)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine descriptors temporarely with in train and test data for graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[9, 78], edge_index=[2, 20], edge_attr=[20, 10], y=[1, 15], descriptors=[179])\n"
     ]
    }
   ],
   "source": [
    "#Testing stuff\n",
    "\n",
    "temp = train_data[1000]\n",
    "temp.descriptors = train_data_descriptor[1000,:]\n",
    "\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[9, 78], edge_index=[2, 24], edge_attr=[24, 10], y=[1, 15], descriptors=[1, 987], fingerprints=[1, 2048])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for index in range(0,len(train_data)):\n",
    "    train_data[index].descriptors = torch.FloatTensor([train_data_descriptor[index,:]])\n",
    "    train_data[index].fingerprints = torch.FloatTensor([train_data_fingerprints[index,:]])\n",
    "\n",
    "for index in range(0,len(test_data)):\n",
    "    test_data[index].descriptors = torch.FloatTensor([test_data_desriptors[index,:]])\n",
    "    test_data[index].fingerprints = torch.FloatTensor([test_data_fingerprints[index,:]])\n",
    "\n",
    "# Example\n",
    "print(train_data[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main GNN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model for all targets at once"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GNN function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_labels = dataset[50].y.shape[1]\n",
    "data_features = dataset[50].x.shape[1]\n",
    "descriptors_features = train_data[0].descriptors.shape[1]\n",
    "fingerprints_features = train_data[0].fingerprints.shape[1]\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GATConv(data_features, hidden_channels)\n",
    "        self.conv2 = GATConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GATConv(hidden_channels, hidden_channels)\n",
    "        self.conv4 = GATConv(hidden_channels, hidden_channels)\n",
    "        self.conv5 = GATConv(hidden_channels, hidden_channels)\n",
    "        self.lin = Linear(hidden_channels+descriptors_features+fingerprints_features, 512)\n",
    "        self.lin2 = Linear(512, 256)\n",
    "        self.lin3 = Linear(256, data_labels)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch, descriptors, fingerprints): \n",
    "        x = self.conv1(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv4(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv5(x, edge_index, edge_attr)\n",
    "\n",
    "        #Returns batch-wise graph-level-outputs by averaging node features across the node dimension, so that for a single graph G\n",
    "        #its output is computed by\n",
    "        x = global_mean_pool(x, batch) \n",
    "        x2 = descriptors\n",
    "        x3 = fingerprints\n",
    "        x = torch.cat((x,x2,x3),1)\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = F.relu(x)\n",
    "        x = self.lin2(x)\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = F.relu(x)\n",
    "        x = self.lin3(x)\n",
    "        \n",
    " \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With normalization layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_labels = dataset[50].y.shape[1]\n",
    "data_features = dataset[50].x.shape[1]\n",
    "descriptors_features = train_data[0].descriptors.shape[1]\n",
    "fingerprints_features = train_data[0].fingerprints.shape[1]\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GATConv(data_features, hidden_channels)\n",
    "        self.conv1_norm = GraphNorm(hidden_channels)\n",
    "        self.conv2 = GATConv(hidden_channels, hidden_channels)\n",
    "        self.conv2_norm = GraphNorm(hidden_channels)\n",
    "        self.conv3 = GATConv(hidden_channels, hidden_channels)\n",
    "        self.conv3_norm = GraphNorm(hidden_channels)\n",
    "        self.conv4 = GATConv(hidden_channels, hidden_channels)\n",
    "        self.conv4_norm = GraphNorm(hidden_channels)\n",
    "        self.conv5 = GATConv(hidden_channels, hidden_channels)\n",
    "        self.conv5_norm = GraphNorm(hidden_channels)\n",
    "        self.lin = Linear(hidden_channels+descriptors_features+fingerprints_features, 512)\n",
    "        self.norm = BatchNorm(hidden_channels+descriptors_features+fingerprints_features)\n",
    "        self.lin2 = Linear(512, 256)\n",
    "        self.norm2 = BatchNorm(512)\n",
    "        self.lin3 = Linear(256, data_labels)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch, descriptors, fingerprints): \n",
    "        x = self.conv1(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv1_norm(x)\n",
    "        x = self.conv2(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2_norm(x)\n",
    "        x = self.conv3(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3_norm(x)\n",
    "        x = self.conv4(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv4_norm(x)\n",
    "        x = self.conv5(x, edge_index, edge_attr)\n",
    "        x = self.conv5_norm(x)\n",
    "\n",
    "        #Returns batch-wise graph-level-outputs by averaging node features across the node dimension, so that for a single graph G\n",
    "        #its output is computed by\n",
    "        x = global_mean_pool(x, batch) \n",
    "        x2 = descriptors\n",
    "        x3 = fingerprints\n",
    "        x = torch.cat((x,x2,x3),1)\n",
    "        x = self.norm(x)\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = F.relu(x)\n",
    "        x = self.norm2(x)\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.lin2(x)\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = F.relu(x)\n",
    "        x = self.lin3(x)\n",
    "        \n",
    " \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With attention before dense layer (Doesn't work as it is now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import MultiheadAttention\n",
    "\n",
    "data_labels = dataset[50].y.shape[1]\n",
    "data_features = dataset[50].x.shape[1]\n",
    "descriptors_features = train_data[0].descriptors.shape[1]\n",
    "fingerprints_features = train_data[0].fingerprints.shape[1]\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GATConv(data_features, hidden_channels)\n",
    "        self.conv2 = GATConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GATConv(hidden_channels, hidden_channels)\n",
    "        self.conv4 = GATConv(hidden_channels, hidden_channels)\n",
    "        self.conv5 = GATConv(hidden_channels, hidden_channels)\n",
    "        self.attention = MultiheadAttention(hidden_channels, 1) # input_size, hidden_size, num_heads(number of parallel self-attention heads to use)\n",
    "        self.attention2 = MultiheadAttention(descriptors_features, 1) \n",
    "        self.attention3 = MultiheadAttention(fingerprints_features, 1) \n",
    "        self.lin = Linear(hidden_channels+descriptors_features+fingerprints_features, 512)\n",
    "        self.lin2 = Linear(512, 256)\n",
    "        self.lin3 = Linear(256, data_labels)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch, descriptors, fingerprints): \n",
    "        x = self.conv1(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv4(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv5(x, edge_index, edge_attr)\n",
    "\n",
    "        #Returns batch-wise graph-level-outputs by averaging node features across the node dimension, so that for a single graph G\n",
    "        #its output is computed by\n",
    "        x = global_mean_pool(x, batch) \n",
    "        x,_ = self.attention(x,x,x)\n",
    "        x2 = descriptors\n",
    "        x2,_ = self.attention2(x2,x2,x2)\n",
    "        x3 = fingerprints\n",
    "        #x3,_ = self.attention3(x3,x3,x3)\n",
    "        x = torch.cat((x,x2,x3),1)\n",
    "        #x,_ = self.attention(x,x,x)\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = F.relu(x)\n",
    "        x = self.lin2(x)\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = F.relu(x)\n",
    "        x = self.lin3(x)\n",
    "        \n",
    " \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial training R2:  [-0.02154199 -0.01994992 -0.01134434 -0.0244989  -0.01556203 -0.02195235\n",
      " -0.01098373 -0.01563356 -0.01897385 -0.02065857 -0.01878104 -0.02010609\n",
      " -0.02260715 -0.02517599 -0.01925064]\n",
      "Initial test R2:  [-0.02300163 -0.02310306 -0.01319037 -0.02044358 -0.01602927 -0.02159862\n",
      " -0.01221507 -0.01462832 -0.02145343 -0.02202356 -0.01905706 -0.02022325\n",
      " -0.0251529  -0.02757073 -0.02030301]\n",
      "Epoch: 001, Loss: 0.1490\n",
      "Epoch: 002, Loss: 0.1206\n",
      "Epoch: 003, Loss: 0.1169\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[145], line 70\u001b[0m\n\u001b[0;32m     68\u001b[0m average_loss \u001b[39m=\u001b[39m []\n\u001b[0;32m     69\u001b[0m \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m train_loader:\n\u001b[1;32m---> 70\u001b[0m       loss \u001b[39m=\u001b[39m train(data)\n\u001b[0;32m     71\u001b[0m       average_loss\u001b[39m.\u001b[39mappend(loss)\n\u001b[0;32m     72\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m:\u001b[39;00m\u001b[39m03d\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Loss: \u001b[39m\u001b[39m{\u001b[39;00m(\u001b[39msum\u001b[39m(average_loss)\u001b[39m/\u001b[39m\u001b[39mlen\u001b[39m(average_loss))\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[145], line 22\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(data_in)\u001b[0m\n\u001b[0;32m     15\u001b[0m loss \u001b[39m=\u001b[39m criterion(out, data_in\u001b[39m.\u001b[39my) \n\u001b[0;32m     17\u001b[0m \u001b[39m#Alt 2\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[39m#loss = 0\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[39m#for item in range(0,len(data_in.y[0,:])):\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[39m#      loss += criterion(out[:,item], data_in.y[:,item]) \u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()  \u001b[39m# Derive gradients.\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[39m# scheduler.step()\u001b[39;00m\n\u001b[0;32m     24\u001b[0m optimizer\u001b[39m.\u001b[39mstep()  \u001b[39m# Update parameters based on gradients.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = GNN(hidden_channels=128) \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005, weight_decay=5e-4)\n",
    "\n",
    "# Will multiply learning rate by 0.2 every 10 epoch (Haven't tried this yet)\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.2) \n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "def train(data_in):\n",
    "      model.train()\n",
    "      optimizer.zero_grad()  # Clear gradients.\n",
    "      out = model(data_in.x, data_in.edge_index, data_in.edge_attr, data_in.batch, data_in.descriptors, data_in.fingerprints)  # Perform a single forward pass.\n",
    "\n",
    "      #Alt 1\n",
    "      loss = criterion(out, data_in.y) \n",
    "\n",
    "      #Alt 2\n",
    "      #loss = 0\n",
    "      #for item in range(0,len(data_in.y[0,:])):\n",
    "      #      loss += criterion(out[:,item], data_in.y[:,item]) \n",
    "\n",
    "      loss.backward()  # Derive gradients.\n",
    "      # scheduler.step()\n",
    "      optimizer.step()  # Update parameters based on gradients.\n",
    "      return loss\n",
    "\n",
    "def test(data):\n",
    "      all_test_r2 = []\n",
    "      counter = -1    \n",
    "      for data_in in data:\n",
    "            counter += 1\n",
    "            model.eval()\n",
    "            \n",
    "            out = model(data_in.x, data_in.edge_index, data_in.edge_attr, data_in.batch, data_in.descriptors, data_in.fingerprints)\n",
    "\n",
    "            # Caculate R2\n",
    "            r2_score_var = []\n",
    "            for item in range(0,data_in.y.shape[1]):\n",
    "                  if item == 0:\n",
    "                        r2_score_var = r2_score(data_in.y[:,item].detach().numpy(), out[:,item].detach().numpy())\n",
    "                  else:\n",
    "                        r2_score_var = np.vstack((r2_score_var,(r2_score(data_in.y[:,item].detach().numpy(), out[:,item].detach().numpy()))))\n",
    "\n",
    "            if counter == 0:\n",
    "                  all_test_r2 = r2_score_var\n",
    "            else:\n",
    "                  all_test_r2 = np.hstack((all_test_r2,r2_score_var))\n",
    "\n",
    "      average_test_r2 = np.sum(all_test_r2,axis=1) / all_test_r2.shape[1]\n",
    "\n",
    "      return average_test_r2\n",
    "\n",
    "# Vectors to append accuracy to:\n",
    "Train_r2 = []\n",
    "Test_r2 = []\n",
    "N = 5 # How often it should calculate R2\n",
    "\n",
    "# Calculate accuracy before training \n",
    "Train_r2.append(test(train_loader))\n",
    "Test_r2.append(test(test_loader))\n",
    "print(\"Initial training R2: \", Train_r2[0])\n",
    "print(\"Initial test R2: \", Test_r2[0])\n",
    "\n",
    "print_r2_option = True\n",
    "counter = 0\n",
    "for epoch in range(1, 31):\n",
    "      counter += 1\n",
    "      average_loss = []\n",
    "      for data in train_loader:\n",
    "            loss = train(data)\n",
    "            average_loss.append(loss)\n",
    "      print(f'Epoch: {epoch:03d}, Loss: {(sum(average_loss)/len(average_loss)):.4f}')\n",
    "\n",
    "      if print_r2_option & (counter % N == 0):\n",
    "\n",
    "            temp_train_r2 = test(train_loader)\n",
    "            Train_r2.append(temp_train_r2)\n",
    "\n",
    "            temp_test_r2 = test(test_loader)\n",
    "            Test_r2.append(temp_test_r2)\n",
    "\n",
    "            print(f'Average Train R2: {temp_train_r2}')\n",
    "            print(f'Average Test R2: {temp_test_r2:}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model for just one target"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GNN function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GATConv: https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GATConv.html#torch_geometric.nn.conv.GATConv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GENConv: https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GENConv.html#torch_geometric.nn.conv.GENConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_labels = 1\n",
    "data_features = dataset[50].x.shape[1]\n",
    "descriptors_features = train_data[50].descriptors.shape[1]\n",
    "fingerprints_features = train_data[0].fingerprints.shape[1]\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GATConv(data_features, hidden_channels)\n",
    "        self.conv2 = GATConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GATConv(hidden_channels, hidden_channels)\n",
    "        self.conv4 = GATConv(hidden_channels, hidden_channels)\n",
    "        self.conv5 = GATConv(hidden_channels, hidden_channels)\n",
    "        self.lin = Linear(hidden_channels+descriptors_features+fingerprints_features, 512)\n",
    "        self.lin2 = Linear(512, 256)\n",
    "        self.lin3 = Linear(256, data_labels)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch, descriptors, fingerprints): \n",
    "        x = self.conv1(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv4(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv5(x, edge_index, edge_attr)\n",
    "\n",
    "        #Returns batch-wise graph-level-outputs by averaging node features across the node dimension, so that for a single graph G\n",
    "        #its output is computed by\n",
    "        x = global_mean_pool(x, batch) \n",
    "        x2 = descriptors\n",
    "        x3 = fingerprints\n",
    "        x = torch.cat((x,x2,x3),1)\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = F.relu(x)\n",
    "        x = self.lin2(x)\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = F.relu(x)\n",
    "        x = self.lin3(x)\n",
    "        \n",
    " \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target index:  0\n",
      "Initial training R2:  -0.018678295383913163\n",
      "Initial test R2:  -0.03308373704488183\n",
      "Epoch: 001, Loss: 0.76507\n",
      "Epoch: 002, Loss: 0.63190\n",
      "Epoch: 003, Loss: 0.56252\n",
      "Epoch: 004, Loss: 0.51782\n",
      "Epoch: 005, Loss: 0.48434\n",
      "Epoch: 006, Loss: 0.46161\n",
      "Epoch: 007, Loss: 0.44506\n",
      "Epoch: 008, Loss: 0.43061\n",
      "Epoch: 009, Loss: 0.41689\n",
      "Epoch: 010, Loss: 0.40696\n",
      "Epoch: 011, Loss: 0.39623\n",
      "Epoch: 012, Loss: 0.38796\n",
      "Epoch: 013, Loss: 0.37597\n",
      "Epoch: 014, Loss: 0.37060\n",
      "Epoch: 015, Loss: 0.36610\n",
      "Epoch: 016, Loss: 0.35919\n",
      "Epoch: 017, Loss: 0.35353\n",
      "Epoch: 018, Loss: 0.34897\n",
      "Epoch: 019, Loss: 0.34382\n",
      "Epoch: 020, Loss: 0.34009\n",
      "Best training R2 for target 0: 0.6581509149316066\n",
      "Best test R2 for target 0: 0.6554282682601079\n",
      "Target index:  1\n",
      "Initial training R2:  -0.01632305111746684\n",
      "Initial test R2:  -0.017224460553526586\n",
      "Epoch: 001, Loss: 0.63536\n",
      "Epoch: 002, Loss: 0.51249\n",
      "Epoch: 003, Loss: 0.45557\n",
      "Epoch: 004, Loss: 0.41914\n",
      "Epoch: 005, Loss: 0.40053\n",
      "Epoch: 006, Loss: 0.38774\n",
      "Epoch: 007, Loss: 0.37415\n",
      "Epoch: 008, Loss: 0.36468\n",
      "Epoch: 009, Loss: 0.36037\n",
      "Epoch: 010, Loss: 0.35133\n",
      "Epoch: 011, Loss: 0.34609\n",
      "Epoch: 012, Loss: 0.34169\n",
      "Epoch: 013, Loss: 0.33759\n",
      "Epoch: 014, Loss: 0.33326\n",
      "Epoch: 015, Loss: 0.32592\n",
      "Epoch: 016, Loss: 0.32326\n",
      "Epoch: 017, Loss: 0.31861\n",
      "Epoch: 018, Loss: 0.31623\n",
      "Epoch: 019, Loss: 0.31525\n",
      "Epoch: 020, Loss: 0.31122\n",
      "Best training R2 for target 1: 0.7033298631127642\n",
      "Best test R2 for target 1: 0.7007529061747805\n",
      "Target index:  2\n",
      "Initial training R2:  -0.017158692973941334\n",
      "Initial test R2:  -0.01787220238902933\n",
      "Epoch: 001, Loss: 0.58541\n",
      "Epoch: 002, Loss: 0.46367\n",
      "Epoch: 003, Loss: 0.43245\n",
      "Epoch: 004, Loss: 0.41423\n",
      "Epoch: 005, Loss: 0.40000\n",
      "Epoch: 006, Loss: 0.39028\n",
      "Epoch: 007, Loss: 0.37836\n",
      "Epoch: 008, Loss: 0.36869\n",
      "Epoch: 009, Loss: 0.35685\n",
      "Epoch: 010, Loss: 0.34449\n",
      "Epoch: 011, Loss: 0.33835\n",
      "Epoch: 012, Loss: 0.33475\n",
      "Epoch: 013, Loss: 0.33009\n",
      "Epoch: 014, Loss: 0.33005\n",
      "Epoch: 015, Loss: 0.32430\n",
      "Epoch: 016, Loss: 0.32364\n",
      "Epoch: 017, Loss: 0.31936\n",
      "Epoch: 018, Loss: 0.31726\n",
      "Epoch: 019, Loss: 0.31689\n",
      "Epoch: 020, Loss: 0.31391\n",
      "Best training R2 for target 2: 0.7010053077575664\n",
      "Best test R2 for target 2: 0.7051230319763993\n",
      "Target index:  3\n",
      "Initial training R2:  -0.017280060359220337\n",
      "Initial test R2:  -0.01613028571963556\n",
      "Epoch: 001, Loss: 0.61169\n",
      "Epoch: 002, Loss: 0.49573\n",
      "Epoch: 003, Loss: 0.47538\n",
      "Epoch: 004, Loss: 0.46577\n",
      "Epoch: 005, Loss: 0.45996\n",
      "Epoch: 006, Loss: 0.45543\n",
      "Epoch: 007, Loss: 0.45275\n",
      "Epoch: 008, Loss: 0.45026\n",
      "Epoch: 009, Loss: 0.44536\n",
      "Epoch: 010, Loss: 0.44352\n",
      "Epoch: 011, Loss: 0.43955\n",
      "Epoch: 012, Loss: 0.43897\n",
      "Epoch: 013, Loss: 0.43680\n",
      "Epoch: 014, Loss: 0.43439\n",
      "Epoch: 015, Loss: 0.43222\n",
      "Epoch: 016, Loss: 0.43057\n",
      "Epoch: 017, Loss: 0.42784\n",
      "Epoch: 018, Loss: 0.42613\n",
      "Epoch: 019, Loss: 0.42451\n",
      "Epoch: 020, Loss: 0.42348\n",
      "Best training R2 for target 3: 0.5765110817904114\n",
      "Best test R2 for target 3: 0.5754772103382034\n",
      "Target index:  4\n",
      "Initial training R2:  -0.016626750506755876\n",
      "Initial test R2:  -0.016156849228845478\n",
      "Epoch: 001, Loss: 0.46523\n",
      "Epoch: 002, Loss: 0.32744\n",
      "Epoch: 003, Loss: 0.31264\n",
      "Epoch: 004, Loss: 0.30525\n",
      "Epoch: 005, Loss: 0.29862\n",
      "Epoch: 006, Loss: 0.29497\n",
      "Epoch: 007, Loss: 0.28676\n",
      "Epoch: 008, Loss: 0.27884\n",
      "Epoch: 009, Loss: 0.27257\n",
      "Epoch: 010, Loss: 0.26339\n",
      "Epoch: 011, Loss: 0.25305\n",
      "Epoch: 012, Loss: 0.24510\n",
      "Epoch: 013, Loss: 0.23823\n",
      "Epoch: 014, Loss: 0.23340\n",
      "Epoch: 015, Loss: 0.22897\n",
      "Epoch: 016, Loss: 0.22712\n",
      "Epoch: 017, Loss: 0.22235\n",
      "Epoch: 018, Loss: 0.22087\n",
      "Epoch: 019, Loss: 0.21953\n",
      "Epoch: 020, Loss: 0.21672\n",
      "Best training R2 for target 4: 0.7934011110496367\n",
      "Best test R2 for target 4: 0.7904534953760501\n",
      "Target index:  5\n",
      "Initial training R2:  -0.015699766693490477\n",
      "Initial test R2:  -0.020215554713385035\n",
      "Epoch: 001, Loss: 0.53577\n",
      "Epoch: 002, Loss: 0.30047\n",
      "Epoch: 003, Loss: 0.24951\n",
      "Epoch: 004, Loss: 0.22262\n",
      "Epoch: 005, Loss: 0.20876\n",
      "Epoch: 006, Loss: 0.19892\n",
      "Epoch: 007, Loss: 0.19191\n",
      "Epoch: 008, Loss: 0.18725\n",
      "Epoch: 009, Loss: 0.18285\n",
      "Epoch: 010, Loss: 0.17969\n",
      "Epoch: 011, Loss: 0.17705\n",
      "Epoch: 012, Loss: 0.17526\n",
      "Epoch: 013, Loss: 0.17210\n",
      "Epoch: 014, Loss: 0.17050\n",
      "Epoch: 015, Loss: 0.16862\n",
      "Epoch: 016, Loss: 0.16711\n",
      "Epoch: 017, Loss: 0.16533\n",
      "Epoch: 018, Loss: 0.16390\n",
      "Epoch: 019, Loss: 0.16287\n",
      "Epoch: 020, Loss: 0.16133\n",
      "Best training R2 for target 5: 0.8525178905415829\n",
      "Best test R2 for target 5: 0.8503384060262259\n",
      "Target index:  6\n",
      "Initial training R2:  -0.016569716452883656\n",
      "Initial test R2:  -0.019000348652367938\n",
      "Epoch: 001, Loss: 0.34956\n",
      "Epoch: 002, Loss: 0.13672\n",
      "Epoch: 003, Loss: 0.09012\n",
      "Epoch: 004, Loss: 0.08017\n",
      "Epoch: 005, Loss: 0.07513\n",
      "Epoch: 006, Loss: 0.07152\n",
      "Epoch: 007, Loss: 0.06937\n",
      "Epoch: 008, Loss: 0.06812\n",
      "Epoch: 009, Loss: 0.06600\n",
      "Epoch: 010, Loss: 0.06503\n",
      "Epoch: 011, Loss: 0.06422\n",
      "Epoch: 012, Loss: 0.06269\n",
      "Epoch: 013, Loss: 0.06192\n",
      "Epoch: 014, Loss: 0.06143\n",
      "Epoch: 015, Loss: 0.06127\n",
      "Epoch: 016, Loss: 0.06090\n",
      "Epoch: 017, Loss: 0.06016\n",
      "Epoch: 018, Loss: 0.05929\n",
      "Epoch: 019, Loss: 0.05912\n",
      "Epoch: 020, Loss: 0.05889\n",
      "Best training R2 for target 6: 0.9551711734279436\n",
      "Best test R2 for target 6: 0.9529787811572525\n",
      "Target index:  7\n",
      "Initial training R2:  -0.017588421650010122\n",
      "Initial test R2:  -0.01987757820897951\n",
      "Epoch: 001, Loss: 0.37236\n",
      "Epoch: 002, Loss: 0.16974\n",
      "Epoch: 003, Loss: 0.13013\n",
      "Epoch: 004, Loss: 0.11872\n",
      "Epoch: 005, Loss: 0.11221\n",
      "Epoch: 006, Loss: 0.10797\n",
      "Epoch: 007, Loss: 0.10558\n",
      "Epoch: 008, Loss: 0.10324\n",
      "Epoch: 009, Loss: 0.10113\n",
      "Epoch: 010, Loss: 0.09997\n",
      "Epoch: 011, Loss: 0.09964\n",
      "Epoch: 012, Loss: 0.09837\n",
      "Epoch: 013, Loss: 0.09782\n",
      "Epoch: 014, Loss: 0.09684\n",
      "Epoch: 015, Loss: 0.09620\n",
      "Epoch: 016, Loss: 0.09596\n",
      "Epoch: 017, Loss: 0.09569\n",
      "Epoch: 018, Loss: 0.09454\n",
      "Epoch: 019, Loss: 0.09414\n",
      "Epoch: 020, Loss: 0.09392\n",
      "Best training R2 for target 7: 0.920172195538476\n",
      "Best test R2 for target 7: 0.9168450292658281\n",
      "Target index:  8\n",
      "Initial training R2:  -0.017800306739373505\n",
      "Initial test R2:  -0.016984664227168923\n",
      "Epoch: 001, Loss: 0.47503\n",
      "Epoch: 002, Loss: 0.32488\n",
      "Epoch: 003, Loss: 0.29816\n",
      "Epoch: 004, Loss: 0.28315\n",
      "Epoch: 005, Loss: 0.27019\n",
      "Epoch: 006, Loss: 0.26035\n",
      "Epoch: 007, Loss: 0.25028\n",
      "Epoch: 008, Loss: 0.24012\n",
      "Epoch: 009, Loss: 0.23424\n",
      "Epoch: 010, Loss: 0.22802\n",
      "Epoch: 011, Loss: 0.22358\n",
      "Epoch: 012, Loss: 0.21924\n",
      "Epoch: 013, Loss: 0.21448\n",
      "Epoch: 014, Loss: 0.21248\n",
      "Epoch: 015, Loss: 0.21082\n",
      "Epoch: 016, Loss: 0.20706\n",
      "Epoch: 017, Loss: 0.20329\n",
      "Epoch: 018, Loss: 0.20152\n",
      "Epoch: 019, Loss: 0.19976\n",
      "Epoch: 020, Loss: 0.19826\n",
      "Best training R2 for target 8: 0.8048886073117744\n",
      "Best test R2 for target 8: 0.8043829131692173\n",
      "Target index:  9\n",
      "Initial training R2:  -0.016665912447528068\n",
      "Initial test R2:  -0.016718129018806652\n",
      "Epoch: 001, Loss: 0.21575\n",
      "Epoch: 002, Loss: 0.09124\n",
      "Epoch: 003, Loss: 0.08770\n",
      "Epoch: 004, Loss: 0.08564\n",
      "Epoch: 005, Loss: 0.08436\n",
      "Epoch: 006, Loss: 0.08366\n",
      "Epoch: 007, Loss: 0.08196\n",
      "Epoch: 008, Loss: 0.08146\n",
      "Epoch: 009, Loss: 0.08138\n",
      "Epoch: 010, Loss: 0.08082\n",
      "Epoch: 011, Loss: 0.07982\n",
      "Epoch: 012, Loss: 0.07948\n",
      "Epoch: 013, Loss: 0.07866\n",
      "Epoch: 014, Loss: 0.07837\n",
      "Epoch: 015, Loss: 0.07816\n",
      "Epoch: 016, Loss: 0.07814\n",
      "Epoch: 017, Loss: 0.07736\n",
      "Epoch: 018, Loss: 0.07650\n",
      "Epoch: 019, Loss: 0.07737\n",
      "Epoch: 020, Loss: 0.07598\n",
      "Best training R2 for target 9: 0.9361102795233491\n",
      "Best test R2 for target 9: 0.9370695113856015\n",
      "Target index:  10\n",
      "Initial training R2:  -0.017364368297613595\n",
      "Initial test R2:  -0.01713278458127302\n",
      "Epoch: 001, Loss: 0.45867\n",
      "Epoch: 002, Loss: 0.33914\n",
      "Epoch: 003, Loss: 0.32958\n",
      "Epoch: 004, Loss: 0.32283\n",
      "Epoch: 005, Loss: 0.31714\n",
      "Epoch: 006, Loss: 0.31086\n",
      "Epoch: 007, Loss: 0.30528\n",
      "Epoch: 008, Loss: 0.29636\n",
      "Epoch: 009, Loss: 0.28784\n",
      "Epoch: 010, Loss: 0.27824\n",
      "Epoch: 011, Loss: 0.26954\n",
      "Epoch: 012, Loss: 0.26180\n",
      "Epoch: 013, Loss: 0.25419\n",
      "Epoch: 014, Loss: 0.24885\n",
      "Epoch: 015, Loss: 0.24295\n",
      "Epoch: 016, Loss: 0.24087\n",
      "Epoch: 017, Loss: 0.23793\n",
      "Epoch: 018, Loss: 0.23556\n",
      "Epoch: 019, Loss: 0.23349\n",
      "Epoch: 020, Loss: 0.23147\n",
      "Best training R2 for target 10: 0.7793755651862267\n",
      "Best test R2 for target 10: 0.7908691529239255\n",
      "Target index:  11\n",
      "Initial training R2:  -0.01736436466257797\n",
      "Initial test R2:  -0.01713273523164014\n",
      "Epoch: 001, Loss: 0.45880\n",
      "Epoch: 002, Loss: 0.33916\n",
      "Epoch: 003, Loss: 0.32958\n",
      "Epoch: 004, Loss: 0.32288\n",
      "Epoch: 005, Loss: 0.31722\n",
      "Epoch: 006, Loss: 0.31099\n",
      "Epoch: 007, Loss: 0.30545\n",
      "Epoch: 008, Loss: 0.29672\n",
      "Epoch: 009, Loss: 0.28818\n",
      "Epoch: 010, Loss: 0.27862\n",
      "Epoch: 011, Loss: 0.27021\n",
      "Epoch: 012, Loss: 0.26226\n",
      "Epoch: 013, Loss: 0.25432\n",
      "Epoch: 014, Loss: 0.24882\n",
      "Epoch: 015, Loss: 0.24300\n",
      "Epoch: 016, Loss: 0.24071\n",
      "Epoch: 017, Loss: 0.23776\n",
      "Epoch: 018, Loss: 0.23521\n",
      "Epoch: 019, Loss: 0.23325\n",
      "Epoch: 020, Loss: 0.23119\n",
      "Best training R2 for target 11: 0.7805941048124122\n",
      "Best test R2 for target 11: 0.791619032127061\n",
      "Target index:  12\n",
      "Initial training R2:  -0.017364365211631653\n",
      "Initial test R2:  -0.017132735507439385\n",
      "Epoch: 001, Loss: 0.45876\n",
      "Epoch: 002, Loss: 0.33912\n",
      "Epoch: 003, Loss: 0.32959\n",
      "Epoch: 004, Loss: 0.32277\n",
      "Epoch: 005, Loss: 0.31708\n",
      "Epoch: 006, Loss: 0.31078\n",
      "Epoch: 007, Loss: 0.30503\n",
      "Epoch: 008, Loss: 0.29596\n",
      "Epoch: 009, Loss: 0.28713\n",
      "Epoch: 010, Loss: 0.27731\n",
      "Epoch: 011, Loss: 0.26877\n",
      "Epoch: 012, Loss: 0.26159\n",
      "Epoch: 013, Loss: 0.25402\n",
      "Epoch: 014, Loss: 0.24879\n",
      "Epoch: 015, Loss: 0.24293\n",
      "Epoch: 016, Loss: 0.24042\n",
      "Epoch: 017, Loss: 0.23767\n",
      "Epoch: 018, Loss: 0.23529\n",
      "Epoch: 019, Loss: 0.23320\n",
      "Epoch: 020, Loss: 0.23094\n",
      "Best training R2 for target 12: 0.7804756162043607\n",
      "Best test R2 for target 12: 0.7917490501930888\n",
      "Target index:  13\n",
      "Initial training R2:  -0.01736438236310565\n",
      "Initial test R2:  -0.017132841159794427\n",
      "Epoch: 001, Loss: 0.45887\n",
      "Epoch: 002, Loss: 0.33927\n",
      "Epoch: 003, Loss: 0.32977\n",
      "Epoch: 004, Loss: 0.32280\n",
      "Epoch: 005, Loss: 0.31719\n",
      "Epoch: 006, Loss: 0.31120\n",
      "Epoch: 007, Loss: 0.30555\n",
      "Epoch: 008, Loss: 0.29675\n",
      "Epoch: 009, Loss: 0.28807\n",
      "Epoch: 010, Loss: 0.27883\n",
      "Epoch: 011, Loss: 0.27063\n",
      "Epoch: 012, Loss: 0.26326\n",
      "Epoch: 013, Loss: 0.25576\n",
      "Epoch: 014, Loss: 0.25075\n",
      "Epoch: 015, Loss: 0.24497\n",
      "Epoch: 016, Loss: 0.24225\n",
      "Epoch: 017, Loss: 0.23918\n",
      "Epoch: 018, Loss: 0.23672\n",
      "Epoch: 019, Loss: 0.23453\n",
      "Epoch: 020, Loss: 0.23212\n",
      "Best training R2 for target 13: 0.7805468861408887\n",
      "Best test R2 for target 13: 0.7917087618526121\n",
      "Target index:  14\n",
      "Initial training R2:  -0.016812941087139623\n",
      "Initial test R2:  -0.01641816365757765\n",
      "Epoch: 001, Loss: 0.37350\n",
      "Epoch: 002, Loss: 0.26317\n",
      "Epoch: 003, Loss: 0.25462\n",
      "Epoch: 004, Loss: 0.25044\n",
      "Epoch: 005, Loss: 0.24564\n",
      "Epoch: 006, Loss: 0.24187\n",
      "Epoch: 007, Loss: 0.23616\n",
      "Epoch: 008, Loss: 0.23209\n",
      "Epoch: 009, Loss: 0.22682\n",
      "Epoch: 010, Loss: 0.22074\n",
      "Epoch: 011, Loss: 0.21211\n",
      "Epoch: 012, Loss: 0.20324\n",
      "Epoch: 013, Loss: 0.19537\n",
      "Epoch: 014, Loss: 0.19013\n",
      "Epoch: 015, Loss: 0.18668\n",
      "Epoch: 016, Loss: 0.18508\n",
      "Epoch: 017, Loss: 0.18218\n",
      "Epoch: 018, Loss: 0.18100\n",
      "Epoch: 019, Loss: 0.18156\n",
      "Epoch: 020, Loss: 0.17853\n",
      "Best training R2 for target 14: 0.8324319187720561\n",
      "Best test R2 for target 14: 0.8361393717094485\n"
     ]
    }
   ],
   "source": [
    "def train(data_in, target):\n",
    "      model.train()\n",
    "      optimizer.zero_grad()  # Clear gradients.\n",
    "      out = model(data_in.x, data_in.edge_index, data_in.edge_attr, data_in.batch, data_in.descriptors, data_in.fingerprints)\n",
    "      targets = data_in.y[:,target].reshape(-1,1)\n",
    "      \n",
    "      #Alt 1\n",
    "      loss = criterion(out, targets)   \n",
    "\n",
    "      loss.backward()  # Derive gradients.\n",
    "      optimizer.step()  # Update parameters based on gradients.\n",
    "      return loss\n",
    "\n",
    "def test(data, target):\n",
    "      all_test_r2 = []\n",
    "      counter = -1    \n",
    "      for data_in in data:\n",
    "            counter += 1\n",
    "            model.eval()\n",
    "            out = model(data_in.x, data_in.edge_index, data_in.edge_attr, data_in.batch, data_in.descriptors, data_in.fingerprints)\n",
    "            targets = data_in.y[:,target].reshape(-1,1)\n",
    "            \n",
    "            # Caculate R2\n",
    "            r2_score_var = r2_score(targets.detach().numpy(), out.detach().numpy())\n",
    "\n",
    "            all_test_r2 .append(r2_score_var)\n",
    "\n",
    "      average_test_r2 = np.sum(all_test_r2) / len(all_test_r2)\n",
    "\n",
    "      return average_test_r2\n",
    "\n",
    "num_targets = dataset[50].y.shape[1]\n",
    "for target_index in range(num_targets):\n",
    "      print(\"Target index: \", target_index)\n",
    "\n",
    "      model = GNN(hidden_channels=64) \n",
    "      optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=5e-4)\n",
    "      criterion = torch.nn.MSELoss()\n",
    "\n",
    "      # Vectors to append accuracy to:\n",
    "      Train_r2 = []\n",
    "      Test_r2 = []\n",
    "\n",
    "      # Calculate accuracy before training \n",
    "      Train_r2.append(test(train_loader, target_index))\n",
    "      Test_r2.append(test(test_loader, target_index))\n",
    "      print(\"Initial training R2: \", Train_r2[0])\n",
    "      print(\"Initial test R2: \", Test_r2[0])\n",
    "\n",
    "      print_r2_option = True\n",
    "      for epoch in range(1, 21):\n",
    "            average_loss = []\n",
    "            for data in train_loader:\n",
    "                  loss = train(data, target_index)\n",
    "                  average_loss.append(loss)\n",
    "            print(f'Epoch: {epoch:03d}, Loss: {(sum(average_loss)/len(average_loss)):.5f}')\n",
    "\n",
    "            if print_r2_option:\n",
    "                  temp_train_r2 = test(train_loader, target_index)\n",
    "                  Train_r2.append(temp_train_r2)\n",
    "\n",
    "                  temp_test_r2 = test(test_loader, target_index)\n",
    "                  Test_r2.append(temp_test_r2)\n",
    "\n",
    "                  # print(f'Average Train R2: {temp_train_r2}')\n",
    "                  # print(f'Average Test R2: {temp_test_r2:}')\n",
    "\n",
    "      print(f\"Best training R2 for target {target_index}: {np.max(Train_r2)}\")\n",
    "      print(f\"Best test R2 for target {target_index}: {np.max(Test_r2)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
