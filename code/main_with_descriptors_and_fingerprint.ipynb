{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TIF360 Project"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main source: https://www.kaggle.com/code/rmonge/predicting-molecule-properties-based-on-its-smiles/notebook"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import rdkit\n",
    "from rdkit import Chem  # To extract information of the molecules\n",
    "from rdkit.Chem import Draw  # To draw the molecules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch_geometric\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch_geometric.utils as utils\n",
    "import networkx as nx\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import global_mean_pool, GraphConv, GATConv, GCNConv\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/smiles_and_targets.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "mol_descriptor = np.load(\"../data/mol_descriptors.npy\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Morgan fingerprints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "mol_fingerprints = np.load(\"../data/mol_morgan_fingerprints.npy\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigate Neural Networks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Neural Networks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert data to graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "from rdkit.Chem import GetAdjacencyMatrix\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch.utils.data import DataLoader\n",
    "from rdkit.Chem import Descriptors\n",
    "from rdkit.ML.Descriptors import MoleculeDescriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(x, permitted_list):\n",
    "    if x not in permitted_list:\n",
    "        x = permitted_list[-1]  # If the atom is not in the list, get \"Unknown\"\n",
    "        \n",
    "    binary_encoding = [int(boolean) for boolean in list(map(lambda s: x==s, permitted_list))]\n",
    "    \n",
    "    return binary_encoding    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Atom featurisation\\\n",
    "Currently generates ca. 80 node features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_atom_features(atom, use_chirality = True, hydrogens_implicit = True):\n",
    "    # list of permitted atoms\n",
    "    permitted_atom_list = ['C','N','O','S','F','Si','P','Cl','Br','Mg','Na','Ca',\n",
    "                           'Fe','As','Al','I', 'B','V','K','Tl','Yb','Sb','Sn','Ag','Pd','Co',\n",
    "                           'Se','Ti','Zn', 'Li','Ge','Cu','Au','Ni','Cd','In','Mn','Zr','Cr','Pt',\n",
    "                           'Hg','Pb','Unknown']\n",
    "    \n",
    "    atom_type_enc = one_hot_encoding(str(atom.GetSymbol()), permitted_atom_list)\n",
    "    \n",
    "    n_heavy_neighbors = one_hot_encoding(int(atom.GetDegree()), [0,1,2,3,4,\"MoreThanFour\"])\n",
    "    \n",
    "    formal_charge_enc = one_hot_encoding(int(atom.GetFormalCharge()), [-3, -2, -1, 0, 1, 2, 3, 'Extreme'])\n",
    "    \n",
    "    hybridisation_type_enc = one_hot_encoding(str(atom.GetHybridization()), [\"S\", \"SP\", \"SP2\", \"SP3\", \"SP3D\", \"SP3D2\", \"OTHER\"])\n",
    "    \n",
    "    is_in_ring_enc = one_hot_encoding(int(atom.IsInRing()), [0, 1])\n",
    "    \n",
    "    is_aromatic_enc = one_hot_encoding(int(atom.GetIsAromatic()), [0, 1])\n",
    "    \n",
    "    atomic_mass_scaled = [float(atom.GetMass() - 10.812)/116.092] # (?) replace 10.812 with mean the and 116.092 with std\n",
    "    \n",
    "    vdw_radius_scaled = [float((Chem.GetPeriodicTable().GetRvdw(atom.GetAtomicNum()) - 1.5)/0.6)] # (?) replace 1.5 with mean the and 0.6 with std\n",
    "    \n",
    "    covalent_radius_scaled = [float((Chem.GetPeriodicTable().GetRcovalent(atom.GetAtomicNum()) - 0.64)/0.76)] # (?) replace 0.64 with mean the and 0.76 with std\n",
    "                              \n",
    "    atom_feature_vector = atom_type_enc + n_heavy_neighbors + formal_charge_enc + hybridisation_type_enc + is_in_ring_enc + is_aromatic_enc + atomic_mass_scaled + vdw_radius_scaled + covalent_radius_scaled\n",
    "    \n",
    "    if use_chirality:\n",
    "        chirality_type_enc = one_hot_encoding(str(atom.GetChiralTag()), [\"CHI_UNSPECIFIED\", \"CHI_TETRAHEDRAL_CW\", \"CHI_TETRAHEDRAL_CCW\", \"CHI_OTHER\"])\n",
    "        atom_feature_vector += chirality_type_enc\n",
    "        \n",
    "    if hydrogens_implicit:\n",
    "        n_hydrogens_enc = one_hot_encoding(int(atom.GetTotalNumHs()), [0, 1, 2, 3, 4, \"MoreThanFour\"])\n",
    "        atom_feature_vector += n_hydrogens_enc\n",
    "        \n",
    "    return np.array(atom_feature_vector) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bond Featurisation\\\n",
    "Currently generates ca. 10 edge features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bond_features(bond, use_stereochemistry=True):\n",
    "    permitted_bond_types = [Chem.rdchem.BondType.SINGLE, Chem.rdchem.BondType.DOUBLE, \n",
    "                            Chem.rdchem.BondType.TRIPLE, Chem.rdchem.BondType.AROMATIC]\n",
    "    \n",
    "    bond_type_enc = one_hot_encoding(str(bond.GetBondType()), permitted_bond_types)\n",
    "    \n",
    "    bond_is_conjugated_enc = [int(bond.GetIsConjugated())]\n",
    "    \n",
    "    bond_is_in_ring_enc = [int(bond.IsInRing())]\n",
    "    \n",
    "    bond_feature_vector = bond_type_enc + bond_is_conjugated_enc + bond_is_in_ring_enc\n",
    "    \n",
    "    if use_stereochemistry:\n",
    "        stereo_type_enc = one_hot_encoding(str(bond.GetStereo()), [\"STEREOZ\", \"STEREOE\", \"STEREOANY\", \"STEREONONE\"])\n",
    "        bond_feature_vector += stereo_type_enc\n",
    "        \n",
    "    return np.array(bond_feature_vector)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function to generate dataset of labeled Pytorch Geometric Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph_dataset_from_smiles(x_smiles, y):\n",
    "    ## Inputs:\n",
    "    # x_smiles = [smiles_1, smiles_2, ...], smiles representation of molecules\n",
    "    # y = [y_1, y_2, ...] list of numerical labels for each smiles string, here chemical properties\n",
    "    \n",
    "    # Outputs:\n",
    "    # dataset = [data_1, data_2, ...] list of torch_geometric.data.Data objects representing molecular graphs\n",
    "    \n",
    "    dataset = []\n",
    "    \n",
    "    for (smiles, y_val) in zip(x_smiles, y):\n",
    "        # convert smiles to molecular object\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        \n",
    "        # get feature dimensions\n",
    "        n_nodes = mol.GetNumAtoms()\n",
    "        n_edges = 2*mol.GetNumBonds() # each bond is represented twice in the adjacency matrix\n",
    "        n_node_features = len(get_atom_features(mol.GetAtomWithIdx(0)))\n",
    "        if n_nodes > 1:\n",
    "            n_edge_features = len(get_bond_features(mol.GetBondBetweenAtoms(0,1)))\n",
    "        else:\n",
    "            n_edge_features = 0  # for single atom molecules -> no edges\n",
    "        \n",
    "        # construct node feature matrix X \n",
    "        X = np.zeros((n_nodes, n_node_features))\n",
    "        \n",
    "        for atom in mol.GetAtoms():\n",
    "            X[atom.GetIdx(), :] = get_atom_features(atom)\n",
    "        \n",
    "        X = torch.tensor(X, dtype=torch.float)\n",
    "        \n",
    "        # construct edge index array E, shape = (2, n_edges)\n",
    "        (rows, cols) = np.nonzero(GetAdjacencyMatrix(mol))\n",
    "        torch_rows = torch.tensor(rows.astype(np.int64)).to(torch.long)\n",
    "        torch_cols = torch.tensor(cols.astype(np.int64)).to(torch.long)\n",
    "        E = torch.stack([torch_rows, torch_cols], dim=0)\n",
    "        \n",
    "        # construct edge feature matrix EF\n",
    "        EF = np.zeros((n_edges, n_edge_features))       # Note: generates zero matrix if n_edges = n_edge_features = 0\n",
    "        for (k, (i,j)) in enumerate(zip(rows, cols)):\n",
    "            EF[k] = get_bond_features(mol.GetBondBetweenAtoms(int(i),int(j)))\n",
    "        EF = torch.tensor(EF, dtype=torch.float)\n",
    "        \n",
    "        # construct label/y tensor\n",
    "        y_tensor = torch.tensor(np.array([y_val]), dtype=torch.float)\n",
    "        \n",
    "        # construct torch_geometric.data.Data object and append to dataset\n",
    "        dataset.append(Data(x=X, edge_index=E, edge_attr=EF, y=y_tensor))\n",
    "        \n",
    "    return dataset\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the above functions to create a dataset of molecular graphs from the smiles and labels corresponding to chemical properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties_names = ['A', 'B', 'C', 'mu', 'alfa', 'homo', 'lumo', 'gap', 'R²', 'zpve', 'U0', 'U', 'H', 'G', 'Cv']\n",
    "\n",
    "x_smiles = df.smiles.values\n",
    "y = df.loc[:, properties_names].values  # shape = (n_samples, n_properties)\n",
    "\n",
    "dataset = create_graph_dataset_from_smiles(x_smiles, y[0:len(x_smiles), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132820\n",
      "C#CC#C\n",
      "Data(x=[4, 81], edge_index=[2, 6], edge_attr=[6, 10], y=[1, 15])\n",
      "N#CC#N\n",
      "Data(x=[4, 81], edge_index=[2, 6], edge_attr=[6, 10], y=[1, 15])\n",
      "CC1=CNC=C1\n",
      "Data(x=[6, 81], edge_index=[2, 12], edge_attr=[12, 10], y=[1, 15])\n",
      "tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 1., 1., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 1., 1., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 1., 1., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 1., 1., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 1., 1., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 1., 1., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 1., 1., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 1., 1., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 1., 1., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 1., 1., 0., 0., 0., 1.]])\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
      "         0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.0103, 0.3333, 0.0526, 1.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000],\n",
      "        [1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0103, 0.3333, 0.0526, 1.0000,\n",
      "         0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0103, 0.3333, 0.0526, 1.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0275, 0.1667, 0.0526, 1.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0103, 0.3333, 0.0526, 1.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0103, 0.3333, 0.0526, 1.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))\n",
    "\n",
    "# Example entries\n",
    "print(df.smiles.values[0])\n",
    "print(dataset[0])\n",
    "print(df.smiles.values[2])\n",
    "print(dataset[2])\n",
    "print(df.smiles.values[50])\n",
    "print(dataset[50])\n",
    "\n",
    "print(dataset[50].edge_attr)\n",
    "print(dataset[50].x)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information of the graph dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of graphs (molecules): 132820\n",
      "=================================================================================\n",
      "Properties of graph 50, molecule smiles: CC1=CNC=C1\n",
      "Number of nodes: 6\n",
      "Number of edges: 12\n",
      "Number of node features: 81\n",
      "Number of edge features: 10\n",
      "Number of properties: 15\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of graphs (molecules): {len(dataset)}')\n",
    "graph = dataset[50]\n",
    "print('=================================================================================')\n",
    "print(f'Properties of graph {50}, molecule smiles: {df.smiles.values[50]}')\n",
    "print(f'Number of nodes: {dataset[50].x.shape[0]}')\n",
    "print(f'Number of edges: {dataset[50].edge_index.shape[1]}')\n",
    "print(f'Number of node features: {dataset[50].x.shape[1]}')\n",
    "print(f'Number of edge features: {dataset[50].edge_attr.shape[1]}')\n",
    "print(f'Number of properties: {dataset[50].y.shape[1]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filterout data with no edge features defined (Like ex: CH4) (These causes problems down the line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of none edge feature molecules:  0\n",
      "Before:  132820\n",
      "After:  132820\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "indexes_to_delete = []\n",
    "for item in range(0,len(dataset)):\n",
    "    if dataset[item].edge_attr.shape[1] == 0:\n",
    "        indexes_to_delete.append(item)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "indexes_to_delete.sort()\n",
    "\n",
    "print(\"Number of none edge feature molecules: \", len(indexes_to_delete))\n",
    "\n",
    "print(\"Before: \", len(dataset))\n",
    "\n",
    "for item in range(0,len(indexes_to_delete)):\n",
    "    print(\"Molecule to delete: \", df.smiles.values[indexes_to_delete[item]])\n",
    "    #del dataset[indexes_to_delete[item] - item] \n",
    "    dataset.pop((indexes_to_delete[item] - item)) # -item since all future data points will have its index reduced by 1 for each deleted previous data point\n",
    "print(\"After: \", len(dataset))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example train data target before scaling tensor([[ 2.9693e+00,  1.9777e+00,  1.8423e+00,  1.2093e+00,  8.4060e+01,\n",
      "         -1.8970e-01, -7.0400e-02,  1.1930e-01,  8.7339e+02,  1.5988e-01,\n",
      "         -3.4861e+02, -3.4860e+02, -3.4860e+02, -3.4864e+02,  2.9189e+01]])\n",
      "Example train data target after scaling: tensor([[-0.3350,  1.3300,  2.2841, -1.0264,  1.0990,  2.2975, -1.7445, -2.7966,\n",
      "         -1.1527,  0.3357,  1.6155,  1.6155,  1.6155,  1.6156, -0.6067]])\n",
      "Total data size:  132820\n",
      "Train data size:  106256\n",
      "Test data size:  26564\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# split the dataset into test and validation:\n",
    "num_samples = len(dataset)\n",
    "\n",
    "# Want to divide data randomly\n",
    "random.seed(42)\n",
    "random_indexes = np.array(random.sample(range(num_samples), num_samples)) # random.sample ensures no duplicates\n",
    "\n",
    "train_data = [dataset[index] for index in random_indexes[int(.2 * num_samples ) :]] # 80%\n",
    "test_data = [dataset[index] for index in random_indexes[: int(.2 * num_samples )]] # 20%\n",
    "\n",
    "print(\"Example train data target before scaling\", train_data[0].y)\n",
    "train_data_targets = [data.y for data in train_data]\n",
    "train_data_targets = torch.concatenate(train_data_targets, axis=0)\n",
    "test_data_targets = [data.y for data in test_data]\n",
    "test_data_targets = torch.concatenate(test_data_targets, axis=0)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_data_targets = scaler.fit_transform(train_data_targets)\n",
    "test_data_targets = scaler.transform(test_data_targets)\n",
    "\n",
    "# print(\"Example train data target\", train_data_targets[0].reshape(1,-1).shape)\n",
    "train_data_targets = torch.tensor(train_data_targets, dtype=torch.float)\n",
    "test_data_targets = torch.tensor(test_data_targets, dtype=torch.float)\n",
    "\n",
    "train_data = [Data(x=data.x, edge_index=data.edge_index, edge_attr=data.edge_attr, y=train_data_targets[index].reshape(1,-1)) for index, data in enumerate(train_data)]\n",
    "test_data = [Data(x=data.x, edge_index=data.edge_index, edge_attr=data.edge_attr, y=test_data_targets[index].reshape(1,-1)) for index, data in enumerate(test_data)]\n",
    "print(\"Example train data target after scaling:\", train_data[0].y)\n",
    "\n",
    "print(\"Total data size: \", len(dataset))\n",
    "print(\"Train data size: \", len(train_data))\n",
    "print(\"Test data size: \", len(test_data))\n",
    "\n",
    "#train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "#test_loader = DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide descriptors and fingerprints into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_descriptor = [mol_descriptor[index,:] for index in random_indexes[int(.2 * num_samples ) :]] # 80%\n",
    "test_data_desriptors = [mol_descriptor[index,:] for index in random_indexes[: int(.2 * num_samples )]] # 20%\n",
    "\n",
    "train_data_fingerprints = [mol_fingerprints[index,:] for index in random_indexes[int(.2 * num_samples ) :]] # 80%\n",
    "test_data_fingerprints = [mol_fingerprints[index,:] for index in random_indexes[: int(.2 * num_samples )]] # 20%\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize using MinMax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "minmax_scaler = MinMaxScaler()\n",
    "\n",
    "train_data_descriptor = minmax_scaler.fit_transform(train_data_descriptor)\n",
    "\n",
    "test_data_desriptors = minmax_scaler.transform(test_data_desriptors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "minmax_scaler = MinMaxScaler()\n",
    "\n",
    "train_data_fingerprints = minmax_scaler.fit_transform(train_data_fingerprints)\n",
    "\n",
    "test_data_fingerprints = minmax_scaler.transform(test_data_fingerprints)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine descriptors temporarely with in train and test data for graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[9, 81], edge_index=[2, 20], edge_attr=[20, 10], y=[1, 15], descriptors=[209])\n"
     ]
    }
   ],
   "source": [
    "#Testing stuff\n",
    "\n",
    "temp = train_data[1000]\n",
    "temp.descriptors = train_data_descriptor[1000,:]\n",
    "\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[9, 81], edge_index=[2, 24], edge_attr=[24, 10], y=[1, 15], descriptors=[1, 209], fingerprints=[1, 2048])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for index in range(0,len(train_data)):\n",
    "    train_data[index].descriptors = torch.FloatTensor([train_data_descriptor[index,:]])\n",
    "    train_data[index].fingerprints = torch.FloatTensor([train_data_fingerprints[index,:]])\n",
    "\n",
    "for index in range(0,len(test_data)):\n",
    "    test_data[index].descriptors = torch.FloatTensor([test_data_desriptors[index,:]])\n",
    "    test_data[index].fingerprints = torch.FloatTensor([test_data_fingerprints[index,:]])\n",
    "\n",
    "# Example\n",
    "print(train_data[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main GNN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model for all targets at once"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GNN function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_labels = dataset[50].y.shape[1]\n",
    "data_features = dataset[50].x.shape[1]\n",
    "descriptors_features = train_data[0].descriptors.shape[1]\n",
    "fingerprints_features = train_data[0].fingerprints.shape[1]\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GATConv(data_features, hidden_channels)\n",
    "        self.conv2 = GATConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GATConv(hidden_channels, hidden_channels)\n",
    "        self.conv4 = GATConv(hidden_channels, hidden_channels)\n",
    "        self.conv5 = GATConv(hidden_channels, hidden_channels)\n",
    "        self.lin = Linear(hidden_channels+descriptors_features+fingerprints_features, 512)\n",
    "        self.lin2 = Linear(512, 256)\n",
    "        self.lin3 = Linear(256, data_labels)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch, descriptors, fingerprints): \n",
    "        x = self.conv1(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv4(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv5(x, edge_index, edge_attr)\n",
    "\n",
    "        #Returns batch-wise graph-level-outputs by averaging node features across the node dimension, so that for a single graph G\n",
    "        #its output is computed by\n",
    "        x = global_mean_pool(x, batch) \n",
    "        x2 = descriptors\n",
    "        x3 = fingerprints\n",
    "        x = torch.cat((x,x2,x3),1)\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = F.relu(x)\n",
    "        x = self.lin2(x)\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = F.relu(x)\n",
    "        x = self.lin3(x)\n",
    "        \n",
    " \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial training R2:  [-0.01786751 -0.01907158 -0.01778504 -0.01851151 -0.01937978 -0.01912908\n",
      " -0.01832286 -0.02028113 -0.01993596 -0.01715232 -0.01879422 -0.01726186\n",
      " -0.0173024  -0.01916103 -0.01718507]\n",
      "Initial test R2:  [-0.01919517 -0.02337165 -0.02472942 -0.01925437 -0.01813356 -0.03530905\n",
      " -0.01944394 -0.0249084  -0.02150489 -0.01534811 -0.01938838 -0.01746742\n",
      " -0.01681031 -0.02003296 -0.01818333]\n",
      "Epoch: 001, Loss: 0.3745\n",
      "Average Train R2: [0.48134638 0.58089365 0.64129422 0.55033842 0.68763284 0.76551601\n",
      " 0.90070546 0.8567328  0.73795854 0.92937347 0.72811626 0.72897797\n",
      " 0.72834824 0.72837007 0.79489107]\n",
      "Average Test R2: [0.48380134 0.58176591 0.64277423 0.54239914 0.68742244 0.75976413\n",
      " 0.89740615 0.85148402 0.73870801 0.92912315 0.73505371 0.73588194\n",
      " 0.7352384  0.73521437 0.79119349]\n",
      "Epoch: 002, Loss: 0.3003\n",
      "Epoch: 003, Loss: 0.2668\n",
      "Average Train R2: [0.62506738 0.6749078  0.7129591  0.59675261 0.81297281 0.78442026\n",
      " 0.92294703 0.88452688 0.82402857 0.94119888 0.81886539 0.8190961\n",
      " 0.81925273 0.81923185 0.84429594]\n",
      "Average Test R2: [0.60618551 0.66014669 0.70033984 0.57926337 0.80226177 0.76833058\n",
      " 0.91876885 0.878294   0.81585916 0.93925377 0.81177378 0.81199968\n",
      " 0.81218841 0.81216209 0.83482457]\n",
      "Epoch: 004, Loss: 0.2471\n",
      "Epoch: 005, Loss: 0.2382\n",
      "Average Train R2: [0.66782565 0.71979573 0.74938    0.60198351 0.83421513 0.80395481\n",
      " 0.92687668 0.89436529 0.84343304 0.94711339 0.84206833 0.84205349\n",
      " 0.84210309 0.84208652 0.86666523]\n",
      "Average Test R2: [0.64867572 0.70382208 0.72902292 0.58772705 0.82398854 0.79629522\n",
      " 0.92103444 0.88716817 0.83266712 0.94465366 0.8353402  0.8353283\n",
      " 0.83536955 0.83535474 0.8567616 ]\n",
      "Epoch: 006, Loss: 0.2326\n",
      "Epoch: 007, Loss: 0.2280\n",
      "Average Train R2: [0.68313289 0.73649368 0.77007632 0.60673757 0.83036968 0.80120657\n",
      " 0.93651374 0.90546526 0.85319199 0.93568452 0.83532305 0.83532704\n",
      " 0.83522667 0.8353125  0.87300601]\n",
      "Average Test R2: [0.67104377 0.7203724  0.75553664 0.59400488 0.8197382  0.79368043\n",
      " 0.93140992 0.89782153 0.84363433 0.93356065 0.82894662 0.82895148\n",
      " 0.8288462  0.82893224 0.86311276]\n",
      "Epoch: 008, Loss: 0.2226\n",
      "Epoch: 009, Loss: 0.2205\n",
      "Average Train R2: [0.62499971 0.71703763 0.73174119 0.61470098 0.82250584 0.80465169\n",
      " 0.93835139 0.90492716 0.82380189 0.93980273 0.78364621 0.78365398\n",
      " 0.78373281 0.78366954 0.80687488]\n",
      "Average Test R2: [0.60119953 0.70032619 0.71696781 0.60025548 0.80836464 0.79864561\n",
      " 0.93379703 0.89818383 0.81324419 0.93760487 0.77551627 0.77552424\n",
      " 0.77560296 0.77553902 0.7966309 ]\n",
      "Epoch: 010, Loss: 0.2181\n",
      "Epoch: 011, Loss: 0.2171\n",
      "Average Train R2: [0.69677523 0.74713618 0.76107805 0.62249749 0.81680238 0.8030861\n",
      " 0.93350785 0.89164792 0.83722116 0.95314166 0.85503647 0.85504289\n",
      " 0.85502108 0.85502543 0.86974511]\n",
      "Average Test R2: [0.6801037  0.72925855 0.74386229 0.60879587 0.80175163 0.79488438\n",
      " 0.92812536 0.8832629  0.82550133 0.95070968 0.84476188 0.84476855\n",
      " 0.84474678 0.84474996 0.85946915]\n",
      "Epoch: 012, Loss: 0.2136\n",
      "Epoch: 013, Loss: 0.2126\n",
      "Average Train R2: [0.71168224 0.76507056 0.78778775 0.60694426 0.85543868 0.81658336\n",
      " 0.94217263 0.91044412 0.86340979 0.9546467  0.86241807 0.86242162\n",
      " 0.86242584 0.8624158  0.88287183]\n",
      "Average Test R2: [0.69608698 0.73808167 0.77227603 0.5879454  0.84323518 0.81107694\n",
      " 0.93732573 0.90011574 0.85304696 0.95262449 0.85496865 0.85497254\n",
      " 0.85497602 0.85496582 0.8733916 ]\n",
      "Epoch: 014, Loss: 0.2094\n",
      "Epoch: 015, Loss: 0.2095\n",
      "Average Train R2: [0.72160987 0.76941324 0.79220713 0.62803199 0.86210272 0.81554474\n",
      " 0.94583372 0.91082634 0.87484773 0.95763683 0.87396634 0.87396974\n",
      " 0.87396662 0.87396199 0.8957714 ]\n",
      "Average Test R2: [0.70734529 0.7529275  0.77688841 0.61130524 0.8492676  0.80787734\n",
      " 0.94081242 0.90355141 0.86530026 0.95539596 0.86485389 0.8648576\n",
      " 0.86485338 0.8648489  0.88641297]\n",
      "Epoch: 016, Loss: 0.2061\n",
      "Epoch: 017, Loss: 0.2049\n",
      "Average Train R2: [0.67446493 0.75946219 0.77583692 0.61542022 0.85214701 0.81526672\n",
      " 0.9456964  0.91313182 0.86799489 0.945553   0.87935778 0.8793611\n",
      " 0.87936197 0.87935409 0.88173296]\n",
      "Average Test R2: [0.66458268 0.74052202 0.75815454 0.59865909 0.83881275 0.79847406\n",
      " 0.94143506 0.90504388 0.85735554 0.94270517 0.87139749 0.87140109\n",
      " 0.8714014  0.87139331 0.87266548]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 65\u001b[0m\n\u001b[0;32m     63\u001b[0m average_loss \u001b[39m=\u001b[39m []\n\u001b[0;32m     64\u001b[0m \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m train_loader:\n\u001b[1;32m---> 65\u001b[0m       loss \u001b[39m=\u001b[39m train(data)\n\u001b[0;32m     66\u001b[0m       average_loss\u001b[39m.\u001b[39mappend(loss)\n\u001b[0;32m     67\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m:\u001b[39;00m\u001b[39m03d\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Loss: \u001b[39m\u001b[39m{\u001b[39;00m(\u001b[39msum\u001b[39m(average_loss)\u001b[39m/\u001b[39m\u001b[39mlen\u001b[39m(average_loss))\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[52], line 18\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(data_in)\u001b[0m\n\u001b[0;32m     11\u001b[0m loss \u001b[39m=\u001b[39m criterion(out, data_in\u001b[39m.\u001b[39my) \n\u001b[0;32m     13\u001b[0m \u001b[39m#Alt 2\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[39m#loss = 0\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[39m#for item in range(0,len(data_in.y[0,:])):\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[39m#      loss += criterion(out[:,item], data_in.y[:,item]) \u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()  \u001b[39m# Derive gradients.\u001b[39;00m\n\u001b[0;32m     19\u001b[0m optimizer\u001b[39m.\u001b[39mstep()  \u001b[39m# Update parameters based on gradients.\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = GNN(hidden_channels=128) \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005, weight_decay=5e-4)\n",
    "\n",
    "# Will multiply learning rate by 0.2 every 10 epoch (Haven't tried this yet)\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.2) \n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "def train(data_in):\n",
    "      model.train()\n",
    "      optimizer.zero_grad()  # Clear gradients.\n",
    "      out = model(data_in.x, data_in.edge_index, data_in.edge_attr, data_in.batch, data_in.descriptors, data_in.fingerprints)  # Perform a single forward pass.\n",
    "\n",
    "      #Alt 1\n",
    "      loss = criterion(out, data_in.y) \n",
    "\n",
    "      #Alt 2\n",
    "      #loss = 0\n",
    "      #for item in range(0,len(data_in.y[0,:])):\n",
    "      #      loss += criterion(out[:,item], data_in.y[:,item]) \n",
    "\n",
    "      loss.backward()  # Derive gradients.\n",
    "      # scheduler.step()\n",
    "      optimizer.step()  # Update parameters based on gradients.\n",
    "      return loss\n",
    "\n",
    "def test(data):\n",
    "      all_test_r2 = []\n",
    "      counter = -1    \n",
    "      for data_in in data:\n",
    "            counter += 1\n",
    "            model.eval()\n",
    "            \n",
    "            out = model(data_in.x, data_in.edge_index, data_in.edge_attr, data_in.batch, data_in.descriptors, data_in.fingerprints)\n",
    "\n",
    "            # Caculate R2\n",
    "            r2_score_var = []\n",
    "            for item in range(0,data_in.y.shape[1]):\n",
    "                  if item == 0:\n",
    "                        r2_score_var = r2_score(data_in.y[:,item].detach().numpy(), out[:,item].detach().numpy())\n",
    "                  else:\n",
    "                        r2_score_var = np.vstack((r2_score_var,(r2_score(data_in.y[:,item].detach().numpy(), out[:,item].detach().numpy()))))\n",
    "\n",
    "            if counter == 0:\n",
    "                  all_test_r2 = r2_score_var\n",
    "            else:\n",
    "                  all_test_r2 = np.hstack((all_test_r2,r2_score_var))\n",
    "\n",
    "      average_test_r2 = np.sum(all_test_r2,axis=1) / all_test_r2.shape[1]\n",
    "\n",
    "      return average_test_r2\n",
    "\n",
    "# Vectors to append accuracy to:\n",
    "Train_r2 = []\n",
    "Test_r2 = []\n",
    "N = 2 # How often it should calculate R2\n",
    "\n",
    "# Calculate accuracy before training \n",
    "Train_r2.append(test(train_loader))\n",
    "Test_r2.append(test(test_loader))\n",
    "print(\"Initial training R2: \", Train_r2[0])\n",
    "print(\"Initial test R2: \", Test_r2[0])\n",
    "\n",
    "print_r2_option = True\n",
    "counter = 1\n",
    "for epoch in range(1, 31):\n",
    "      counter += 1\n",
    "      average_loss = []\n",
    "      for data in train_loader:\n",
    "            loss = train(data)\n",
    "            average_loss.append(loss)\n",
    "      print(f'Epoch: {epoch:03d}, Loss: {(sum(average_loss)/len(average_loss)):.4f}')\n",
    "\n",
    "      if print_r2_option & counter % N == 0:\n",
    "\n",
    "            temp_train_r2 = test(train_loader)\n",
    "            Train_r2.append(temp_train_r2)\n",
    "\n",
    "            temp_test_r2 = test(test_loader)\n",
    "            Test_r2.append(temp_test_r2)\n",
    "\n",
    "            print(f'Average Train R2: {temp_train_r2}')\n",
    "            print(f'Average Test R2: {temp_test_r2:}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model for just one target"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GNN function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GATConv: https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GATConv.html#torch_geometric.nn.conv.GATConv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GENConv: https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GENConv.html#torch_geometric.nn.conv.GENConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_labels = 1\n",
    "data_features = dataset[50].x.shape[1]\n",
    "descriptors_features = train_data[50].descriptors.shape[1]\n",
    "fingerprints_features = train_data[0].fingerprints.shape[1]\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GATConv(data_features, hidden_channels)\n",
    "        self.conv2 = GATConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GATConv(hidden_channels, hidden_channels)\n",
    "        self.conv4 = GATConv(hidden_channels, hidden_channels)\n",
    "        self.conv5 = GATConv(hidden_channels, hidden_channels)\n",
    "        self.lin = Linear(hidden_channels+descriptors_features+fingerprints_features, 512)\n",
    "        self.lin2 = Linear(512, 256)\n",
    "        self.lin3 = Linear(256, data_labels)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch, descriptors, fingerprints): \n",
    "        x = self.conv1(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv4(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv5(x, edge_index, edge_attr)\n",
    "\n",
    "        #Returns batch-wise graph-level-outputs by averaging node features across the node dimension, so that for a single graph G\n",
    "        #its output is computed by\n",
    "        x = global_mean_pool(x, batch) \n",
    "        x2 = descriptors\n",
    "        x3 = fingerprints\n",
    "        x = torch.cat((x,x2,x3),1)\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = F.relu(x)\n",
    "        x = self.lin2(x)\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = F.relu(x)\n",
    "        x = self.lin3(x)\n",
    "        \n",
    " \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target index:  0\n",
      "Initial training R2:  -0.018678295383913163\n",
      "Initial test R2:  -0.03308373704488183\n",
      "Epoch: 001, Loss: 0.76507\n",
      "Epoch: 002, Loss: 0.63190\n",
      "Epoch: 003, Loss: 0.56252\n",
      "Epoch: 004, Loss: 0.51782\n",
      "Epoch: 005, Loss: 0.48434\n",
      "Epoch: 006, Loss: 0.46161\n",
      "Epoch: 007, Loss: 0.44506\n",
      "Epoch: 008, Loss: 0.43061\n",
      "Epoch: 009, Loss: 0.41689\n",
      "Epoch: 010, Loss: 0.40696\n",
      "Epoch: 011, Loss: 0.39623\n",
      "Epoch: 012, Loss: 0.38796\n",
      "Epoch: 013, Loss: 0.37597\n",
      "Epoch: 014, Loss: 0.37060\n",
      "Epoch: 015, Loss: 0.36610\n",
      "Epoch: 016, Loss: 0.35919\n",
      "Epoch: 017, Loss: 0.35353\n",
      "Epoch: 018, Loss: 0.34897\n",
      "Epoch: 019, Loss: 0.34382\n",
      "Epoch: 020, Loss: 0.34009\n",
      "Best training R2 for target 0: 0.6581509149316066\n",
      "Best test R2 for target 0: 0.6554282682601079\n",
      "Target index:  1\n",
      "Initial training R2:  -0.01632305111746684\n",
      "Initial test R2:  -0.017224460553526586\n",
      "Epoch: 001, Loss: 0.63536\n",
      "Epoch: 002, Loss: 0.51249\n",
      "Epoch: 003, Loss: 0.45557\n",
      "Epoch: 004, Loss: 0.41914\n",
      "Epoch: 005, Loss: 0.40053\n",
      "Epoch: 006, Loss: 0.38774\n",
      "Epoch: 007, Loss: 0.37415\n",
      "Epoch: 008, Loss: 0.36468\n",
      "Epoch: 009, Loss: 0.36037\n",
      "Epoch: 010, Loss: 0.35133\n",
      "Epoch: 011, Loss: 0.34609\n",
      "Epoch: 012, Loss: 0.34169\n",
      "Epoch: 013, Loss: 0.33759\n",
      "Epoch: 014, Loss: 0.33326\n",
      "Epoch: 015, Loss: 0.32592\n",
      "Epoch: 016, Loss: 0.32326\n",
      "Epoch: 017, Loss: 0.31861\n",
      "Epoch: 018, Loss: 0.31623\n",
      "Epoch: 019, Loss: 0.31525\n",
      "Epoch: 020, Loss: 0.31122\n",
      "Best training R2 for target 1: 0.7033298631127642\n",
      "Best test R2 for target 1: 0.7007529061747805\n",
      "Target index:  2\n",
      "Initial training R2:  -0.017158692973941334\n",
      "Initial test R2:  -0.01787220238902933\n",
      "Epoch: 001, Loss: 0.58541\n",
      "Epoch: 002, Loss: 0.46367\n",
      "Epoch: 003, Loss: 0.43245\n",
      "Epoch: 004, Loss: 0.41423\n",
      "Epoch: 005, Loss: 0.40000\n",
      "Epoch: 006, Loss: 0.39028\n",
      "Epoch: 007, Loss: 0.37836\n",
      "Epoch: 008, Loss: 0.36869\n",
      "Epoch: 009, Loss: 0.35685\n",
      "Epoch: 010, Loss: 0.34449\n",
      "Epoch: 011, Loss: 0.33835\n",
      "Epoch: 012, Loss: 0.33475\n",
      "Epoch: 013, Loss: 0.33009\n",
      "Epoch: 014, Loss: 0.33005\n",
      "Epoch: 015, Loss: 0.32430\n",
      "Epoch: 016, Loss: 0.32364\n",
      "Epoch: 017, Loss: 0.31936\n",
      "Epoch: 018, Loss: 0.31726\n",
      "Epoch: 019, Loss: 0.31689\n",
      "Epoch: 020, Loss: 0.31391\n",
      "Best training R2 for target 2: 0.7010053077575664\n",
      "Best test R2 for target 2: 0.7051230319763993\n",
      "Target index:  3\n",
      "Initial training R2:  -0.017280060359220337\n",
      "Initial test R2:  -0.01613028571963556\n",
      "Epoch: 001, Loss: 0.61169\n",
      "Epoch: 002, Loss: 0.49573\n",
      "Epoch: 003, Loss: 0.47538\n",
      "Epoch: 004, Loss: 0.46577\n",
      "Epoch: 005, Loss: 0.45996\n",
      "Epoch: 006, Loss: 0.45543\n",
      "Epoch: 007, Loss: 0.45275\n",
      "Epoch: 008, Loss: 0.45026\n",
      "Epoch: 009, Loss: 0.44536\n",
      "Epoch: 010, Loss: 0.44352\n",
      "Epoch: 011, Loss: 0.43955\n",
      "Epoch: 012, Loss: 0.43897\n",
      "Epoch: 013, Loss: 0.43680\n",
      "Epoch: 014, Loss: 0.43439\n",
      "Epoch: 015, Loss: 0.43222\n",
      "Epoch: 016, Loss: 0.43057\n",
      "Epoch: 017, Loss: 0.42784\n",
      "Epoch: 018, Loss: 0.42613\n",
      "Epoch: 019, Loss: 0.42451\n",
      "Epoch: 020, Loss: 0.42348\n",
      "Best training R2 for target 3: 0.5765110817904114\n",
      "Best test R2 for target 3: 0.5754772103382034\n",
      "Target index:  4\n",
      "Initial training R2:  -0.016626750506755876\n",
      "Initial test R2:  -0.016156849228845478\n",
      "Epoch: 001, Loss: 0.46523\n",
      "Epoch: 002, Loss: 0.32744\n",
      "Epoch: 003, Loss: 0.31264\n",
      "Epoch: 004, Loss: 0.30525\n",
      "Epoch: 005, Loss: 0.29862\n",
      "Epoch: 006, Loss: 0.29497\n",
      "Epoch: 007, Loss: 0.28676\n",
      "Epoch: 008, Loss: 0.27884\n",
      "Epoch: 009, Loss: 0.27257\n",
      "Epoch: 010, Loss: 0.26339\n",
      "Epoch: 011, Loss: 0.25305\n",
      "Epoch: 012, Loss: 0.24510\n",
      "Epoch: 013, Loss: 0.23823\n",
      "Epoch: 014, Loss: 0.23340\n",
      "Epoch: 015, Loss: 0.22897\n",
      "Epoch: 016, Loss: 0.22712\n",
      "Epoch: 017, Loss: 0.22235\n",
      "Epoch: 018, Loss: 0.22087\n",
      "Epoch: 019, Loss: 0.21953\n",
      "Epoch: 020, Loss: 0.21672\n",
      "Best training R2 for target 4: 0.7934011110496367\n",
      "Best test R2 for target 4: 0.7904534953760501\n",
      "Target index:  5\n",
      "Initial training R2:  -0.015699766693490477\n",
      "Initial test R2:  -0.020215554713385035\n",
      "Epoch: 001, Loss: 0.53577\n",
      "Epoch: 002, Loss: 0.30047\n",
      "Epoch: 003, Loss: 0.24951\n",
      "Epoch: 004, Loss: 0.22262\n",
      "Epoch: 005, Loss: 0.20876\n",
      "Epoch: 006, Loss: 0.19892\n",
      "Epoch: 007, Loss: 0.19191\n",
      "Epoch: 008, Loss: 0.18725\n",
      "Epoch: 009, Loss: 0.18285\n",
      "Epoch: 010, Loss: 0.17969\n",
      "Epoch: 011, Loss: 0.17705\n",
      "Epoch: 012, Loss: 0.17526\n",
      "Epoch: 013, Loss: 0.17210\n",
      "Epoch: 014, Loss: 0.17050\n",
      "Epoch: 015, Loss: 0.16862\n",
      "Epoch: 016, Loss: 0.16711\n",
      "Epoch: 017, Loss: 0.16533\n",
      "Epoch: 018, Loss: 0.16390\n",
      "Epoch: 019, Loss: 0.16287\n",
      "Epoch: 020, Loss: 0.16133\n",
      "Best training R2 for target 5: 0.8525178905415829\n",
      "Best test R2 for target 5: 0.8503384060262259\n",
      "Target index:  6\n",
      "Initial training R2:  -0.016569716452883656\n",
      "Initial test R2:  -0.019000348652367938\n",
      "Epoch: 001, Loss: 0.34956\n",
      "Epoch: 002, Loss: 0.13672\n",
      "Epoch: 003, Loss: 0.09012\n",
      "Epoch: 004, Loss: 0.08017\n",
      "Epoch: 005, Loss: 0.07513\n",
      "Epoch: 006, Loss: 0.07152\n",
      "Epoch: 007, Loss: 0.06937\n",
      "Epoch: 008, Loss: 0.06812\n",
      "Epoch: 009, Loss: 0.06600\n",
      "Epoch: 010, Loss: 0.06503\n",
      "Epoch: 011, Loss: 0.06422\n",
      "Epoch: 012, Loss: 0.06269\n",
      "Epoch: 013, Loss: 0.06192\n",
      "Epoch: 014, Loss: 0.06143\n",
      "Epoch: 015, Loss: 0.06127\n",
      "Epoch: 016, Loss: 0.06090\n",
      "Epoch: 017, Loss: 0.06016\n",
      "Epoch: 018, Loss: 0.05929\n",
      "Epoch: 019, Loss: 0.05912\n",
      "Epoch: 020, Loss: 0.05889\n",
      "Best training R2 for target 6: 0.9551711734279436\n",
      "Best test R2 for target 6: 0.9529787811572525\n",
      "Target index:  7\n",
      "Initial training R2:  -0.017588421650010122\n",
      "Initial test R2:  -0.01987757820897951\n",
      "Epoch: 001, Loss: 0.37236\n",
      "Epoch: 002, Loss: 0.16974\n",
      "Epoch: 003, Loss: 0.13013\n",
      "Epoch: 004, Loss: 0.11872\n",
      "Epoch: 005, Loss: 0.11221\n",
      "Epoch: 006, Loss: 0.10797\n",
      "Epoch: 007, Loss: 0.10558\n",
      "Epoch: 008, Loss: 0.10324\n",
      "Epoch: 009, Loss: 0.10113\n",
      "Epoch: 010, Loss: 0.09997\n",
      "Epoch: 011, Loss: 0.09964\n",
      "Epoch: 012, Loss: 0.09837\n",
      "Epoch: 013, Loss: 0.09782\n",
      "Epoch: 014, Loss: 0.09684\n",
      "Epoch: 015, Loss: 0.09620\n",
      "Epoch: 016, Loss: 0.09596\n",
      "Epoch: 017, Loss: 0.09569\n",
      "Epoch: 018, Loss: 0.09454\n",
      "Epoch: 019, Loss: 0.09414\n",
      "Epoch: 020, Loss: 0.09392\n",
      "Best training R2 for target 7: 0.920172195538476\n",
      "Best test R2 for target 7: 0.9168450292658281\n",
      "Target index:  8\n",
      "Initial training R2:  -0.017800306739373505\n",
      "Initial test R2:  -0.016984664227168923\n",
      "Epoch: 001, Loss: 0.47503\n",
      "Epoch: 002, Loss: 0.32488\n",
      "Epoch: 003, Loss: 0.29816\n",
      "Epoch: 004, Loss: 0.28315\n",
      "Epoch: 005, Loss: 0.27019\n",
      "Epoch: 006, Loss: 0.26035\n",
      "Epoch: 007, Loss: 0.25028\n",
      "Epoch: 008, Loss: 0.24012\n",
      "Epoch: 009, Loss: 0.23424\n",
      "Epoch: 010, Loss: 0.22802\n",
      "Epoch: 011, Loss: 0.22358\n",
      "Epoch: 012, Loss: 0.21924\n",
      "Epoch: 013, Loss: 0.21448\n",
      "Epoch: 014, Loss: 0.21248\n",
      "Epoch: 015, Loss: 0.21082\n",
      "Epoch: 016, Loss: 0.20706\n",
      "Epoch: 017, Loss: 0.20329\n",
      "Epoch: 018, Loss: 0.20152\n",
      "Epoch: 019, Loss: 0.19976\n",
      "Epoch: 020, Loss: 0.19826\n",
      "Best training R2 for target 8: 0.8048886073117744\n",
      "Best test R2 for target 8: 0.8043829131692173\n",
      "Target index:  9\n",
      "Initial training R2:  -0.016665912447528068\n",
      "Initial test R2:  -0.016718129018806652\n",
      "Epoch: 001, Loss: 0.21575\n",
      "Epoch: 002, Loss: 0.09124\n",
      "Epoch: 003, Loss: 0.08770\n",
      "Epoch: 004, Loss: 0.08564\n",
      "Epoch: 005, Loss: 0.08436\n",
      "Epoch: 006, Loss: 0.08366\n",
      "Epoch: 007, Loss: 0.08196\n",
      "Epoch: 008, Loss: 0.08146\n",
      "Epoch: 009, Loss: 0.08138\n",
      "Epoch: 010, Loss: 0.08082\n",
      "Epoch: 011, Loss: 0.07982\n",
      "Epoch: 012, Loss: 0.07948\n",
      "Epoch: 013, Loss: 0.07866\n",
      "Epoch: 014, Loss: 0.07837\n",
      "Epoch: 015, Loss: 0.07816\n",
      "Epoch: 016, Loss: 0.07814\n",
      "Epoch: 017, Loss: 0.07736\n",
      "Epoch: 018, Loss: 0.07650\n",
      "Epoch: 019, Loss: 0.07737\n",
      "Epoch: 020, Loss: 0.07598\n",
      "Best training R2 for target 9: 0.9361102795233491\n",
      "Best test R2 for target 9: 0.9370695113856015\n",
      "Target index:  10\n",
      "Initial training R2:  -0.017364368297613595\n",
      "Initial test R2:  -0.01713278458127302\n",
      "Epoch: 001, Loss: 0.45867\n",
      "Epoch: 002, Loss: 0.33914\n",
      "Epoch: 003, Loss: 0.32958\n",
      "Epoch: 004, Loss: 0.32283\n",
      "Epoch: 005, Loss: 0.31714\n",
      "Epoch: 006, Loss: 0.31086\n",
      "Epoch: 007, Loss: 0.30528\n",
      "Epoch: 008, Loss: 0.29636\n",
      "Epoch: 009, Loss: 0.28784\n",
      "Epoch: 010, Loss: 0.27824\n",
      "Epoch: 011, Loss: 0.26954\n",
      "Epoch: 012, Loss: 0.26180\n",
      "Epoch: 013, Loss: 0.25419\n",
      "Epoch: 014, Loss: 0.24885\n",
      "Epoch: 015, Loss: 0.24295\n",
      "Epoch: 016, Loss: 0.24087\n",
      "Epoch: 017, Loss: 0.23793\n",
      "Epoch: 018, Loss: 0.23556\n",
      "Epoch: 019, Loss: 0.23349\n",
      "Epoch: 020, Loss: 0.23147\n",
      "Best training R2 for target 10: 0.7793755651862267\n",
      "Best test R2 for target 10: 0.7908691529239255\n",
      "Target index:  11\n",
      "Initial training R2:  -0.01736436466257797\n",
      "Initial test R2:  -0.01713273523164014\n",
      "Epoch: 001, Loss: 0.45880\n",
      "Epoch: 002, Loss: 0.33916\n",
      "Epoch: 003, Loss: 0.32958\n",
      "Epoch: 004, Loss: 0.32288\n",
      "Epoch: 005, Loss: 0.31722\n",
      "Epoch: 006, Loss: 0.31099\n",
      "Epoch: 007, Loss: 0.30545\n",
      "Epoch: 008, Loss: 0.29672\n",
      "Epoch: 009, Loss: 0.28818\n",
      "Epoch: 010, Loss: 0.27862\n",
      "Epoch: 011, Loss: 0.27021\n",
      "Epoch: 012, Loss: 0.26226\n",
      "Epoch: 013, Loss: 0.25432\n",
      "Epoch: 014, Loss: 0.24882\n",
      "Epoch: 015, Loss: 0.24300\n",
      "Epoch: 016, Loss: 0.24071\n",
      "Epoch: 017, Loss: 0.23776\n",
      "Epoch: 018, Loss: 0.23521\n",
      "Epoch: 019, Loss: 0.23325\n",
      "Epoch: 020, Loss: 0.23119\n",
      "Best training R2 for target 11: 0.7805941048124122\n",
      "Best test R2 for target 11: 0.791619032127061\n",
      "Target index:  12\n",
      "Initial training R2:  -0.017364365211631653\n",
      "Initial test R2:  -0.017132735507439385\n",
      "Epoch: 001, Loss: 0.45876\n",
      "Epoch: 002, Loss: 0.33912\n",
      "Epoch: 003, Loss: 0.32959\n",
      "Epoch: 004, Loss: 0.32277\n",
      "Epoch: 005, Loss: 0.31708\n",
      "Epoch: 006, Loss: 0.31078\n",
      "Epoch: 007, Loss: 0.30503\n",
      "Epoch: 008, Loss: 0.29596\n",
      "Epoch: 009, Loss: 0.28713\n",
      "Epoch: 010, Loss: 0.27731\n",
      "Epoch: 011, Loss: 0.26877\n",
      "Epoch: 012, Loss: 0.26159\n",
      "Epoch: 013, Loss: 0.25402\n",
      "Epoch: 014, Loss: 0.24879\n",
      "Epoch: 015, Loss: 0.24293\n",
      "Epoch: 016, Loss: 0.24042\n",
      "Epoch: 017, Loss: 0.23767\n",
      "Epoch: 018, Loss: 0.23529\n",
      "Epoch: 019, Loss: 0.23320\n",
      "Epoch: 020, Loss: 0.23094\n",
      "Best training R2 for target 12: 0.7804756162043607\n",
      "Best test R2 for target 12: 0.7917490501930888\n",
      "Target index:  13\n",
      "Initial training R2:  -0.01736438236310565\n",
      "Initial test R2:  -0.017132841159794427\n",
      "Epoch: 001, Loss: 0.45887\n",
      "Epoch: 002, Loss: 0.33927\n",
      "Epoch: 003, Loss: 0.32977\n",
      "Epoch: 004, Loss: 0.32280\n",
      "Epoch: 005, Loss: 0.31719\n",
      "Epoch: 006, Loss: 0.31120\n",
      "Epoch: 007, Loss: 0.30555\n",
      "Epoch: 008, Loss: 0.29675\n",
      "Epoch: 009, Loss: 0.28807\n",
      "Epoch: 010, Loss: 0.27883\n",
      "Epoch: 011, Loss: 0.27063\n",
      "Epoch: 012, Loss: 0.26326\n",
      "Epoch: 013, Loss: 0.25576\n",
      "Epoch: 014, Loss: 0.25075\n",
      "Epoch: 015, Loss: 0.24497\n",
      "Epoch: 016, Loss: 0.24225\n",
      "Epoch: 017, Loss: 0.23918\n",
      "Epoch: 018, Loss: 0.23672\n",
      "Epoch: 019, Loss: 0.23453\n",
      "Epoch: 020, Loss: 0.23212\n",
      "Best training R2 for target 13: 0.7805468861408887\n",
      "Best test R2 for target 13: 0.7917087618526121\n",
      "Target index:  14\n",
      "Initial training R2:  -0.016812941087139623\n",
      "Initial test R2:  -0.01641816365757765\n",
      "Epoch: 001, Loss: 0.37350\n",
      "Epoch: 002, Loss: 0.26317\n",
      "Epoch: 003, Loss: 0.25462\n",
      "Epoch: 004, Loss: 0.25044\n",
      "Epoch: 005, Loss: 0.24564\n",
      "Epoch: 006, Loss: 0.24187\n",
      "Epoch: 007, Loss: 0.23616\n",
      "Epoch: 008, Loss: 0.23209\n",
      "Epoch: 009, Loss: 0.22682\n",
      "Epoch: 010, Loss: 0.22074\n",
      "Epoch: 011, Loss: 0.21211\n",
      "Epoch: 012, Loss: 0.20324\n",
      "Epoch: 013, Loss: 0.19537\n",
      "Epoch: 014, Loss: 0.19013\n",
      "Epoch: 015, Loss: 0.18668\n",
      "Epoch: 016, Loss: 0.18508\n",
      "Epoch: 017, Loss: 0.18218\n",
      "Epoch: 018, Loss: 0.18100\n",
      "Epoch: 019, Loss: 0.18156\n",
      "Epoch: 020, Loss: 0.17853\n",
      "Best training R2 for target 14: 0.8324319187720561\n",
      "Best test R2 for target 14: 0.8361393717094485\n"
     ]
    }
   ],
   "source": [
    "def train(data_in, target):\n",
    "      model.train()\n",
    "      optimizer.zero_grad()  # Clear gradients.\n",
    "      out = model(data_in.x, data_in.edge_index, data_in.edge_attr, data_in.batch, data_in.descriptors, data_in.fingerprints)\n",
    "      targets = data_in.y[:,target].reshape(-1,1)\n",
    "      \n",
    "      #Alt 1\n",
    "      loss = criterion(out, targets)   \n",
    "\n",
    "      loss.backward()  # Derive gradients.\n",
    "      optimizer.step()  # Update parameters based on gradients.\n",
    "      return loss\n",
    "\n",
    "def test(data, target):\n",
    "      all_test_r2 = []\n",
    "      counter = -1    \n",
    "      for data_in in data:\n",
    "            counter += 1\n",
    "            model.eval()\n",
    "            out = model(data_in.x, data_in.edge_index, data_in.edge_attr, data_in.batch, data_in.descriptors, data_in.fingerprints)\n",
    "            targets = data_in.y[:,target].reshape(-1,1)\n",
    "            \n",
    "            # Caculate R2\n",
    "            r2_score_var = r2_score(targets.detach().numpy(), out.detach().numpy())\n",
    "\n",
    "            all_test_r2 .append(r2_score_var)\n",
    "\n",
    "      average_test_r2 = np.sum(all_test_r2) / len(all_test_r2)\n",
    "\n",
    "      return average_test_r2\n",
    "\n",
    "num_targets = dataset[50].y.shape[1]\n",
    "for target_index in range(num_targets):\n",
    "      print(\"Target index: \", target_index)\n",
    "\n",
    "      model = GNN(hidden_channels=64) \n",
    "      optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=5e-4)\n",
    "      criterion = torch.nn.MSELoss()\n",
    "\n",
    "      # Vectors to append accuracy to:\n",
    "      Train_r2 = []\n",
    "      Test_r2 = []\n",
    "\n",
    "      # Calculate accuracy before training \n",
    "      Train_r2.append(test(train_loader, target_index))\n",
    "      Test_r2.append(test(test_loader, target_index))\n",
    "      print(\"Initial training R2: \", Train_r2[0])\n",
    "      print(\"Initial test R2: \", Test_r2[0])\n",
    "\n",
    "      print_r2_option = True\n",
    "      for epoch in range(1, 21):\n",
    "            average_loss = []\n",
    "            for data in train_loader:\n",
    "                  loss = train(data, target_index)\n",
    "                  average_loss.append(loss)\n",
    "            print(f'Epoch: {epoch:03d}, Loss: {(sum(average_loss)/len(average_loss)):.5f}')\n",
    "\n",
    "            if print_r2_option:\n",
    "                  temp_train_r2 = test(train_loader, target_index)\n",
    "                  Train_r2.append(temp_train_r2)\n",
    "\n",
    "                  temp_test_r2 = test(test_loader, target_index)\n",
    "                  Test_r2.append(temp_test_r2)\n",
    "\n",
    "                  # print(f'Average Train R2: {temp_train_r2}')\n",
    "                  # print(f'Average Test R2: {temp_test_r2:}')\n",
    "\n",
    "      print(f\"Best training R2 for target {target_index}: {np.max(Train_r2)}\")\n",
    "      print(f\"Best test R2 for target {target_index}: {np.max(Test_r2)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
