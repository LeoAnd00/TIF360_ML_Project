{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TIF360 Project"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main source: https://www.kaggle.com/code/rmonge/predicting-molecule-properties-based-on-its-smiles/notebook"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import rdkit\n",
    "from rdkit import Chem  # To extract information of the molecules\n",
    "from rdkit.Chem import Draw  # To draw the molecules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch_geometric\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch_geometric.utils as utils\n",
    "import networkx as nx\n",
    "from torch.nn import Linear\n",
    "from torch_geometric.nn import global_mean_pool, GraphConv, GATConv, GCNConv\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/smiles_and_targets.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mol_descriptor = np.load(\"../data/mol_descriptors.npy\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigate Neural Networks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Neural Networks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert data to graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "from rdkit.Chem import GetAdjacencyMatrix\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch.utils.data import DataLoader\n",
    "from rdkit.Chem import Descriptors\n",
    "from rdkit.ML.Descriptors import MoleculeDescriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(x, permitted_list):\n",
    "    if x not in permitted_list:\n",
    "        x = permitted_list[-1]  # If the atom is not in the list, get \"Unknown\"\n",
    "        \n",
    "    binary_encoding = [int(boolean) for boolean in list(map(lambda s: x==s, permitted_list))]\n",
    "    \n",
    "    return binary_encoding    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Atom featurisation\\\n",
    "Currently generates ca. 80 node features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_atom_features(atom, use_chirality = True, hydrogens_implicit = True):\n",
    "    # list of permitted atoms\n",
    "    permitted_atom_list = ['C','N','O','S','F','Si','P','Cl','Br','Mg','Na','Ca',\n",
    "                           'Fe','As','Al','I', 'B','V','K','Tl','Yb','Sb','Sn','Ag','Pd','Co',\n",
    "                           'Se','Ti','Zn', 'Li','Ge','Cu','Au','Ni','Cd','In','Mn','Zr','Cr','Pt',\n",
    "                           'Hg','Pb','Unknown']\n",
    "    \n",
    "    atom_type_enc = one_hot_encoding(str(atom.GetSymbol()), permitted_atom_list)\n",
    "    \n",
    "    n_heavy_neighbors = one_hot_encoding(int(atom.GetDegree()), [0,1,2,3,4,\"MoreThanFour\"])\n",
    "    \n",
    "    formal_charge_enc = one_hot_encoding(int(atom.GetFormalCharge()), [-3, -2, -1, 0, 1, 2, 3, 'Extreme'])\n",
    "    \n",
    "    hybridisation_type_enc = one_hot_encoding(str(atom.GetHybridization()), [\"S\", \"SP\", \"SP2\", \"SP3\", \"SP3D\", \"SP3D2\", \"OTHER\"])\n",
    "    \n",
    "    is_in_ring_enc = one_hot_encoding(int(atom.IsInRing()), [0, 1])\n",
    "    \n",
    "    is_aromatic_enc = one_hot_encoding(int(atom.GetIsAromatic()), [0, 1])\n",
    "    \n",
    "    atomic_mass_scaled = [float(atom.GetMass() - 10.812)/116.092] # (?) replace 10.812 with mean the and 116.092 with std\n",
    "    \n",
    "    vdw_radius_scaled = [float((Chem.GetPeriodicTable().GetRvdw(atom.GetAtomicNum()) - 1.5)/0.6)] # (?) replace 1.5 with mean the and 0.6 with std\n",
    "    \n",
    "    covalent_radius_scaled = [float((Chem.GetPeriodicTable().GetRcovalent(atom.GetAtomicNum()) - 0.64)/0.76)] # (?) replace 0.64 with mean the and 0.76 with std\n",
    "                              \n",
    "    atom_feature_vector = atom_type_enc + n_heavy_neighbors + formal_charge_enc + hybridisation_type_enc + is_in_ring_enc + is_aromatic_enc + atomic_mass_scaled + vdw_radius_scaled + covalent_radius_scaled\n",
    "    \n",
    "    if use_chirality:\n",
    "        chirality_type_enc = one_hot_encoding(str(atom.GetChiralTag()), [\"CHI_UNSPECIFIED\", \"CHI_TETRAHEDRAL_CW\", \"CHI_TETRAHEDRAL_CCW\", \"CHI_OTHER\"])\n",
    "        atom_feature_vector += chirality_type_enc\n",
    "        \n",
    "    if hydrogens_implicit:\n",
    "        n_hydrogens_enc = one_hot_encoding(int(atom.GetTotalNumHs()), [0, 1, 2, 3, 4, \"MoreThanFour\"])\n",
    "        atom_feature_vector += n_hydrogens_enc\n",
    "        \n",
    "    return np.array(atom_feature_vector) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bond Featurisation\\\n",
    "Currently generates ca. 10 edge features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bond_features(bond, use_stereochemistry=True):\n",
    "    permitted_bond_types = [Chem.rdchem.BondType.SINGLE, Chem.rdchem.BondType.DOUBLE, \n",
    "                            Chem.rdchem.BondType.TRIPLE, Chem.rdchem.BondType.AROMATIC]\n",
    "    \n",
    "    bond_type_enc = one_hot_encoding(str(bond.GetBondType()), permitted_bond_types)\n",
    "    \n",
    "    bond_is_conjugated_enc = [int(bond.GetIsConjugated())]\n",
    "    \n",
    "    bond_is_in_ring_enc = [int(bond.IsInRing())]\n",
    "    \n",
    "    bond_feature_vector = bond_type_enc + bond_is_conjugated_enc + bond_is_in_ring_enc\n",
    "    \n",
    "    if use_stereochemistry:\n",
    "        stereo_type_enc = one_hot_encoding(str(bond.GetStereo()), [\"STEREOZ\", \"STEREOE\", \"STEREOANY\", \"STEREONONE\"])\n",
    "        bond_feature_vector += stereo_type_enc\n",
    "        \n",
    "    return np.array(bond_feature_vector)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function to generate dataset of labeled Pytorch Geometric Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph_dataset_from_smiles(x_smiles, y):\n",
    "    ## Inputs:\n",
    "    # x_smiles = [smiles_1, smiles_2, ...], smiles representation of molecules\n",
    "    # y = [y_1, y_2, ...] list of numerical labels for each smiles string, here chemical properties\n",
    "    \n",
    "    # Outputs:\n",
    "    # dataset = [data_1, data_2, ...] list of torch_geometric.data.Data objects representing molecular graphs\n",
    "    \n",
    "    dataset = []\n",
    "    \n",
    "    for (smiles, y_val) in zip(x_smiles, y):\n",
    "        # convert smiles to molecular object\n",
    "        mol = Chem.MolFromSmiles(smiles)\n",
    "        \n",
    "        # get feature dimensions\n",
    "        n_nodes = mol.GetNumAtoms()\n",
    "        n_edges = 2*mol.GetNumBonds() # each bond is represented twice in the adjacency matrix\n",
    "        n_node_features = len(get_atom_features(mol.GetAtomWithIdx(0)))\n",
    "        if n_nodes > 1:\n",
    "            n_edge_features = len(get_bond_features(mol.GetBondBetweenAtoms(0,1)))\n",
    "        else:\n",
    "            n_edge_features = 0  # for single atom molecules -> no edges\n",
    "        \n",
    "        # construct node feature matrix X \n",
    "        X = np.zeros((n_nodes, n_node_features))\n",
    "        \n",
    "        for atom in mol.GetAtoms():\n",
    "            X[atom.GetIdx(), :] = get_atom_features(atom)\n",
    "        \n",
    "        X = torch.tensor(X, dtype=torch.float)\n",
    "        \n",
    "        # construct edge index array E, shape = (2, n_edges)\n",
    "        (rows, cols) = np.nonzero(GetAdjacencyMatrix(mol))\n",
    "        torch_rows = torch.tensor(rows.astype(np.int64)).to(torch.long)\n",
    "        torch_cols = torch.tensor(cols.astype(np.int64)).to(torch.long)\n",
    "        E = torch.stack([torch_rows, torch_cols], dim=0)\n",
    "        \n",
    "        # construct edge feature matrix EF\n",
    "        EF = np.zeros((n_edges, n_edge_features))       # Note: generates zero matrix if n_edges = n_edge_features = 0\n",
    "        for (k, (i,j)) in enumerate(zip(rows, cols)):\n",
    "            EF[k] = get_bond_features(mol.GetBondBetweenAtoms(int(i),int(j)))\n",
    "        EF = torch.tensor(EF, dtype=torch.float)\n",
    "        \n",
    "        # construct label/y tensor\n",
    "        y_tensor = torch.tensor(np.array([y_val]), dtype=torch.float)\n",
    "        \n",
    "        # construct torch_geometric.data.Data object and append to dataset\n",
    "        dataset.append(Data(x=X, edge_index=E, edge_attr=EF, y=y_tensor))\n",
    "        \n",
    "    return dataset\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the above functions to create a dataset of molecular graphs from the smiles and labels corresponding to chemical properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties_names = ['A', 'B', 'C', 'mu', 'alfa', 'homo', 'lumo', 'gap', 'RÂ²', 'zpve', 'U0', 'U', 'H', 'G', 'Cv']\n",
    "\n",
    "x_smiles = df.smiles.values\n",
    "y = df.loc[:, properties_names].values  # shape = (n_samples, n_properties)\n",
    "\n",
    "dataset = create_graph_dataset_from_smiles(x_smiles, y[0:len(x_smiles), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132820\n",
      "C#CC#C\n",
      "Data(x=[4, 81], edge_index=[2, 6], edge_attr=[6, 10], y=[1, 15])\n",
      "N#CC#N\n",
      "Data(x=[4, 81], edge_index=[2, 6], edge_attr=[6, 10], y=[1, 15])\n",
      "CC1=CNC=C1\n",
      "Data(x=[6, 81], edge_index=[2, 12], edge_attr=[12, 10], y=[1, 15])\n",
      "tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 1., 1., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 1., 1., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 1., 1., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 1., 1., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 1., 1., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 1., 1., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 1., 1., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 1., 1., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 1., 1., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 1., 1., 1., 0., 0., 0., 1.]])\n",
      "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000,\n",
      "         0.0000, 1.0000, 0.0000, 1.0000, 0.0000, 0.0103, 0.3333, 0.0526, 1.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000],\n",
      "        [1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0103, 0.3333, 0.0526, 1.0000,\n",
      "         0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0103, 0.3333, 0.0526, 1.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0275, 0.1667, 0.0526, 1.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0103, 0.3333, 0.0526, 1.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
      "        [1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 1.0000, 0.0000, 1.0000, 0.0103, 0.3333, 0.0526, 1.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "print(len(dataset))\n",
    "\n",
    "# Example entries\n",
    "print(df.smiles.values[0])\n",
    "print(dataset[0])\n",
    "print(df.smiles.values[2])\n",
    "print(dataset[2])\n",
    "print(df.smiles.values[50])\n",
    "print(dataset[50])\n",
    "\n",
    "print(dataset[50].edge_attr)\n",
    "print(dataset[50].x)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information of the graph dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of graphs (molecules): 132820\n",
      "=================================================================================\n",
      "Properties of graph 50, molecule smiles: CC1=CNC=C1\n",
      "Number of nodes: 6\n",
      "Number of edges: 12\n",
      "Number of node features: 81\n",
      "Number of edge features: 10\n",
      "Number of properties: 15\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of graphs (molecules): {len(dataset)}')\n",
    "graph = dataset[50]\n",
    "print('=================================================================================')\n",
    "print(f'Properties of graph {50}, molecule smiles: {df.smiles.values[50]}')\n",
    "print(f'Number of nodes: {dataset[50].x.shape[0]}')\n",
    "print(f'Number of edges: {dataset[50].edge_index.shape[1]}')\n",
    "print(f'Number of node features: {dataset[50].x.shape[1]}')\n",
    "print(f'Number of edge features: {dataset[50].edge_attr.shape[1]}')\n",
    "print(f'Number of properties: {dataset[50].y.shape[1]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filterout data with no edge features defined (Like ex: CH4) (These causes problems down the line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of none edge feature molecules:  0\n",
      "Before:  132820\n",
      "After:  132820\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "indexes_to_delete = []\n",
    "for item in range(0,len(dataset)):\n",
    "    if dataset[item].edge_attr.shape[1] == 0:\n",
    "        indexes_to_delete.append(item)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "indexes_to_delete.sort()\n",
    "\n",
    "print(\"Number of none edge feature molecules: \", len(indexes_to_delete))\n",
    "\n",
    "print(\"Before: \", len(dataset))\n",
    "\n",
    "for item in range(0,len(indexes_to_delete)):\n",
    "    print(\"Molecule to delete: \", df.smiles.values[indexes_to_delete[item]])\n",
    "    #del dataset[indexes_to_delete[item] - item] \n",
    "    dataset.pop((indexes_to_delete[item] - item)) # -item since all future data points will have its index reduced by 1 for each deleted previous data point\n",
    "print(\"After: \", len(dataset))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example train data target before scaling tensor([[ 2.9693e+00,  1.9777e+00,  1.8423e+00,  1.2093e+00,  8.4060e+01,\n",
      "         -1.8970e-01, -7.0400e-02,  1.1930e-01,  8.7339e+02,  1.5988e-01,\n",
      "         -3.4861e+02, -3.4860e+02, -3.4860e+02, -3.4864e+02,  2.9189e+01]])\n",
      "Example train data target after scaling: tensor([[-0.3350,  1.3300,  2.2841, -1.0264,  1.0990,  2.2975, -1.7445, -2.7966,\n",
      "         -1.1527,  0.3357,  1.6155,  1.6155,  1.6155,  1.6156, -0.6067]])\n",
      "Total data size:  132820\n",
      "Train data size:  106256\n",
      "Test data size:  26564\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "# split the dataset into test and validation:\n",
    "num_samples = len(dataset)\n",
    "\n",
    "# Want to divide data randomly\n",
    "random.seed(42)\n",
    "random_indexes = np.array(random.sample(range(num_samples), num_samples)) # random.sample ensures no duplicates\n",
    "\n",
    "train_data = [dataset[index] for index in random_indexes[int(.2 * num_samples ) :]] # 80%\n",
    "test_data = [dataset[index] for index in random_indexes[: int(.2 * num_samples )]] # 20%\n",
    "\n",
    "print(\"Example train data target before scaling\", train_data[0].y)\n",
    "train_data_targets = [data.y for data in train_data]\n",
    "train_data_targets = torch.concatenate(train_data_targets, axis=0)\n",
    "test_data_targets = [data.y for data in test_data]\n",
    "test_data_targets = torch.concatenate(test_data_targets, axis=0)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_data_targets = scaler.fit_transform(train_data_targets)\n",
    "test_data_targets = scaler.transform(test_data_targets)\n",
    "\n",
    "# print(\"Example train data target\", train_data_targets[0].reshape(1,-1).shape)\n",
    "train_data_targets = torch.tensor(train_data_targets, dtype=torch.float)\n",
    "test_data_targets = torch.tensor(test_data_targets, dtype=torch.float)\n",
    "\n",
    "train_data = [Data(x=data.x, edge_index=data.edge_index, edge_attr=data.edge_attr, y=train_data_targets[index].reshape(1,-1)) for index, data in enumerate(train_data)]\n",
    "test_data = [Data(x=data.x, edge_index=data.edge_index, edge_attr=data.edge_attr, y=test_data_targets[index].reshape(1,-1)) for index, data in enumerate(test_data)]\n",
    "print(\"Example train data target after scaling:\", train_data[0].y)\n",
    "\n",
    "print(\"Total data size: \", len(dataset))\n",
    "print(\"Train data size: \", len(train_data))\n",
    "print(\"Test data size: \", len(test_data))\n",
    "\n",
    "#train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "#test_loader = DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide descriptors into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_descriptor = [mol_descriptor[index,:] for index in random_indexes[int(.2 * num_samples ) :]] # 80%\n",
    "test_data_desriptors = [mol_descriptor[index,:] for index in random_indexes[: int(.2 * num_samples )]] # 20%"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize using MinMax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "minmax_scaler = MinMaxScaler()\n",
    "\n",
    "train_data_descriptor = minmax_scaler.fit_transform(train_data_descriptor)\n",
    "\n",
    "test_data_desriptors = minmax_scaler.transform(test_data_desriptors)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine descriptors temporarely with in train and test data for graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[9, 81], edge_index=[2, 20], edge_attr=[20, 10], y=[1, 15], descriptors=[209])\n"
     ]
    }
   ],
   "source": [
    "#Testing stuff\n",
    "\n",
    "temp = train_data[1000]\n",
    "temp.descriptors = train_data_descriptor[1000,:]\n",
    "\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jespe\\AppData\\Local\\Temp\\ipykernel_27512\\2381591684.py:2: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\tensor_new.cpp:248.)\n",
      "  train_data[index].descriptors = torch.FloatTensor([train_data_descriptor[index,:]])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[9, 81], edge_index=[2, 24], edge_attr=[24, 10], y=[1, 15], descriptors=[1, 209])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for index in range(0,len(train_data)):\n",
    "    train_data[index].descriptors = torch.FloatTensor([train_data_descriptor[index,:]])\n",
    "\n",
    "for index in range(0,len(test_data)):\n",
    "    test_data[index].descriptors = torch.FloatTensor([test_data_desriptors[index,:]])\n",
    "\n",
    "# Example\n",
    "print(train_data[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main GNN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model for all targets at once"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GNN function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_labels = dataset[50].y.shape[1]\n",
    "data_features = dataset[50].x.shape[1]\n",
    "descriptors_features = train_data[0].descriptors.shape[1]\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GATConv(data_features, hidden_channels)\n",
    "        self.conv2 = GATConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GATConv(hidden_channels, hidden_channels)\n",
    "        self.conv4 = GATConv(hidden_channels, hidden_channels)\n",
    "        self.conv5 = GATConv(hidden_channels, hidden_channels)\n",
    "        self.lin = Linear(hidden_channels+descriptors_features, hidden_channels)\n",
    "        self.lin2 = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin3 = Linear(hidden_channels, data_labels)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch, descriptors): \n",
    "        x = self.conv1(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv4(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv5(x, edge_index, edge_attr)\n",
    "\n",
    "        #Returns batch-wise graph-level-outputs by averaging node features across the node dimension, so that for a single graph G\n",
    "        #its output is computed by\n",
    "        x = global_mean_pool(x, batch) \n",
    "        x2 = descriptors\n",
    "        x = torch.cat((x,x2),1)\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = F.relu(x)\n",
    "        x = self.lin2(x)\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = F.relu(x)\n",
    "        x = self.lin3(x)\n",
    "        \n",
    " \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial training R2:  [-0.01705708 -0.02392712 -0.02885762 -0.01883505 -0.01613748 -0.01843543\n",
      " -0.02023741 -0.02122771 -0.02410174 -0.02204338 -0.02338289 -0.01762631\n",
      " -0.01646597 -0.01633073 -0.01749621]\n",
      "Initial test R2:  [-0.01533362 -0.02738384 -0.03015566 -0.02017704 -0.01920805 -0.01552731\n",
      " -0.02400283 -0.02496579 -0.0326653  -0.02405264 -0.02738946 -0.02029527\n",
      " -0.01842577 -0.01770193 -0.02494242]\n",
      "Epoch: 001, Loss: 0.5238\n",
      "Average Train R2: [0.22847661 0.46773771 0.54854758 0.30336871 0.64142304 0.25837144\n",
      " 0.69420346 0.66529158 0.67549807 0.81822918 0.64069408 0.64095012\n",
      " 0.64154641 0.64146957 0.67103744]\n",
      "Average Test R2: [0.24687975 0.47440902 0.55531682 0.30077576 0.64368154 0.25591969\n",
      " 0.69711906 0.66444665 0.68072809 0.82052071 0.65389337 0.65402857\n",
      " 0.65447316 0.65439378 0.67246593]\n",
      "Epoch: 002, Loss: 0.4227\n",
      "Average Train R2: [0.24156402 0.51757186 0.58219469 0.35308764 0.63696386 0.49958403\n",
      " 0.76235581 0.70998913 0.70944523 0.83048874 0.68278697 0.682838\n",
      " 0.68285565 0.68290663 0.665901  ]\n",
      "Average Test R2: [0.26070668 0.52387143 0.59103516 0.35351737 0.63821694 0.50649781\n",
      " 0.76461279 0.70829142 0.71736302 0.83278162 0.69696907 0.69701317\n",
      " 0.69704063 0.69704439 0.67016778]\n",
      "Epoch: 003, Loss: 0.3829\n",
      "Average Train R2: [0.25926782 0.53903339 0.60791443 0.36880324 0.65421113 0.62246874\n",
      " 0.8034052  0.74144806 0.73188295 0.86299525 0.7117933  0.71175777\n",
      " 0.71165451 0.71155202 0.70670147]\n",
      "Average Test R2: [0.26082428 0.54273519 0.61348264 0.36557351 0.65589349 0.62635952\n",
      " 0.79638221 0.7370495  0.73258917 0.86422101 0.72417875 0.72412622\n",
      " 0.72403967 0.72393337 0.69692646]\n",
      "Epoch: 004, Loss: 0.3552\n",
      "Average Train R2: [0.34504485 0.57932324 0.62732012 0.36863942 0.67774953 0.67761069\n",
      " 0.85852719 0.82110331 0.73554742 0.84657772 0.72517303 0.72508216\n",
      " 0.7249899  0.72488846 0.74132303]\n",
      "Average Test R2: [0.35910788 0.57927802 0.62841296 0.37108799 0.68398089 0.67838759\n",
      " 0.8585775  0.8198067  0.73384138 0.84874178 0.73417133 0.73406328\n",
      " 0.73398089 0.73387268 0.74339616]\n",
      "Epoch: 005, Loss: 0.3255\n",
      "Average Train R2: [0.42811783 0.55529821 0.58804924 0.37848066 0.72525953 0.70567813\n",
      " 0.88425271 0.85272879 0.72555774 0.8677294  0.71751293 0.71762728\n",
      " 0.71754683 0.71768153 0.75714217]\n",
      "Average Test R2: [0.43230109 0.55936045 0.59272217 0.37655209 0.72584762 0.70445957\n",
      " 0.88371166 0.85027752 0.72483045 0.86635038 0.72554775 0.72565182\n",
      " 0.72557369 0.72569822 0.75525435]\n",
      "Epoch: 006, Loss: 0.3089\n",
      "Average Train R2: [0.48767706 0.60409312 0.64570953 0.37231147 0.71897293 0.72495883\n",
      " 0.87967455 0.84319606 0.75781869 0.86286939 0.72275029 0.72276336\n",
      " 0.72275773 0.72274094 0.78872216]\n",
      "Average Test R2: [0.49531872 0.60625821 0.64751    0.36896445 0.7191168  0.72810519\n",
      " 0.8786924  0.83960328 0.75747439 0.8642839  0.72773448 0.72775063\n",
      " 0.72774355 0.72772932 0.78652995]\n",
      "Epoch: 007, Loss: 0.3014\n",
      "Average Train R2: [0.50017424 0.62264103 0.66040613 0.39487108 0.71800312 0.74367529\n",
      " 0.89469356 0.8627027  0.77669276 0.85244033 0.76309634 0.7631044\n",
      " 0.76310579 0.76309789 0.7936867 ]\n",
      "Average Test R2: [0.50948467 0.62282994 0.66451185 0.39055023 0.71668109 0.74210147\n",
      " 0.89265707 0.85929805 0.77679129 0.85241103 0.77228433 0.77229202\n",
      " 0.77229188 0.77228174 0.78957784]\n",
      "Epoch: 008, Loss: 0.2964\n",
      "Average Train R2: [0.50994264 0.65229974 0.67835375 0.40210378 0.74902753 0.75086139\n",
      " 0.90326979 0.8712933  0.7924947  0.86337183 0.75295775 0.75297544\n",
      " 0.75298107 0.75298055 0.78403274]\n",
      "Average Test R2: [0.51124362 0.6558273  0.68380501 0.39784029 0.74848216 0.75115364\n",
      " 0.90256011 0.86810736 0.7919235  0.86220069 0.75985972 0.75987619\n",
      " 0.75988011 0.75987712 0.78313046]\n",
      "Epoch: 009, Loss: 0.2925\n",
      "Average Train R2: [0.52747078 0.64044141 0.66847423 0.39767546 0.73436121 0.75383318\n",
      " 0.90406108 0.8730835  0.76631258 0.86017    0.77709732 0.77710152\n",
      " 0.7771025  0.7770901  0.78330175]\n",
      "Average Test R2: [0.53345726 0.64166034 0.6708673  0.39795234 0.73254977 0.75563788\n",
      " 0.90339036 0.87077544 0.76504045 0.86103946 0.78257337 0.78257743\n",
      " 0.78257783 0.78256458 0.77990052]\n",
      "Epoch: 010, Loss: 0.2885\n",
      "Average Train R2: [0.50586335 0.62539515 0.650784   0.39436481 0.73956019 0.7493965\n",
      " 0.90428226 0.87575772 0.77335976 0.86327223 0.71017932 0.71019762\n",
      " 0.71020997 0.71020592 0.74678085]\n",
      "Average Test R2: [0.47316932 0.62924355 0.65436229 0.39266121 0.73828891 0.75113205\n",
      " 0.90477151 0.87432961 0.77476217 0.8629078  0.71511255 0.71513152\n",
      " 0.71514433 0.71514163 0.74078488]\n",
      "Epoch: 011, Loss: 0.2858\n",
      "Average Train R2: [0.51931851 0.63958151 0.66926052 0.40902682 0.7661997  0.75322476\n",
      " 0.91250458 0.88054355 0.76690133 0.86378477 0.78542605 0.78543286\n",
      " 0.78543359 0.78542224 0.80191253]\n",
      "Average Test R2: [0.52516692 0.63587065 0.66984724 0.40270593 0.76625774 0.75495707\n",
      " 0.91199726 0.87815289 0.76507045 0.86396121 0.79270592 0.79271266\n",
      " 0.79271332 0.79270204 0.80034502]\n",
      "Epoch: 012, Loss: 0.2824\n",
      "Average Train R2: [0.55746493 0.65926944 0.68083818 0.40457094 0.76910786 0.76422395\n",
      " 0.91287847 0.87687049 0.78761997 0.8750143  0.78710851 0.78711376\n",
      " 0.78711293 0.78709872 0.79875989]\n",
      "Average Test R2: [0.56275626 0.65913847 0.68252366 0.4001949  0.76901715 0.76339538\n",
      " 0.91216429 0.87486245 0.78656927 0.87635193 0.79262127 0.79262634\n",
      " 0.79262535 0.79261077 0.79145478]\n",
      "Epoch: 013, Loss: 0.2810\n",
      "Average Train R2: [0.54250364 0.66312784 0.69023498 0.40636932 0.76448494 0.72655972\n",
      " 0.8988237  0.88034432 0.79716976 0.86734495 0.78811675 0.78812147\n",
      " 0.78812115 0.78810996 0.81199429]\n",
      "Average Test R2: [0.54659547 0.66244243 0.69208025 0.40176635 0.76475627 0.73038059\n",
      " 0.87982398 0.87321178 0.79543697 0.8691335  0.79534058 0.79534503\n",
      " 0.79534461 0.79533359 0.81156829]\n",
      "Epoch: 014, Loss: 0.2793\n",
      "Average Train R2: [0.56668924 0.67454498 0.70147462 0.40929249 0.77559655 0.74767206\n",
      " 0.89752362 0.87701617 0.80654458 0.8632292  0.79397469 0.79397926\n",
      " 0.79397851 0.79396627 0.81649071]\n",
      "Average Test R2: [0.56248846 0.67511361 0.70064589 0.39993622 0.77475857 0.74703788\n",
      " 0.89681046 0.8744068  0.80565547 0.8650342  0.8008655  0.80086999\n",
      " 0.80086922 0.80085701 0.81474239]\n",
      "Epoch: 015, Loss: 0.2784\n",
      "Average Train R2: [0.56669642 0.64208108 0.6560347  0.40291264 0.77762597 0.69234865\n",
      " 0.89161586 0.87738365 0.77618324 0.85476275 0.79236917 0.79237447\n",
      " 0.7923738  0.79236    0.80572111]\n",
      "Average Test R2: [0.5739756  0.6418242  0.65652039 0.39924257 0.77682273 0.69528035\n",
      " 0.8922875  0.87479477 0.77411149 0.85634425 0.79956258 0.79956781\n",
      " 0.79956718 0.79955366 0.80458069]\n",
      "Epoch: 016, Loss: 0.2764\n",
      "Average Train R2: [0.5561733  0.66806912 0.68549099 0.41462806 0.76939259 0.7542716\n",
      " 0.91343343 0.88208222 0.79398373 0.86074748 0.78541267 0.78541821\n",
      " 0.78541809 0.78540601 0.81173442]\n",
      "Average Test R2: [0.55924807 0.66916131 0.68783785 0.41278295 0.76615115 0.75444135\n",
      " 0.91304759 0.88006198 0.79223417 0.86095395 0.79025983 0.79026524\n",
      " 0.7902651  0.79025318 0.80888473]\n",
      "Epoch: 017, Loss: 0.2752\n",
      "Average Train R2: [0.57012618 0.6682852  0.69408737 0.41451514 0.78073392 0.75493776\n",
      " 0.91150887 0.88238879 0.80345876 0.86082266 0.79232857 0.79233422\n",
      " 0.79233427 0.79232234 0.79674475]\n",
      "Average Test R2: [0.57508314 0.66768416 0.69042473 0.4131293  0.70538401 0.75828483\n",
      " 0.91053411 0.87891191 0.80200519 0.86179604 0.79920983 0.79921541\n",
      " 0.79921545 0.79920364 0.79500442]\n",
      "Epoch: 018, Loss: 0.2744\n",
      "Average Train R2: [0.40274374 0.663884   0.70264234 0.42016447 0.77741531 0.76965284\n",
      " 0.90089502 0.87461876 0.80544379 0.86070383 0.79574348 0.79574979\n",
      " 0.79574991 0.79573725 0.77527992]\n",
      "Average Test R2: [0.40628484 0.66355489 0.70131041 0.42088958 0.77318543 0.761899\n",
      " 0.89977259 0.87291003 0.80228087 0.86268917 0.79688766 0.79689384\n",
      " 0.79689394 0.7968816  0.77194188]\n",
      "Epoch: 019, Loss: 0.2734\n",
      "Average Train R2: [0.5612177  0.66063633 0.68725456 0.41849904 0.78189117 0.76344688\n",
      " 0.90757615 0.88300259 0.80158875 0.86096625 0.79248105 0.79248617\n",
      " 0.79248606 0.79247435 0.80823799]\n",
      "Average Test R2: [0.56351409 0.66287508 0.68881952 0.41296484 0.78050387 0.76630977\n",
      " 0.90670559 0.87945277 0.8024251  0.86267448 0.79870554 0.79871058\n",
      " 0.79871047 0.79869891 0.80538299]\n",
      "Epoch: 020, Loss: 0.2724\n",
      "Average Train R2: [0.57848346 0.68673413 0.69606381 0.41638998 0.77620022 0.76557394\n",
      " 0.91292395 0.88330001 0.80848603 0.8653276  0.7983503  0.79835548\n",
      " 0.79835555 0.79834484 0.83212735]\n",
      "Average Test R2: [0.58193967 0.68588251 0.69827138 0.41323513 0.77532855 0.76469316\n",
      " 0.91176206 0.87978724 0.81030365 0.86523202 0.80520198 0.80520707\n",
      " 0.80520712 0.80519664 0.83264092]\n",
      "Epoch: 021, Loss: 0.2717\n",
      "Average Train R2: [0.58912054 0.68551407 0.70029215 0.41568281 0.78418613 0.76634583\n",
      " 0.90972316 0.87401336 0.8126811  0.8638301  0.79411907 0.79412425\n",
      " 0.79412412 0.7941128  0.8279216 ]\n",
      "Average Test R2: [0.58723606 0.68303142 0.69864994 0.41501446 0.78189434 0.76663731\n",
      " 0.90951643 0.87212354 0.81328    0.86524404 0.79843312 0.79843821\n",
      " 0.79843807 0.79842693 0.82618848]\n",
      "Epoch: 022, Loss: 0.2704\n",
      "Average Train R2: [0.57060004 0.66589679 0.68172146 0.42262514 0.78566051 0.76631536\n",
      " 0.91663271 0.88621631 0.7997617  0.85821193 0.75733056 0.75733659\n",
      " 0.75733673 0.75732372 0.81992682]\n",
      "Average Test R2: [0.57259898 0.66342132 0.67920814 0.4224963  0.78277589 0.7659983\n",
      " 0.91501247 0.88296242 0.79468735 0.85989826 0.7591195  0.75912547\n",
      " 0.75912562 0.7591129  0.81721883]\n",
      "Epoch: 023, Loss: 0.2687\n",
      "Average Train R2: [0.54903176 0.6799739  0.69521832 0.41496726 0.77573623 0.76234044\n",
      " 0.9168497  0.88590343 0.79803386 0.8597599  0.78299025 0.78299578\n",
      " 0.78299571 0.78298452 0.81401328]\n",
      "Average Test R2: [0.54959413 0.67830277 0.69556644 0.41215371 0.77389637 0.75799675\n",
      " 0.91523234 0.88245789 0.79854918 0.86043128 0.7867096  0.78671499\n",
      " 0.78671493 0.78670419 0.81202293]\n",
      "Epoch: 024, Loss: 0.2683\n",
      "Average Train R2: [0.5860087  0.69205164 0.70914686 0.41897669 0.78311037 0.76749379\n",
      " 0.9186414  0.88365945 0.81710664 0.87508444 0.79642215 0.79642681\n",
      " 0.7964268  0.79641712 0.81963592]\n",
      "Average Test R2: [0.58425303 0.68918799 0.70830314 0.40592008 0.77787782 0.76839724\n",
      " 0.91773232 0.88139508 0.81262479 0.87635738 0.80105691 0.80106156\n",
      " 0.80106156 0.80105186 0.81547443]\n",
      "Epoch: 025, Loss: 0.2674\n",
      "Average Train R2: [0.58771359 0.68948019 0.69246623 0.41126851 0.78314375 0.76384885\n",
      " 0.90053176 0.87309358 0.80602556 0.86390245 0.79933696 0.79934201\n",
      " 0.79934203 0.79933147 0.831925  ]\n",
      "Average Test R2: [0.59306556 0.68637662 0.69163099 0.40536035 0.77882257 0.76484415\n",
      " 0.9003252  0.87032189 0.80560503 0.86492959 0.80342306 0.80342813\n",
      " 0.80342814 0.80341761 0.82785017]\n",
      "Epoch: 026, Loss: 0.2664\n",
      "Average Train R2: [0.59012599 0.68824235 0.70531602 0.41796131 0.78392304 0.76431491\n",
      " 0.92165921 0.8874626  0.81663536 0.86980043 0.80467283 0.80467761\n",
      " 0.80467759 0.80466742 0.83695547]\n",
      "Average Test R2: [0.57727098 0.68733958 0.70376944 0.41500316 0.78367312 0.76701835\n",
      " 0.92078841 0.88458494 0.8150404  0.87196415 0.80985991 0.80986455\n",
      " 0.80986453 0.80985468 0.83479158]\n",
      "Epoch: 027, Loss: 0.2654\n",
      "Average Train R2: [0.59631308 0.68301455 0.69348371 0.42340206 0.78647701 0.76805205\n",
      " 0.91350988 0.87989278 0.81279353 0.86389967 0.79082599 0.79083068\n",
      " 0.79083066 0.79082038 0.828536  ]\n",
      "Average Test R2: [0.5979837  0.68318893 0.6942101  0.41914042 0.78321161 0.76752578\n",
      " 0.9123048  0.87677839 0.81081486 0.86496129 0.79137126 0.79137477\n",
      " 0.79137474 0.79136746 0.82624872]\n",
      "Epoch: 028, Loss: 0.2641\n",
      "Average Train R2: [0.58728543 0.68054446 0.70314543 0.4209212  0.78999817 0.76732849\n",
      " 0.92487707 0.89118747 0.81454823 0.86633123 0.80813973 0.80814472\n",
      " 0.80814472 0.80813446 0.83095344]\n",
      "Average Test R2: [0.58764568 0.68042906 0.70222437 0.42027156 0.78814605 0.76839781\n",
      " 0.92384847 0.88886852 0.8131142  0.86753651 0.81415538 0.81416038\n",
      " 0.81416038 0.81415009 0.82775636]\n",
      "Epoch: 029, Loss: 0.2635\n",
      "Average Train R2: [0.59447499 0.69653739 0.70231129 0.42365274 0.79456488 0.76209276\n",
      " 0.92238218 0.88942822 0.80678594 0.86397115 0.80176995 0.80177574\n",
      " 0.80177575 0.80176349 0.81386525]\n",
      "Average Test R2: [0.59620521 0.69362441 0.69941225 0.42322187 0.78934919 0.76261764\n",
      " 0.92023472 0.88644426 0.80561697 0.86540715 0.80477611 0.80478191\n",
      " 0.80478192 0.8047697  0.81062699]\n",
      "Epoch: 030, Loss: 0.2612\n",
      "Average Train R2: [0.53529822 0.68763493 0.70380221 0.41990698 0.75889555 0.77351119\n",
      " 0.92389892 0.89617718 0.79623929 0.85240489 0.77233385 0.77233985\n",
      " 0.77233985 0.77232758 0.79862971]\n",
      "Average Test R2: [0.53741861 0.68635409 0.70337326 0.42014436 0.75371382 0.7734379\n",
      " 0.9222773  0.89322483 0.7926027  0.85075535 0.77823151 0.77823749\n",
      " 0.77823748 0.7782253  0.78978265]\n",
      "Epoch: 031, Loss: 0.2603\n",
      "Average Train R2: [0.59529836 0.67553176 0.70447822 0.41823129 0.80148555 0.76842884\n",
      " 0.9199919  0.88978858 0.8240883  0.87543285 0.82024533 0.82024986\n",
      " 0.82024986 0.82024047 0.84118653]\n",
      "Average Test R2: [0.59266935 0.67816744 0.70625207 0.41733165 0.79652235 0.75689662\n",
      " 0.91849549 0.8856353  0.82341186 0.87540925 0.82501093 0.82501546\n",
      " 0.82501546 0.82500601 0.83742399]\n",
      "Epoch: 032, Loss: 0.2583\n",
      "Average Train R2: [0.59995513 0.65391021 0.68620027 0.42470683 0.78584344 0.76446284\n",
      " 0.9182771  0.88872022 0.80282749 0.87482571 0.8085999  0.80860453\n",
      " 0.80860453 0.80859501 0.83272108]\n",
      "Average Test R2: [0.602503   0.65432765 0.6772174  0.42446724 0.78196834 0.76638361\n",
      " 0.91612807 0.88623315 0.80146769 0.87522104 0.81285026 0.81285486\n",
      " 0.81285486 0.81284542 0.82960528]\n",
      "Epoch: 033, Loss: 0.2572\n",
      "Average Train R2: [0.61256297 0.70577957 0.72539436 0.41535689 0.80160022 0.76613841\n",
      " 0.91490958 0.88984476 0.83293117 0.87513813 0.80769033 0.80769512\n",
      " 0.80769512 0.80768506 0.84244432]\n",
      "Average Test R2: [0.60782401 0.70429913 0.72272832 0.41432545 0.79901332 0.75740032\n",
      " 0.91283961 0.88656491 0.83045485 0.87511009 0.81324244 0.81324722\n",
      " 0.81324722 0.81323713 0.83814094]\n",
      "Epoch: 034, Loss: 0.2553\n",
      "Average Train R2: [0.61836514 0.70339496 0.7192324  0.42023984 0.81131357 0.76356135\n",
      " 0.91842276 0.88209306 0.8178162  0.87513562 0.82198361 0.82198793\n",
      " 0.82198793 0.82197864 0.83874674]\n",
      "Average Test R2: [0.61833437 0.70131698 0.71282871 0.41609016 0.80792467 0.74895444\n",
      " 0.91861423 0.88042246 0.81357514 0.87603364 0.82591643 0.82592078\n",
      " 0.82592078 0.82591143 0.83310851]\n",
      "Epoch: 035, Loss: 0.2517\n",
      "Average Train R2: [0.59001075 0.70063046 0.72031606 0.42434607 0.8047891  0.76370502\n",
      " 0.92144716 0.8957272  0.8301501  0.87834225 0.82651472 0.82651902\n",
      " 0.82651902 0.82651    0.8469617 ]\n",
      "Average Test R2: [0.5902801  0.69408024 0.71611625 0.42113411 0.79859406 0.76466382\n",
      " 0.92112736 0.89240289 0.82315491 0.87441141 0.82911438 0.82911867\n",
      " 0.82911867 0.82910971 0.84438811]\n",
      "Epoch: 036, Loss: 0.2491\n",
      "Average Train R2: [0.62384646 0.72483943 0.73482684 0.41505969 0.81613626 0.74944088\n",
      " 0.92365409 0.89206849 0.83806189 0.88011953 0.83548029 0.8354845\n",
      " 0.8354845  0.83547573 0.84893107]\n",
      "Average Test R2: [0.62741596 0.7244898  0.73507179 0.41431931 0.81271723 0.75243197\n",
      " 0.92249222 0.89001948 0.83654987 0.88139215 0.83914115 0.8391453\n",
      " 0.8391453  0.83913661 0.8464939 ]\n",
      "Epoch: 037, Loss: 0.2446\n",
      "Average Train R2: [0.57903657 0.72017371 0.73174485 0.42208057 0.7995362  0.74373547\n",
      " 0.92059101 0.87764883 0.82321444 0.87328464 0.8143362  0.81434127\n",
      " 0.81434127 0.81433095 0.82560855]\n",
      "Average Test R2: [0.58035419 0.71965121 0.72743984 0.4205907  0.79495769 0.74713225\n",
      " 0.91865159 0.87565208 0.81822913 0.87113149 0.81884942 0.81885446\n",
      " 0.81885446 0.81884427 0.8230206 ]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 62\u001b[0m\n\u001b[0;32m     60\u001b[0m average_loss \u001b[39m=\u001b[39m []\n\u001b[0;32m     61\u001b[0m \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m train_loader:\n\u001b[1;32m---> 62\u001b[0m       loss \u001b[39m=\u001b[39m train(data)\n\u001b[0;32m     63\u001b[0m       average_loss\u001b[39m.\u001b[39mappend(loss)\n\u001b[0;32m     64\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m:\u001b[39;00m\u001b[39m03d\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Loss: \u001b[39m\u001b[39m{\u001b[39;00m(\u001b[39msum\u001b[39m(average_loss)\u001b[39m/\u001b[39m\u001b[39mlen\u001b[39m(average_loss))\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[39], line 8\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(data_in)\u001b[0m\n\u001b[0;32m      6\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m      7\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()  \u001b[39m# Clear gradients.\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m out \u001b[39m=\u001b[39m model(data_in\u001b[39m.\u001b[39;49mx, data_in\u001b[39m.\u001b[39;49medge_index, data_in\u001b[39m.\u001b[39;49medge_attr, data_in\u001b[39m.\u001b[39;49mbatch, data_in\u001b[39m.\u001b[39;49mdescriptors)  \u001b[39m# Perform a single forward pass.\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[39m#Alt 1\u001b[39;00m\n\u001b[0;32m     11\u001b[0m loss \u001b[39m=\u001b[39m criterion(out, data_in\u001b[39m.\u001b[39my) \n",
      "File \u001b[1;32mc:\\Users\\jespe\\anaconda3\\envs\\proj_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[38], line 19\u001b[0m, in \u001b[0;36mGNN.forward\u001b[1;34m(self, x, edge_index, edge_attr, batch, descriptors)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, edge_index, edge_attr, batch, descriptors): \n\u001b[1;32m---> 19\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x, edge_index, edge_attr)\n\u001b[0;32m     20\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(x)\n\u001b[0;32m     21\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(x, edge_index, edge_attr)\n",
      "File \u001b[1;32mc:\\Users\\jespe\\anaconda3\\envs\\proj_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\jespe\\anaconda3\\envs\\proj_env\\lib\\site-packages\\torch_geometric\\nn\\conv\\gat_conv.py:225\u001b[0m, in \u001b[0;36mGATConv.forward\u001b[1;34m(self, x, edge_index, edge_attr, size, return_attention_weights)\u001b[0m\n\u001b[0;32m    221\u001b[0m x \u001b[39m=\u001b[39m (x_src, x_dst)\n\u001b[0;32m    223\u001b[0m \u001b[39m# Next, we compute node-level attention coefficients, both for source\u001b[39;00m\n\u001b[0;32m    224\u001b[0m \u001b[39m# and target nodes (if present):\u001b[39;00m\n\u001b[1;32m--> 225\u001b[0m alpha_src \u001b[39m=\u001b[39m (x_src \u001b[39m*\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49matt_src)\u001b[39m.\u001b[39;49msum(dim\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m    226\u001b[0m alpha_dst \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m x_dst \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m (x_dst \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39matt_dst)\u001b[39m.\u001b[39msum(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m    227\u001b[0m alpha \u001b[39m=\u001b[39m (alpha_src, alpha_dst)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = GNN(hidden_channels=128) \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005, weight_decay=5e-4)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "def train(data_in):\n",
    "      model.train()\n",
    "      optimizer.zero_grad()  # Clear gradients.\n",
    "      out = model(data_in.x, data_in.edge_index, data_in.edge_attr, data_in.batch, data_in.descriptors)  # Perform a single forward pass.\n",
    "\n",
    "      #Alt 1\n",
    "      loss = criterion(out, data_in.y) \n",
    "\n",
    "      #Alt 2\n",
    "      #loss = 0\n",
    "      #for item in range(0,len(data_in.y[0,:])):\n",
    "      #      loss += criterion(out[:,item], data_in.y[:,item]) \n",
    "\n",
    "      loss.backward()  # Derive gradients.\n",
    "      optimizer.step()  # Update parameters based on gradients.\n",
    "      return loss\n",
    "\n",
    "def test(data):\n",
    "      all_test_r2 = []\n",
    "      counter = -1    \n",
    "      for data_in in data:\n",
    "            counter += 1\n",
    "            model.eval()\n",
    "            \n",
    "            out = model(data_in.x, data_in.edge_index, data_in.edge_attr, data_in.batch, data_in.descriptors)\n",
    "\n",
    "            # Caculate R2\n",
    "            r2_score_var = []\n",
    "            for item in range(0,data_in.y.shape[1]):\n",
    "                  if item == 0:\n",
    "                        r2_score_var = r2_score(data_in.y[:,item].detach().numpy(), out[:,item].detach().numpy())\n",
    "                  else:\n",
    "                        r2_score_var = np.vstack((r2_score_var,(r2_score(data_in.y[:,item].detach().numpy(), out[:,item].detach().numpy()))))\n",
    "\n",
    "            if counter == 0:\n",
    "                  all_test_r2 = r2_score_var\n",
    "            else:\n",
    "                  all_test_r2 = np.hstack((all_test_r2,r2_score_var))\n",
    "\n",
    "      average_test_r2 = np.sum(all_test_r2,axis=1) / all_test_r2.shape[1]\n",
    "\n",
    "      return average_test_r2\n",
    "\n",
    "# Vectors to append accuracy to:\n",
    "Train_r2 = []\n",
    "Test_r2 = []\n",
    "\n",
    "# Calculate accuracy before training \n",
    "Train_r2.append(test(train_loader))\n",
    "Test_r2.append(test(test_loader))\n",
    "print(\"Initial training R2: \", Train_r2[0])\n",
    "print(\"Initial test R2: \", Test_r2[0])\n",
    "\n",
    "print_r2_option = True\n",
    "for epoch in range(1, 31):\n",
    "      average_loss = []\n",
    "      for data in train_loader:\n",
    "            loss = train(data)\n",
    "            average_loss.append(loss)\n",
    "      print(f'Epoch: {epoch:03d}, Loss: {(sum(average_loss)/len(average_loss)):.4f}')\n",
    "\n",
    "      if print_r2_option:\n",
    "\n",
    "            temp_train_r2 = test(train_loader)\n",
    "            Train_r2.append(temp_train_r2)\n",
    "\n",
    "            temp_test_r2 = test(test_loader)\n",
    "            Test_r2.append(temp_test_r2)\n",
    "\n",
    "            print(f'Average Train R2: {temp_train_r2}')\n",
    "            print(f'Average Test R2: {temp_test_r2:}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model for just one target"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GNN function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GATConv: https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GATConv.html#torch_geometric.nn.conv.GATConv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GENConv: https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GENConv.html#torch_geometric.nn.conv.GENConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_labels = 1\n",
    "data_features = dataset[50].x.shape[1]\n",
    "descriptors_features = train_data[50].descriptors.shape[1]\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GATConv(data_features, hidden_channels)\n",
    "        self.conv2 = GATConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GATConv(hidden_channels, hidden_channels)\n",
    "        self.conv4 = GATConv(hidden_channels, hidden_channels)\n",
    "        self.conv5 = GATConv(hidden_channels, hidden_channels)\n",
    "        self.lin = Linear(hidden_channels+descriptors_features, hidden_channels)\n",
    "        self.lin2 = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin3 = Linear(hidden_channels, data_labels)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch, descriptors): \n",
    "        x = self.conv1(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv4(x, edge_index, edge_attr)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv5(x, edge_index, edge_attr)\n",
    "\n",
    "        #Returns batch-wise graph-level-outputs by averaging node features across the node dimension, so that for a single graph G\n",
    "        #its output is computed by\n",
    "        x = global_mean_pool(x, batch) \n",
    "        x2 = descriptors\n",
    "        x = torch.cat((x,x2),1)\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = F.relu(x)\n",
    "        x = self.lin2(x)\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = F.relu(x)\n",
    "        x = self.lin3(x)\n",
    "        \n",
    " \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target index:  0\n",
      "Initial training R2:  -0.018678295383913163\n",
      "Initial test R2:  -0.03308373704488183\n",
      "Epoch: 001, Loss: 0.76507\n",
      "Epoch: 002, Loss: 0.63190\n",
      "Epoch: 003, Loss: 0.56252\n",
      "Epoch: 004, Loss: 0.51782\n",
      "Epoch: 005, Loss: 0.48434\n",
      "Epoch: 006, Loss: 0.46161\n",
      "Epoch: 007, Loss: 0.44506\n",
      "Epoch: 008, Loss: 0.43061\n",
      "Epoch: 009, Loss: 0.41689\n",
      "Epoch: 010, Loss: 0.40696\n",
      "Epoch: 011, Loss: 0.39623\n",
      "Epoch: 012, Loss: 0.38796\n",
      "Epoch: 013, Loss: 0.37597\n",
      "Epoch: 014, Loss: 0.37060\n",
      "Epoch: 015, Loss: 0.36610\n",
      "Epoch: 016, Loss: 0.35919\n",
      "Epoch: 017, Loss: 0.35353\n",
      "Epoch: 018, Loss: 0.34897\n",
      "Epoch: 019, Loss: 0.34382\n",
      "Epoch: 020, Loss: 0.34009\n",
      "Best training R2 for target 0: 0.6581509149316066\n",
      "Best test R2 for target 0: 0.6554282682601079\n",
      "Target index:  1\n",
      "Initial training R2:  -0.01632305111746684\n",
      "Initial test R2:  -0.017224460553526586\n",
      "Epoch: 001, Loss: 0.63536\n",
      "Epoch: 002, Loss: 0.51249\n",
      "Epoch: 003, Loss: 0.45557\n",
      "Epoch: 004, Loss: 0.41914\n",
      "Epoch: 005, Loss: 0.40053\n",
      "Epoch: 006, Loss: 0.38774\n",
      "Epoch: 007, Loss: 0.37415\n",
      "Epoch: 008, Loss: 0.36468\n",
      "Epoch: 009, Loss: 0.36037\n",
      "Epoch: 010, Loss: 0.35133\n",
      "Epoch: 011, Loss: 0.34609\n",
      "Epoch: 012, Loss: 0.34169\n",
      "Epoch: 013, Loss: 0.33759\n",
      "Epoch: 014, Loss: 0.33326\n",
      "Epoch: 015, Loss: 0.32592\n",
      "Epoch: 016, Loss: 0.32326\n",
      "Epoch: 017, Loss: 0.31861\n",
      "Epoch: 018, Loss: 0.31623\n",
      "Epoch: 019, Loss: 0.31525\n",
      "Epoch: 020, Loss: 0.31122\n",
      "Best training R2 for target 1: 0.7033298631127642\n",
      "Best test R2 for target 1: 0.7007529061747805\n",
      "Target index:  2\n",
      "Initial training R2:  -0.017158692973941334\n",
      "Initial test R2:  -0.01787220238902933\n",
      "Epoch: 001, Loss: 0.58541\n",
      "Epoch: 002, Loss: 0.46367\n",
      "Epoch: 003, Loss: 0.43245\n",
      "Epoch: 004, Loss: 0.41423\n",
      "Epoch: 005, Loss: 0.40000\n",
      "Epoch: 006, Loss: 0.39028\n",
      "Epoch: 007, Loss: 0.37836\n",
      "Epoch: 008, Loss: 0.36869\n",
      "Epoch: 009, Loss: 0.35685\n",
      "Epoch: 010, Loss: 0.34449\n",
      "Epoch: 011, Loss: 0.33835\n",
      "Epoch: 012, Loss: 0.33475\n",
      "Epoch: 013, Loss: 0.33009\n",
      "Epoch: 014, Loss: 0.33005\n",
      "Epoch: 015, Loss: 0.32430\n",
      "Epoch: 016, Loss: 0.32364\n",
      "Epoch: 017, Loss: 0.31936\n",
      "Epoch: 018, Loss: 0.31726\n",
      "Epoch: 019, Loss: 0.31689\n",
      "Epoch: 020, Loss: 0.31391\n",
      "Best training R2 for target 2: 0.7010053077575664\n",
      "Best test R2 for target 2: 0.7051230319763993\n",
      "Target index:  3\n",
      "Initial training R2:  -0.017280060359220337\n",
      "Initial test R2:  -0.01613028571963556\n",
      "Epoch: 001, Loss: 0.61169\n",
      "Epoch: 002, Loss: 0.49573\n",
      "Epoch: 003, Loss: 0.47538\n",
      "Epoch: 004, Loss: 0.46577\n",
      "Epoch: 005, Loss: 0.45996\n",
      "Epoch: 006, Loss: 0.45543\n",
      "Epoch: 007, Loss: 0.45275\n",
      "Epoch: 008, Loss: 0.45026\n",
      "Epoch: 009, Loss: 0.44536\n",
      "Epoch: 010, Loss: 0.44352\n",
      "Epoch: 011, Loss: 0.43955\n",
      "Epoch: 012, Loss: 0.43897\n",
      "Epoch: 013, Loss: 0.43680\n",
      "Epoch: 014, Loss: 0.43439\n",
      "Epoch: 015, Loss: 0.43222\n",
      "Epoch: 016, Loss: 0.43057\n",
      "Epoch: 017, Loss: 0.42784\n",
      "Epoch: 018, Loss: 0.42613\n",
      "Epoch: 019, Loss: 0.42451\n",
      "Epoch: 020, Loss: 0.42348\n",
      "Best training R2 for target 3: 0.5765110817904114\n",
      "Best test R2 for target 3: 0.5754772103382034\n",
      "Target index:  4\n",
      "Initial training R2:  -0.016626750506755876\n",
      "Initial test R2:  -0.016156849228845478\n",
      "Epoch: 001, Loss: 0.46523\n",
      "Epoch: 002, Loss: 0.32744\n",
      "Epoch: 003, Loss: 0.31264\n",
      "Epoch: 004, Loss: 0.30525\n",
      "Epoch: 005, Loss: 0.29862\n",
      "Epoch: 006, Loss: 0.29497\n",
      "Epoch: 007, Loss: 0.28676\n",
      "Epoch: 008, Loss: 0.27884\n",
      "Epoch: 009, Loss: 0.27257\n",
      "Epoch: 010, Loss: 0.26339\n",
      "Epoch: 011, Loss: 0.25305\n",
      "Epoch: 012, Loss: 0.24510\n",
      "Epoch: 013, Loss: 0.23823\n",
      "Epoch: 014, Loss: 0.23340\n",
      "Epoch: 015, Loss: 0.22897\n",
      "Epoch: 016, Loss: 0.22712\n",
      "Epoch: 017, Loss: 0.22235\n",
      "Epoch: 018, Loss: 0.22087\n",
      "Epoch: 019, Loss: 0.21953\n",
      "Epoch: 020, Loss: 0.21672\n",
      "Best training R2 for target 4: 0.7934011110496367\n",
      "Best test R2 for target 4: 0.7904534953760501\n",
      "Target index:  5\n",
      "Initial training R2:  -0.015699766693490477\n",
      "Initial test R2:  -0.020215554713385035\n",
      "Epoch: 001, Loss: 0.53577\n",
      "Epoch: 002, Loss: 0.30047\n",
      "Epoch: 003, Loss: 0.24951\n",
      "Epoch: 004, Loss: 0.22262\n",
      "Epoch: 005, Loss: 0.20876\n",
      "Epoch: 006, Loss: 0.19892\n",
      "Epoch: 007, Loss: 0.19191\n",
      "Epoch: 008, Loss: 0.18725\n",
      "Epoch: 009, Loss: 0.18285\n",
      "Epoch: 010, Loss: 0.17969\n",
      "Epoch: 011, Loss: 0.17705\n",
      "Epoch: 012, Loss: 0.17526\n",
      "Epoch: 013, Loss: 0.17210\n",
      "Epoch: 014, Loss: 0.17050\n",
      "Epoch: 015, Loss: 0.16862\n",
      "Epoch: 016, Loss: 0.16711\n",
      "Epoch: 017, Loss: 0.16533\n",
      "Epoch: 018, Loss: 0.16390\n",
      "Epoch: 019, Loss: 0.16287\n",
      "Epoch: 020, Loss: 0.16133\n",
      "Best training R2 for target 5: 0.8525178905415829\n",
      "Best test R2 for target 5: 0.8503384060262259\n",
      "Target index:  6\n",
      "Initial training R2:  -0.016569716452883656\n",
      "Initial test R2:  -0.019000348652367938\n",
      "Epoch: 001, Loss: 0.34956\n",
      "Epoch: 002, Loss: 0.13672\n",
      "Epoch: 003, Loss: 0.09012\n",
      "Epoch: 004, Loss: 0.08017\n",
      "Epoch: 005, Loss: 0.07513\n",
      "Epoch: 006, Loss: 0.07152\n",
      "Epoch: 007, Loss: 0.06937\n",
      "Epoch: 008, Loss: 0.06812\n",
      "Epoch: 009, Loss: 0.06600\n",
      "Epoch: 010, Loss: 0.06503\n",
      "Epoch: 011, Loss: 0.06422\n",
      "Epoch: 012, Loss: 0.06269\n",
      "Epoch: 013, Loss: 0.06192\n",
      "Epoch: 014, Loss: 0.06143\n",
      "Epoch: 015, Loss: 0.06127\n",
      "Epoch: 016, Loss: 0.06090\n",
      "Epoch: 017, Loss: 0.06016\n",
      "Epoch: 018, Loss: 0.05929\n",
      "Epoch: 019, Loss: 0.05912\n",
      "Epoch: 020, Loss: 0.05889\n",
      "Best training R2 for target 6: 0.9551711734279436\n",
      "Best test R2 for target 6: 0.9529787811572525\n",
      "Target index:  7\n",
      "Initial training R2:  -0.017588421650010122\n",
      "Initial test R2:  -0.01987757820897951\n",
      "Epoch: 001, Loss: 0.37236\n",
      "Epoch: 002, Loss: 0.16974\n",
      "Epoch: 003, Loss: 0.13013\n",
      "Epoch: 004, Loss: 0.11872\n",
      "Epoch: 005, Loss: 0.11221\n",
      "Epoch: 006, Loss: 0.10797\n",
      "Epoch: 007, Loss: 0.10558\n",
      "Epoch: 008, Loss: 0.10324\n",
      "Epoch: 009, Loss: 0.10113\n",
      "Epoch: 010, Loss: 0.09997\n",
      "Epoch: 011, Loss: 0.09964\n",
      "Epoch: 012, Loss: 0.09837\n",
      "Epoch: 013, Loss: 0.09782\n",
      "Epoch: 014, Loss: 0.09684\n",
      "Epoch: 015, Loss: 0.09620\n",
      "Epoch: 016, Loss: 0.09596\n",
      "Epoch: 017, Loss: 0.09569\n",
      "Epoch: 018, Loss: 0.09454\n",
      "Epoch: 019, Loss: 0.09414\n",
      "Epoch: 020, Loss: 0.09392\n",
      "Best training R2 for target 7: 0.920172195538476\n",
      "Best test R2 for target 7: 0.9168450292658281\n",
      "Target index:  8\n",
      "Initial training R2:  -0.017800306739373505\n",
      "Initial test R2:  -0.016984664227168923\n",
      "Epoch: 001, Loss: 0.47503\n",
      "Epoch: 002, Loss: 0.32488\n",
      "Epoch: 003, Loss: 0.29816\n",
      "Epoch: 004, Loss: 0.28315\n",
      "Epoch: 005, Loss: 0.27019\n",
      "Epoch: 006, Loss: 0.26035\n",
      "Epoch: 007, Loss: 0.25028\n",
      "Epoch: 008, Loss: 0.24012\n",
      "Epoch: 009, Loss: 0.23424\n",
      "Epoch: 010, Loss: 0.22802\n",
      "Epoch: 011, Loss: 0.22358\n",
      "Epoch: 012, Loss: 0.21924\n",
      "Epoch: 013, Loss: 0.21448\n",
      "Epoch: 014, Loss: 0.21248\n",
      "Epoch: 015, Loss: 0.21082\n",
      "Epoch: 016, Loss: 0.20706\n",
      "Epoch: 017, Loss: 0.20329\n",
      "Epoch: 018, Loss: 0.20152\n",
      "Epoch: 019, Loss: 0.19976\n",
      "Epoch: 020, Loss: 0.19826\n",
      "Best training R2 for target 8: 0.8048886073117744\n",
      "Best test R2 for target 8: 0.8043829131692173\n",
      "Target index:  9\n",
      "Initial training R2:  -0.016665912447528068\n",
      "Initial test R2:  -0.016718129018806652\n",
      "Epoch: 001, Loss: 0.21575\n",
      "Epoch: 002, Loss: 0.09124\n",
      "Epoch: 003, Loss: 0.08770\n",
      "Epoch: 004, Loss: 0.08564\n",
      "Epoch: 005, Loss: 0.08436\n",
      "Epoch: 006, Loss: 0.08366\n",
      "Epoch: 007, Loss: 0.08196\n",
      "Epoch: 008, Loss: 0.08146\n",
      "Epoch: 009, Loss: 0.08138\n",
      "Epoch: 010, Loss: 0.08082\n",
      "Epoch: 011, Loss: 0.07982\n",
      "Epoch: 012, Loss: 0.07948\n",
      "Epoch: 013, Loss: 0.07866\n",
      "Epoch: 014, Loss: 0.07837\n",
      "Epoch: 015, Loss: 0.07816\n",
      "Epoch: 016, Loss: 0.07814\n",
      "Epoch: 017, Loss: 0.07736\n",
      "Epoch: 018, Loss: 0.07650\n",
      "Epoch: 019, Loss: 0.07737\n",
      "Epoch: 020, Loss: 0.07598\n",
      "Best training R2 for target 9: 0.9361102795233491\n",
      "Best test R2 for target 9: 0.9370695113856015\n",
      "Target index:  10\n",
      "Initial training R2:  -0.017364368297613595\n",
      "Initial test R2:  -0.01713278458127302\n",
      "Epoch: 001, Loss: 0.45867\n",
      "Epoch: 002, Loss: 0.33914\n",
      "Epoch: 003, Loss: 0.32958\n",
      "Epoch: 004, Loss: 0.32283\n",
      "Epoch: 005, Loss: 0.31714\n",
      "Epoch: 006, Loss: 0.31086\n",
      "Epoch: 007, Loss: 0.30528\n",
      "Epoch: 008, Loss: 0.29636\n",
      "Epoch: 009, Loss: 0.28784\n",
      "Epoch: 010, Loss: 0.27824\n",
      "Epoch: 011, Loss: 0.26954\n",
      "Epoch: 012, Loss: 0.26180\n",
      "Epoch: 013, Loss: 0.25419\n",
      "Epoch: 014, Loss: 0.24885\n",
      "Epoch: 015, Loss: 0.24295\n",
      "Epoch: 016, Loss: 0.24087\n",
      "Epoch: 017, Loss: 0.23793\n",
      "Epoch: 018, Loss: 0.23556\n",
      "Epoch: 019, Loss: 0.23349\n",
      "Epoch: 020, Loss: 0.23147\n",
      "Best training R2 for target 10: 0.7793755651862267\n",
      "Best test R2 for target 10: 0.7908691529239255\n",
      "Target index:  11\n",
      "Initial training R2:  -0.01736436466257797\n",
      "Initial test R2:  -0.01713273523164014\n",
      "Epoch: 001, Loss: 0.45880\n",
      "Epoch: 002, Loss: 0.33916\n",
      "Epoch: 003, Loss: 0.32958\n",
      "Epoch: 004, Loss: 0.32288\n",
      "Epoch: 005, Loss: 0.31722\n",
      "Epoch: 006, Loss: 0.31099\n",
      "Epoch: 007, Loss: 0.30545\n",
      "Epoch: 008, Loss: 0.29672\n",
      "Epoch: 009, Loss: 0.28818\n",
      "Epoch: 010, Loss: 0.27862\n",
      "Epoch: 011, Loss: 0.27021\n",
      "Epoch: 012, Loss: 0.26226\n",
      "Epoch: 013, Loss: 0.25432\n",
      "Epoch: 014, Loss: 0.24882\n",
      "Epoch: 015, Loss: 0.24300\n",
      "Epoch: 016, Loss: 0.24071\n",
      "Epoch: 017, Loss: 0.23776\n",
      "Epoch: 018, Loss: 0.23521\n",
      "Epoch: 019, Loss: 0.23325\n",
      "Epoch: 020, Loss: 0.23119\n",
      "Best training R2 for target 11: 0.7805941048124122\n",
      "Best test R2 for target 11: 0.791619032127061\n",
      "Target index:  12\n",
      "Initial training R2:  -0.017364365211631653\n",
      "Initial test R2:  -0.017132735507439385\n",
      "Epoch: 001, Loss: 0.45876\n",
      "Epoch: 002, Loss: 0.33912\n",
      "Epoch: 003, Loss: 0.32959\n",
      "Epoch: 004, Loss: 0.32277\n",
      "Epoch: 005, Loss: 0.31708\n",
      "Epoch: 006, Loss: 0.31078\n",
      "Epoch: 007, Loss: 0.30503\n",
      "Epoch: 008, Loss: 0.29596\n",
      "Epoch: 009, Loss: 0.28713\n",
      "Epoch: 010, Loss: 0.27731\n",
      "Epoch: 011, Loss: 0.26877\n",
      "Epoch: 012, Loss: 0.26159\n",
      "Epoch: 013, Loss: 0.25402\n",
      "Epoch: 014, Loss: 0.24879\n",
      "Epoch: 015, Loss: 0.24293\n",
      "Epoch: 016, Loss: 0.24042\n",
      "Epoch: 017, Loss: 0.23767\n",
      "Epoch: 018, Loss: 0.23529\n",
      "Epoch: 019, Loss: 0.23320\n",
      "Epoch: 020, Loss: 0.23094\n",
      "Best training R2 for target 12: 0.7804756162043607\n",
      "Best test R2 for target 12: 0.7917490501930888\n",
      "Target index:  13\n",
      "Initial training R2:  -0.01736438236310565\n",
      "Initial test R2:  -0.017132841159794427\n",
      "Epoch: 001, Loss: 0.45887\n",
      "Epoch: 002, Loss: 0.33927\n",
      "Epoch: 003, Loss: 0.32977\n",
      "Epoch: 004, Loss: 0.32280\n",
      "Epoch: 005, Loss: 0.31719\n",
      "Epoch: 006, Loss: 0.31120\n",
      "Epoch: 007, Loss: 0.30555\n",
      "Epoch: 008, Loss: 0.29675\n",
      "Epoch: 009, Loss: 0.28807\n",
      "Epoch: 010, Loss: 0.27883\n",
      "Epoch: 011, Loss: 0.27063\n",
      "Epoch: 012, Loss: 0.26326\n",
      "Epoch: 013, Loss: 0.25576\n",
      "Epoch: 014, Loss: 0.25075\n",
      "Epoch: 015, Loss: 0.24497\n",
      "Epoch: 016, Loss: 0.24225\n",
      "Epoch: 017, Loss: 0.23918\n",
      "Epoch: 018, Loss: 0.23672\n",
      "Epoch: 019, Loss: 0.23453\n",
      "Epoch: 020, Loss: 0.23212\n",
      "Best training R2 for target 13: 0.7805468861408887\n",
      "Best test R2 for target 13: 0.7917087618526121\n",
      "Target index:  14\n",
      "Initial training R2:  -0.016812941087139623\n",
      "Initial test R2:  -0.01641816365757765\n",
      "Epoch: 001, Loss: 0.37350\n",
      "Epoch: 002, Loss: 0.26317\n",
      "Epoch: 003, Loss: 0.25462\n",
      "Epoch: 004, Loss: 0.25044\n",
      "Epoch: 005, Loss: 0.24564\n",
      "Epoch: 006, Loss: 0.24187\n",
      "Epoch: 007, Loss: 0.23616\n",
      "Epoch: 008, Loss: 0.23209\n",
      "Epoch: 009, Loss: 0.22682\n",
      "Epoch: 010, Loss: 0.22074\n",
      "Epoch: 011, Loss: 0.21211\n",
      "Epoch: 012, Loss: 0.20324\n",
      "Epoch: 013, Loss: 0.19537\n",
      "Epoch: 014, Loss: 0.19013\n",
      "Epoch: 015, Loss: 0.18668\n",
      "Epoch: 016, Loss: 0.18508\n",
      "Epoch: 017, Loss: 0.18218\n",
      "Epoch: 018, Loss: 0.18100\n",
      "Epoch: 019, Loss: 0.18156\n",
      "Epoch: 020, Loss: 0.17853\n",
      "Best training R2 for target 14: 0.8324319187720561\n",
      "Best test R2 for target 14: 0.8361393717094485\n"
     ]
    }
   ],
   "source": [
    "def train(data_in, target):\n",
    "      model.train()\n",
    "      optimizer.zero_grad()  # Clear gradients.\n",
    "      out = model(data_in.x, data_in.edge_index, data_in.edge_attr, data_in.batch, data_in.descriptors)\n",
    "      targets = data_in.y[:,target].reshape(-1,1)\n",
    "      \n",
    "      #Alt 1\n",
    "      loss = criterion(out, targets)   \n",
    "\n",
    "      loss.backward()  # Derive gradients.\n",
    "      optimizer.step()  # Update parameters based on gradients.\n",
    "      return loss\n",
    "\n",
    "def test(data, target):\n",
    "      all_test_r2 = []\n",
    "      counter = -1    \n",
    "      for data_in in data:\n",
    "            counter += 1\n",
    "            model.eval()\n",
    "            out = model(data_in.x, data_in.edge_index, data_in.edge_attr, data_in.batch, data_in.descriptors)\n",
    "            targets = data_in.y[:,target].reshape(-1,1)\n",
    "            \n",
    "            # Caculate R2\n",
    "            r2_score_var = r2_score(targets.detach().numpy(), out.detach().numpy())\n",
    "\n",
    "            all_test_r2 .append(r2_score_var)\n",
    "\n",
    "      average_test_r2 = np.sum(all_test_r2) / len(all_test_r2)\n",
    "\n",
    "      return average_test_r2\n",
    "\n",
    "num_targets = dataset[50].y.shape[1]\n",
    "for target_index in range(num_targets):\n",
    "      print(\"Target index: \", target_index)\n",
    "\n",
    "      model = GNN(hidden_channels=64) \n",
    "      optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=5e-4)\n",
    "      criterion = torch.nn.MSELoss()\n",
    "\n",
    "      # Vectors to append accuracy to:\n",
    "      Train_r2 = []\n",
    "      Test_r2 = []\n",
    "\n",
    "      # Calculate accuracy before training \n",
    "      Train_r2.append(test(train_loader, target_index))\n",
    "      Test_r2.append(test(test_loader, target_index))\n",
    "      print(\"Initial training R2: \", Train_r2[0])\n",
    "      print(\"Initial test R2: \", Test_r2[0])\n",
    "\n",
    "      print_r2_option = True\n",
    "      for epoch in range(1, 21):\n",
    "            average_loss = []\n",
    "            for data in train_loader:\n",
    "                  loss = train(data, target_index)\n",
    "                  average_loss.append(loss)\n",
    "            print(f'Epoch: {epoch:03d}, Loss: {(sum(average_loss)/len(average_loss)):.5f}')\n",
    "\n",
    "            if print_r2_option:\n",
    "                  temp_train_r2 = test(train_loader, target_index)\n",
    "                  Train_r2.append(temp_train_r2)\n",
    "\n",
    "                  temp_test_r2 = test(test_loader, target_index)\n",
    "                  Test_r2.append(temp_test_r2)\n",
    "\n",
    "                  # print(f'Average Train R2: {temp_train_r2}')\n",
    "                  # print(f'Average Test R2: {temp_test_r2:}')\n",
    "\n",
    "      print(f\"Best training R2 for target {target_index}: {np.max(Train_r2)}\")\n",
    "      print(f\"Best test R2 for target {target_index}: {np.max(Test_r2)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
