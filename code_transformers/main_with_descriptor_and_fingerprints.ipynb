{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import atomInSmiles\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from code_graphs.utility_functions import get_num_parameters , get_data_split_indices, scale_targets\n",
    "\n",
    "\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Input is list of strings on the form ['a','b','c']\n",
    "# where the string are tokens\n",
    "# Dictionary is the dictionary containign all possible tokens\n",
    "# and an index for them\n",
    "# MaxTokenLength is the max amount of tokens any input creates\n",
    "\n",
    "# THIS VERSION RETURNS TORCH TENSOR\n",
    "\n",
    "#Version taht returns tensor\n",
    "def create_encoded_tensor(input, dictionary,maxTokenLength):\n",
    "    # Create a matrix of zeros\n",
    "    #encoded_vector = np.zeros((1,maxTokenLength))\n",
    "    encoded_tensor = np.zeros(maxTokenLength,dtype=np.int32)\n",
    "    \n",
    "    # Change value in right place to one\n",
    "    keyCount = 0\n",
    "    for key in input:\n",
    "        encoded_tensor[keyCount] = dictionary[key]\n",
    "        keyCount+=1\n",
    "    \n",
    "    # encoded_tensor = np.expand_dims(encoded_tensor,axis=1)\n",
    "    return encoded_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input is list of strings on the form ['a','b','c']\n",
    "# where the string are tokens\n",
    "# Dictionary is the dictionary containign all possible tokens\n",
    "# and an index for them\n",
    "# MaxTokenLength is the max amount of tokens any input creates\n",
    "\n",
    "# THIS VERSION RETURNS TORCH TENSOR\n",
    "\n",
    "#Version taht returns tensor\n",
    "def create_encoded_tensor(input, dictionary,maxTokenLength):\n",
    "    # Create a matrix of zeros\n",
    "    #encoded_vector = np.zeros((1,maxTokenLength))\n",
    "    encoded_tensor = np.zeros(maxTokenLength,dtype=np.int32)\n",
    "    \n",
    "    # Change value in right place to one\n",
    "    keyCount = 0\n",
    "    for key in input:\n",
    "        encoded_tensor[keyCount] = dictionary[key]\n",
    "        keyCount+=1\n",
    "    \n",
    "    # encoded_tensor = np.expand_dims(encoded_tensor,axis=1)\n",
    "    return encoded_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available: True\n",
      "device: cuda\n",
      "cuda version: 11.7\n",
      "gpu: NVIDIA GeForce GTX 970\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# check if cuda is available\n",
    "print('cuda available:', torch.cuda.is_available())\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('device:', \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    print('cuda version:', torch.version.cuda)\n",
    "    print('gpu:', torch.cuda.get_device_name(0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(132820, 21)\n",
      "(132820, 179)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "df = pd.read_csv(\"../data/smiles_and_targets.csv\")\n",
    "print(np.shape(df))\n",
    "mol_descriptor = np.load(\"../data/mol_descriptors.npy\")\n",
    "mol_fingerprint = np.load(\"../data/mol_morgan_fingerprints.npy\")\n",
    "\n",
    "\n",
    "print(mol_descriptor.shape)\n",
    "\n",
    "#print(mol_descriptor[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "properties_names = ['A', 'B', 'C', 'mu', 'alfa', 'homo', 'lumo', 'gap', 'RÂ²', 'zpve', 'U0', 'U', 'H', 'G', 'Cv']\n",
    "\n",
    "x_smiles = df.smiles.values\n",
    "targetTensor = df.loc[:, properties_names].values # shape = (n_samples, n_properties)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest word (max amount of tokens): 22\n"
     ]
    }
   ],
   "source": [
    "# tokenize all smiles\n",
    "#import atomInSmiles\n",
    "\n",
    "tokenList = []\n",
    "for smile in x_smiles:\n",
    "    tokenList.append(atomInSmiles.encode(smile).split())\n",
    "#tokenList is target\n",
    "\n",
    "# find longest word (max tokens from one smile+1)\n",
    "maxTokenLength = 0\n",
    "for token in tokenList:\n",
    "    if len(token)>maxTokenLength:\n",
    "        maxTokenLength = len(token)\n",
    "print('Longest word (max amount of tokens):', maxTokenLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Give each token a index in a dictionary\n",
    "tokenDict = {}\n",
    "count = 1\n",
    "\n",
    "dictList = []\n",
    "for itokens in tokenList:\n",
    "    for itoke in itokens:\n",
    "        #print(itoke)\n",
    "        if tokenDict.get(itoke) == None:\n",
    "            tokenDict[itoke] = count\n",
    "        \n",
    "            # current = [itoke, count]\n",
    "            # dictList.append(current)\n",
    "            count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodedTokens = []\n",
    "for token in tokenList:\n",
    "    encodedTokens.append(create_encoded_tensor(token,tokenDict,maxTokenLength))\n",
    "    \n",
    "encodedTokens = np.array(encodedTokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From attention is all you need\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "    def __init__(self, d_model, max_len,dropout):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) *\n",
    "                             -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x = x + torch.tensor(self.pe[:, :x.size(1)], \n",
    "        #                  requires_grad=False)\n",
    "        x = x + self.pe[:x.size(0), :].detach()\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TransformerLayer(torch.nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_channels,num_heads):\n",
    "        super().__init__()\n",
    "        self.Attention = torch.nn.MultiheadAttention(embedding_dim,num_heads=num_heads,dropout=0.15)\n",
    "        self.Norm1 = torch.nn.LayerNorm(embedding_dim)\n",
    "        self.Dense1 = torch.nn.Linear(embedding_dim,hidden_channels)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.Dense2 = torch.nn.Linear(hidden_channels,embedding_dim)\n",
    "        \n",
    "\n",
    "        self.Norm2 = torch.nn.LayerNorm(embedding_dim)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        addNormX = x\n",
    "        #print(x.shape)\n",
    "        x, _ = self.Attention(x,x,x)\n",
    "        #print('attention output',x.shape)\n",
    "        x = self.Norm1(x + addNormX)\n",
    "        #print('norm + input',x.shape)\n",
    "        addNormX = x\n",
    "        x = self.Dense1(x)\n",
    "        x = self.relu(x)\n",
    "        #print('first dense output',x.shape)\n",
    "        x = self.Dense2(x)\n",
    "        #print('second dense output',x.shape)\n",
    "        x = self.Norm2(x + addNormX)\n",
    "        #print(x.shape)\n",
    "\n",
    " \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerNetwork(torch.nn.Module):\n",
    "    def __init__(self,hidden_channels,output_dim, vocab_size, embedding_dim, num_heads,maxTokenLength, nDescriptors,nFingerprints):\n",
    "        super().__init__()\n",
    "        # Embedd and add pos encoding to input\n",
    "        self.EmbeddingLayer = torch.nn.Embedding(num_embeddings=vocab_size,embedding_dim = embedding_dim , max_norm=True)\n",
    "        self.PositionalEncoding = PositionalEncoding(embedding_dim, maxTokenLength, dropout = 0.15)\n",
    "\n",
    "        self.TransEnc1 = TransformerLayer(embedding_dim,hidden_channels, num_heads)\n",
    "        self.TransEnc2 = TransformerLayer(embedding_dim,hidden_channels, num_heads)\n",
    "        self.TransEnc3 = TransformerLayer(embedding_dim,hidden_channels, num_heads)\n",
    "        self.Pooling = torch.nn.AvgPool1d(kernel_size= 22)\n",
    "\n",
    "        self.DenseOut1 = torch.nn.Linear(embedding_dim+nDescriptors+nFingerprints,hidden_channels)\n",
    "        self.DenseOut2 = torch.nn.Linear(hidden_channels,output_dim)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "\n",
    "    def forward(self,x,descriptors,fingerprints):\n",
    "        x = self.EmbeddingLayer(x)\n",
    "        x = self.PositionalEncoding(x)\n",
    "        x = self.TransEnc1(x)\n",
    "        x = self.TransEnc2(x)\n",
    "        x = self.TransEnc3(x)\n",
    "        x = self.Pooling(x.permute((0,2,1))).permute((0,2,1))\n",
    "        x = torch.squeeze(x,axis=1)\n",
    "        x = torch.cat((x,descriptors,fingerprints),1)\n",
    "        #x = x[:,-1,:]\n",
    "\n",
    "\n",
    "        x = self.DenseOut1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.DenseOut2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "batch_size = 64\n",
    "## Create iterable dataset class:\n",
    "\n",
    "class datasetObject(Dataset):\n",
    "    def __init__(self,data,targets,descriptors,fingerprints):\n",
    "        self.data = data\n",
    "        self.descriptors = descriptors\n",
    "        self.targets = targets\n",
    "        self.fingerprints = fingerprints\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.data[index]\n",
    "        target = self.targets[index]\n",
    "        descriptor = self.descriptors[index]\n",
    "        fingerprint = self.fingerprints[index]\n",
    "        return (sample), (target), (descriptor), (fingerprint)\n",
    "\n",
    "trainSplit, validationSplit, testSplit = get_data_split_indices(len(encodedTokens),0.1,0.1)\n",
    "\n",
    "# Training\n",
    "encodedTrainData = torch.tensor(encodedTokens[trainSplit], dtype=torch.long, device=device)\n",
    "trainTargets = targetTensor[trainSplit]\n",
    "\n",
    "# Validation\n",
    "encodedValidationData = torch.tensor(encodedTokens[validationSplit], dtype=torch.long, device=device)\n",
    "validationTargets = targetTensor[validationSplit]\n",
    "\n",
    "# Test\n",
    "encodedTestData = torch.tensor(encodedTokens[testSplit], dtype=torch.long, device=device)\n",
    "testTargets = targetTensor[testSplit]\n",
    "\n",
    "# Descriptors\n",
    "trainDescriptors = mol_descriptor[trainSplit]\n",
    "valDescriptors = mol_descriptor[validationSplit]\n",
    "testDescriptors = mol_descriptor[testSplit]\n",
    "\n",
    "trainFingerprints = mol_fingerprint[trainSplit]\n",
    "valFingerprints = mol_fingerprint[validationSplit]\n",
    "testFingerprints = mol_fingerprint[testSplit]\n",
    "\n",
    "\n",
    "trainTargets, validationTargets, testTargets, scalerTargets = scale_targets(trainTargets,validationTargets,testTargets)\n",
    "\n",
    "\n",
    "# Normalize descriptors and fingerprints using minmax scaler\n",
    "minmax_scaler = MinMaxScaler()\n",
    "trainDescriptors = minmax_scaler.fit_transform(trainDescriptors)\n",
    "valDescriptors = minmax_scaler.transform(valDescriptors)\n",
    "testDescriptors = minmax_scaler.transform(testDescriptors)\n",
    "\n",
    "trainFingerprints = minmax_scaler.fit_transform(trainFingerprints)\n",
    "valFingerprints = minmax_scaler.transform(valFingerprints)\n",
    "testFingerprints = minmax_scaler.transform(testFingerprints)\n",
    "\n",
    "\n",
    "# Make into tensors\n",
    "#print(np.dtype(trainTargets[0,0]))\n",
    "trainTargets = torch.tensor(trainTargets,dtype=torch.float, device=device)\n",
    "validationTargets = torch.tensor(validationTargets,dtype=torch.float, device=device)\n",
    "testTargets = torch.tensor(testTargets,dtype=torch.float, device=device)\n",
    "\n",
    "# Decriptors to tensor\n",
    "trainDescriptors = torch.from_numpy(trainDescriptors).float().to(device)\n",
    "valDescriptors = torch.from_numpy(valDescriptors).float().to(device)\n",
    "testDescriptors = torch.from_numpy(testDescriptors).float().to(device)\n",
    "\n",
    "trainFingerprints = torch.from_numpy(trainFingerprints).float().to(device)\n",
    "valFingerprints = torch.from_numpy(valFingerprints).float().to(device)\n",
    "testFingerprints = torch.from_numpy(testFingerprints).float().to(device)\n",
    "\n",
    "trainingData = DataLoader(datasetObject(encodedTrainData,trainTargets,trainDescriptors,trainFingerprints),batch_size)\n",
    "testData = DataLoader(datasetObject(encodedTestData,testTargets,testDescriptors,testFingerprints),batch_size)\n",
    "validationData = DataLoader(datasetObject(encodedValidationData,validationTargets,valDescriptors,valFingerprints),batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Epoch: 0 of 1000 ####\n",
      " Training loss:\t0.1513408405963087\n",
      " Test loss:\t0.10848947671743539\n",
      " Validation loss:\t0.11124770457927997\n",
      "R2:  0.8867747824562054\n",
      "#### Epoch: 1 of 1000 ####\n",
      " Training loss:\t0.09752053772376863\n",
      " Test loss:\t0.09181831433222844\n",
      " Validation loss:\t0.09442358750563401\n",
      "R2:  0.9038356969659085\n",
      "#### Epoch: 2 of 1000 ####\n",
      " Training loss:\t0.08674289949108217\n",
      " Test loss:\t0.08715672676379864\n",
      " Validation loss:\t0.0899059589092548\n",
      "R2:  0.908544341472919\n",
      "#### Epoch: 3 of 1000 ####\n",
      " Training loss:\t0.08045784010361792\n",
      " Test loss:\t0.07989830237168533\n",
      " Validation loss:\t0.08272895446190467\n",
      "R2:  0.9162060320530802\n",
      "#### Epoch: 4 of 1000 ####\n",
      " Training loss:\t0.07632013604556273\n",
      " Test loss:\t0.07648684886785653\n",
      " Validation loss:\t0.07932618031134972\n",
      "R2:  0.9198087253742698\n",
      "#### Epoch: 5 of 1000 ####\n",
      " Training loss:\t0.07345564059648509\n",
      " Test loss:\t0.07421832359754123\n",
      " Validation loss:\t0.07708095587216891\n",
      "R2:  0.9221914419934865\n",
      "#### Epoch: 6 of 1000 ####\n",
      " Training loss:\t0.07135005172371219\n",
      " Test loss:\t0.07153414762937106\n",
      " Validation loss:\t0.07437341029827411\n",
      "R2:  0.9250526541398201\n",
      "#### Epoch: 7 of 1000 ####\n",
      " Training loss:\t0.06971771956781965\n",
      " Test loss:\t0.0699440470108619\n",
      " Validation loss:\t0.07274414484317486\n",
      "R2:  0.9267267580489773\n",
      "#### Epoch: 8 of 1000 ####\n",
      " Training loss:\t0.06838545549497196\n",
      " Test loss:\t0.06920111637849075\n",
      " Validation loss:\t0.07199915097309993\n",
      "R2:  0.9275093229595038\n",
      "#### Epoch: 9 of 1000 ####\n",
      " Training loss:\t0.0673031976322034\n",
      " Test loss:\t0.06834735778661874\n",
      " Validation loss:\t0.0711736403978788\n",
      "R2:  0.9284061037352671\n",
      "#### Epoch: 10 of 1000 ####\n",
      " Training loss:\t0.06636387264061089\n",
      " Test loss:\t0.06772330632576576\n",
      " Validation loss:\t0.07051361524141751\n",
      "R2:  0.929050963010958\n",
      "#### Epoch: 11 of 1000 ####\n",
      " Training loss:\t0.06550613302268729\n",
      " Test loss:\t0.06715491643318763\n",
      " Validation loss:\t0.06993750883982731\n",
      "R2:  0.9296223698057976\n",
      "#### Epoch: 12 of 1000 ####\n",
      " Training loss:\t0.0647518059204028\n",
      " Test loss:\t0.06647707407291119\n",
      " Validation loss:\t0.06926410473310031\n",
      "R2:  0.9303361235992165\n",
      "#### Epoch: 13 of 1000 ####\n",
      " Training loss:\t0.06406640518436513\n",
      " Test loss:\t0.06592822533387405\n",
      " Validation loss:\t0.06870744778559758\n",
      "R2:  0.9309147802037615\n",
      "#### Epoch: 14 of 1000 ####\n",
      " Training loss:\t0.0634489432650112\n",
      " Test loss:\t0.06544544605108407\n",
      " Validation loss:\t0.06820283027795646\n",
      "R2:  0.9314251031671055\n",
      "#### Epoch: 15 of 1000 ####\n",
      " Training loss:\t0.06286939865562444\n",
      " Test loss:\t0.06495625239152175\n",
      " Validation loss:\t0.0677282580962548\n",
      "R2:  0.9319315239062811\n",
      "#### Epoch: 16 of 1000 ####\n",
      " Training loss:\t0.06233497817138819\n",
      " Test loss:\t0.0644709926385146\n",
      " Validation loss:\t0.0672178864479065\n",
      "R2:  0.932455521362879\n",
      "#### Epoch: 17 of 1000 ####\n",
      " Training loss:\t0.061841488171599845\n",
      " Test loss:\t0.06408844544337346\n",
      " Validation loss:\t0.06680371669622567\n",
      "R2:  0.9328466820672795\n",
      "#### Epoch: 18 of 1000 ####\n",
      " Training loss:\t0.061374041920753214\n",
      " Test loss:\t0.06368267994660598\n",
      " Validation loss:\t0.06637527850958017\n",
      "R2:  0.9332897258498085\n",
      "#### Epoch: 19 of 1000 ####\n",
      " Training loss:\t0.06093470178979624\n",
      " Test loss:\t0.0633839827317458\n",
      " Validation loss:\t0.06606079064882718\n",
      "R2:  0.9336032697731854\n",
      "#### Epoch: 20 of 1000 ####\n",
      " Training loss:\t0.060526177798460076\n",
      " Test loss:\t0.06312979184664212\n",
      " Validation loss:\t0.06579153812848605\n",
      "R2:  0.9338776955718535\n",
      "#### Epoch: 21 of 1000 ####\n",
      " Training loss:\t0.060134106842169625\n",
      " Test loss:\t0.06283031518642719\n",
      " Validation loss:\t0.06546630767675546\n",
      "R2:  0.9341944636471453\n",
      "#### Epoch: 22 of 1000 ####\n",
      " Training loss:\t0.05976740901976878\n",
      " Test loss:\t0.06263657258107112\n",
      " Validation loss:\t0.06525691197468685\n",
      "R2:  0.9344029673010309\n",
      "#### Epoch: 23 of 1000 ####\n",
      " Training loss:\t0.059420792902901115\n",
      " Test loss:\t0.06235732023532574\n",
      " Validation loss:\t0.06497148825572087\n",
      "R2:  0.9347052819838987\n",
      "#### Epoch: 24 of 1000 ####\n",
      " Training loss:\t0.05909508987050111\n",
      " Test loss:\t0.062150221604567304\n",
      " Validation loss:\t0.06474852561950684\n",
      "R2:  0.9349348785563021\n",
      "#### Epoch: 25 of 1000 ####\n",
      " Training loss:\t0.0587866207262897\n",
      " Test loss:\t0.06191153709705059\n",
      " Validation loss:\t0.06449496287565964\n",
      "R2:  0.9351899969827691\n",
      "#### Epoch: 26 of 1000 ####\n",
      " Training loss:\t0.05849529360519124\n",
      " Test loss:\t0.061700871357550986\n",
      " Validation loss:\t0.06427945540501522\n",
      "R2:  0.9354217048440664\n",
      "#### Epoch: 27 of 1000 ####\n",
      " Training loss:\t0.05821504541221403\n",
      " Test loss:\t0.061518847942352295\n",
      " Validation loss:\t0.06408909650949332\n",
      "R2:  0.935620988978813\n",
      "#### Epoch: 28 of 1000 ####\n",
      " Training loss:\t0.057945329550158034\n",
      " Test loss:\t0.06132386280940129\n",
      " Validation loss:\t0.06389529888446514\n",
      "R2:  0.935837733082919\n",
      "#### Epoch: 29 of 1000 ####\n",
      " Training loss:\t0.057688295661793065\n",
      " Test loss:\t0.061160348928891696\n",
      " Validation loss:\t0.06372632430149959\n",
      "R2:  0.9360155844470586\n",
      "#### Epoch: 30 of 1000 ####\n",
      " Training loss:\t0.057443176673737585\n",
      " Test loss:\t0.06099721101614145\n",
      " Validation loss:\t0.06357165024830745\n",
      "R2:  0.9361896946875121\n",
      "#### Epoch: 31 of 1000 ####\n",
      " Training loss:\t0.05720973833004873\n",
      " Test loss:\t0.060844701070051924\n",
      " Validation loss:\t0.06340245558665349\n",
      "R2:  0.9363524197231003\n",
      "#### Epoch: 32 of 1000 ####\n",
      " Training loss:\t0.056983961533667596\n",
      " Test loss:\t0.060676551782167874\n",
      " Validation loss:\t0.06323436131844154\n",
      "R2:  0.9365305639047293\n",
      "#### Epoch: 33 of 1000 ####\n",
      " Training loss:\t0.05676835879441272\n",
      " Test loss:\t0.0605581127680265\n",
      " Validation loss:\t0.06310396469556369\n",
      "R2:  0.9366566052316048\n",
      "#### Epoch: 34 of 1000 ####\n",
      " Training loss:\t0.05656263614404209\n",
      " Test loss:\t0.06041954572384174\n",
      " Validation loss:\t0.06295963892569909\n",
      "R2:  0.9368010272141343\n",
      "#### Epoch: 35 of 1000 ####\n",
      " Training loss:\t0.05636891107254901\n",
      " Test loss:\t0.060306365673358626\n",
      " Validation loss:\t0.06284171801347\n",
      "R2:  0.9369192246418296\n",
      "#### Epoch: 36 of 1000 ####\n",
      " Training loss:\t0.05618115723097202\n",
      " Test loss:\t0.060186840020693265\n",
      " Validation loss:\t0.06271440707720242\n",
      "R2:  0.9370430975746699\n",
      "#### Epoch: 37 of 1000 ####\n",
      " Training loss:\t0.05600284252591624\n",
      " Test loss:\t0.06009162389315092\n",
      " Validation loss:\t0.06261223096113938\n",
      "R2:  0.9371411559667169\n",
      "#### Epoch: 38 of 1000 ####\n",
      " Training loss:\t0.05583150497335472\n",
      " Test loss:\t0.0599935559126047\n",
      " Validation loss:\t0.06250620805300199\n",
      "R2:  0.9372441413265189\n",
      "#### Epoch: 39 of 1000 ####\n",
      " Training loss:\t0.05566971679540516\n",
      " Test loss:\t0.05990337867003221\n",
      " Validation loss:\t0.06241716329868023\n",
      "R2:  0.9373347392401172\n",
      "#### Epoch: 40 of 1000 ####\n",
      " Training loss:\t0.05551462558146918\n",
      " Test loss:\t0.05982664456734291\n",
      " Validation loss:\t0.06233408359380869\n",
      "R2:  0.9374135554043269\n",
      "#### Epoch: 41 of 1000 ####\n",
      " Training loss:\t0.05536414140107616\n",
      " Test loss:\t0.059763738742241494\n",
      " Validation loss:\t0.062271760060237005\n",
      "R2:  0.9374807054252421\n",
      "#### Epoch: 42 of 1000 ####\n",
      " Training loss:\t0.055222127180656416\n",
      " Test loss:\t0.05969297427397508\n",
      " Validation loss:\t0.06220709819060106\n",
      "R2:  0.9375544789112366\n",
      "#### Epoch: 43 of 1000 ####\n",
      " Training loss:\t0.05508529874375612\n",
      " Test loss:\t0.059633566783024713\n",
      " Validation loss:\t0.06214327995593731\n",
      "R2:  0.9376153580855331\n",
      "#### Epoch: 44 of 1000 ####\n",
      " Training loss:\t0.054952448064630684\n",
      " Test loss:\t0.05956869858961839\n",
      " Validation loss:\t0.062078420932476334\n",
      "R2:  0.9376836413994085\n",
      "#### Epoch: 45 of 1000 ####\n",
      " Training loss:\t0.054826234637219674\n",
      " Test loss:\t0.059507640508505016\n",
      " Validation loss:\t0.06201515747950627\n",
      "R2:  0.9377466949738223\n",
      "#### Epoch: 46 of 1000 ####\n",
      " Training loss:\t0.054707209651976875\n",
      " Test loss:\t0.05944836598176222\n",
      " Validation loss:\t0.06195518603691688\n",
      "R2:  0.9378085916056322\n",
      "#### Epoch: 47 of 1000 ####\n",
      " Training loss:\t0.05459105085709381\n",
      " Test loss:\t0.05938055423589853\n",
      " Validation loss:\t0.061885797060452975\n",
      "R2:  0.9378796312762194\n",
      "#### Epoch: 48 of 1000 ####\n",
      " Training loss:\t0.05448064740919909\n",
      " Test loss:\t0.05932024350533119\n",
      " Validation loss:\t0.06182324427824754\n",
      "R2:  0.9379439951186226\n",
      "#### Epoch: 49 of 1000 ####\n",
      " Training loss:\t0.054375273574195235\n",
      " Test loss:\t0.059257828272306\n",
      " Validation loss:\t0.06175836691489586\n",
      "R2:  0.9380101439655051\n",
      "#### Epoch: 50 of 1000 ####\n",
      " Training loss:\t0.05427464916360156\n",
      " Test loss:\t0.0591972516133235\n",
      " Validation loss:\t0.06169731800372784\n",
      "R2:  0.9380726720426595\n",
      "#### Epoch: 51 of 1000 ####\n",
      " Training loss:\t0.054178884415508825\n",
      " Test loss:\t0.059137394795050986\n",
      " Validation loss:\t0.061631619930267334\n",
      "R2:  0.938136469262514\n",
      "#### Epoch: 52 of 1000 ####\n",
      " Training loss:\t0.05408751081803131\n",
      " Test loss:\t0.0590822696685791\n",
      " Validation loss:\t0.061569589834946856\n",
      "R2:  0.9381940943752597\n",
      "#### Epoch: 53 of 1000 ####\n",
      " Training loss:\t0.05399952704184462\n",
      " Test loss:\t0.05902954706778893\n",
      " Validation loss:\t0.061515941069676325\n",
      "R2:  0.9382494704698512\n",
      "#### Epoch: 54 of 1000 ####\n",
      " Training loss:\t0.053915424565103384\n",
      " Test loss:\t0.05897777814131517\n",
      " Validation loss:\t0.0614591515981234\n",
      "R2:  0.9383031713023715\n",
      "#### Epoch: 55 of 1000 ####\n",
      " Training loss:\t0.053835129895747104\n",
      " Test loss:\t0.058936540897075944\n",
      " Validation loss:\t0.06141429222547091\n",
      "R2:  0.938346237511796\n",
      "#### Epoch: 56 of 1000 ####\n",
      " Training loss:\t0.05375729721041776\n",
      " Test loss:\t0.058898595663217396\n",
      " Validation loss:\t0.06137123016210703\n",
      "R2:  0.938387195402447\n",
      "#### Epoch: 57 of 1000 ####\n",
      " Training loss:\t0.05368396132054062\n",
      " Test loss:\t0.05886291999083299\n",
      " Validation loss:\t0.06133348666704618\n",
      "R2:  0.9384243378837513\n",
      "#### Epoch: 58 of 1000 ####\n",
      " Training loss:\t0.05361314712698958\n",
      " Test loss:\t0.058833759564619795\n",
      " Validation loss:\t0.061301359763512246\n",
      "R2:  0.9384546489603404\n",
      "#### Epoch: 59 of 1000 ####\n",
      " Training loss:\t0.053546370403512566\n",
      " Test loss:\t0.0588111006296598\n",
      " Validation loss:\t0.06127418463046734\n",
      "R2:  0.9384783612336138\n",
      "#### Epoch: 60 of 1000 ####\n",
      " Training loss:\t0.05348211537636166\n",
      " Test loss:\t0.0587887167930603\n",
      " Validation loss:\t0.061246972817641035\n",
      "R2:  0.9385030068625594\n",
      "#### Epoch: 61 of 1000 ####\n",
      " Training loss:\t0.05342120423796377\n",
      " Test loss:\t0.058770679510556735\n",
      " Validation loss:\t0.06122557475016667\n",
      "R2:  0.9385218942320537\n",
      "#### Epoch: 62 of 1000 ####\n",
      " Training loss:\t0.05336306283159618\n",
      " Test loss:\t0.05875316032996544\n",
      " Validation loss:\t0.06120640956438505\n",
      "R2:  0.9385402539364676\n",
      "#### Epoch: 63 of 1000 ####\n",
      " Training loss:\t0.05330709403426729\n",
      " Test loss:\t0.05873870391112108\n",
      " Validation loss:\t0.06118693260046152\n",
      "R2:  0.9385558094392354\n",
      "#### Epoch: 64 of 1000 ####\n",
      " Training loss:\t0.05325339430430652\n",
      " Test loss:\t0.05872318836358877\n",
      " Validation loss:\t0.061169954446645886\n",
      "R2:  0.9385725748909272\n",
      "#### Epoch: 65 of 1000 ####\n",
      " Training loss:\t0.05320250105240631\n",
      " Test loss:\t0.05870822759775015\n",
      " Validation loss:\t0.061152128072885364\n",
      "R2:  0.9385895365701417\n",
      "#### Epoch: 66 of 1000 ####\n",
      " Training loss:\t0.053153431322257395\n",
      " Test loss:\t0.058691914264972396\n",
      " Validation loss:\t0.06113445300322313\n",
      "R2:  0.9386060006744573\n",
      "#### Epoch: 67 of 1000 ####\n",
      " Training loss:\t0.05310694759398753\n",
      " Test loss:\t0.05867837025569035\n",
      " Validation loss:\t0.0611193455182589\n",
      "R2:  0.9386213685280074\n",
      "#### Epoch: 68 of 1000 ####\n",
      " Training loss:\t0.053062824798161407\n",
      " Test loss:\t0.058662460400508\n",
      " Validation loss:\t0.06110370617646437\n",
      "R2:  0.9386379068629356\n",
      "#### Epoch: 69 of 1000 ####\n",
      " Training loss:\t0.053020222369337\n",
      " Test loss:\t0.05864827908002413\n",
      " Validation loss:\t0.06108710399040809\n",
      "R2:  0.9386531052136622\n",
      "#### Epoch: 70 of 1000 ####\n",
      " Training loss:\t0.052979420495994974\n",
      " Test loss:\t0.05863417570407574\n",
      " Validation loss:\t0.06107163429260254\n",
      "R2:  0.9386684050436932\n",
      "#### Epoch: 71 of 1000 ####\n",
      " Training loss:\t0.05294082338446813\n",
      " Test loss:\t0.058621149796705976\n",
      " Validation loss:\t0.06105796648905827\n",
      "R2:  0.938681828628538\n",
      "#### Epoch: 72 of 1000 ####\n",
      " Training loss:\t0.05290395333636316\n",
      " Test loss:\t0.05860831645818857\n",
      " Validation loss:\t0.06104552745819092\n",
      "R2:  0.9386955101905472\n",
      "#### Epoch: 73 of 1000 ####\n",
      " Training loss:\t0.05286894355603975\n",
      " Test loss:\t0.05859561608387874\n",
      " Validation loss:\t0.06103138281748845\n",
      "R2:  0.9387086875732096\n",
      "#### Epoch: 74 of 1000 ####\n",
      " Training loss:\t0.052835568974062594\n",
      " Test loss:\t0.058585295310387246\n",
      " Validation loss:\t0.06102021382405208\n",
      "R2:  0.9387197689811818\n",
      "#### Epoch: 75 of 1000 ####\n",
      " Training loss:\t0.05280372853884849\n",
      " Test loss:\t0.05857446101995615\n",
      " Validation loss:\t0.06100935660875761\n",
      "R2:  0.9387308494641224\n",
      "#### Epoch: 76 of 1000 ####\n",
      " Training loss:\t0.0527736335400714\n",
      " Test loss:\t0.05856312696750347\n",
      " Validation loss:\t0.060996720424065225\n",
      "R2:  0.9387427047320336\n",
      "#### Epoch: 77 of 1000 ####\n",
      " Training loss:\t0.05274480627933803\n",
      " Test loss:\t0.05855476397734422\n",
      " Validation loss:\t0.06098673435357901\n",
      "R2:  0.9387517220441322\n",
      "#### Epoch: 78 of 1000 ####\n",
      " Training loss:\t0.052717669335996295\n",
      " Test loss:\t0.058544429448934704\n",
      " Validation loss:\t0.06097647776970497\n",
      "R2:  0.9387620775286221\n",
      "#### Epoch: 79 of 1000 ####\n",
      " Training loss:\t0.05269151534896382\n",
      " Test loss:\t0.058535993099212646\n",
      " Validation loss:\t0.06096645960441002\n",
      "R2:  0.9387707746147904\n",
      "#### Epoch: 80 of 1000 ####\n",
      " Training loss:\t0.05266693684797844\n",
      " Test loss:\t0.05852638758145846\n",
      " Validation loss:\t0.060955519859607406\n",
      "R2:  0.9387807834436889\n",
      "#### Epoch: 81 of 1000 ####\n",
      " Training loss:\t0.05264343776163174\n",
      " Test loss:\t0.05851702048228337\n",
      " Validation loss:\t0.060945203671088584\n",
      "R2:  0.9387908028756468\n",
      "#### Epoch: 82 of 1000 ####\n",
      " Training loss:\t0.05262110536174556\n",
      " Test loss:\t0.05850920768884512\n",
      " Validation loss:\t0.06093641427847055\n",
      "R2:  0.938798226162507\n",
      "#### Epoch: 83 of 1000 ####\n",
      " Training loss:\t0.052599751324914866\n",
      " Test loss:\t0.05850110604212834\n",
      " Validation loss:\t0.06092729476781992\n",
      "R2:  0.9388068071053282\n",
      "#### Epoch: 84 of 1000 ####\n",
      " Training loss:\t0.0525794399566926\n",
      " Test loss:\t0.058493155699509844\n",
      " Validation loss:\t0.06091784513913668\n",
      "R2:  0.9388153528365737\n",
      "#### Epoch: 85 of 1000 ####\n",
      " Training loss:\t0.05256009776501825\n",
      " Test loss:\t0.05848484772902269\n",
      " Validation loss:\t0.06090854681455172\n",
      "R2:  0.9388236774795408\n",
      "#### Epoch: 86 of 1000 ####\n",
      " Training loss:\t0.052541660444338875\n",
      " Test loss:\t0.05847833248285147\n",
      " Validation loss:\t0.06090141718204205\n",
      "R2:  0.9388307716262502\n",
      "#### Epoch: 87 of 1000 ####\n",
      " Training loss:\t0.05252428875853684\n",
      " Test loss:\t0.058470505934495196\n",
      " Validation loss:\t0.06089308628669152\n",
      "R2:  0.938838605697043\n",
      "#### Epoch: 88 of 1000 ####\n",
      " Training loss:\t0.05250775763817683\n",
      " Test loss:\t0.058464655509361856\n",
      " Validation loss:\t0.060885956654181846\n",
      "R2:  0.9388447895351896\n",
      "#### Epoch: 89 of 1000 ####\n",
      " Training loss:\t0.05249203952373617\n",
      " Test loss:\t0.05845743417739868\n",
      " Validation loss:\t0.06087779081784762\n",
      "R2:  0.9388522766241182\n",
      "#### Epoch: 90 of 1000 ####\n",
      " Training loss:\t0.052477079296169474\n",
      " Test loss:\t0.05845065300281231\n",
      " Validation loss:\t0.06087044569162222\n",
      "R2:  0.9388596551661472\n",
      "#### Epoch: 91 of 1000 ####\n",
      " Training loss:\t0.05246288154873053\n",
      " Test loss:\t0.05844486218232375\n",
      " Validation loss:\t0.06086414135419405\n",
      "R2:  0.9388656833815646\n",
      "#### Epoch: 92 of 1000 ####\n",
      " Training loss:\t0.052449473840942014\n",
      " Test loss:\t0.05843851658014151\n",
      " Validation loss:\t0.06085741061430711\n",
      "R2:  0.9388723775561686\n",
      "#### Epoch: 93 of 1000 ####\n",
      " Training loss:\t0.05243638766091822\n",
      " Test loss:\t0.058432936668395996\n",
      " Validation loss:\t0.06085082200857309\n",
      "R2:  0.9388784929633421\n",
      "#### Epoch: 94 of 1000 ####\n",
      " Training loss:\t0.05242416960585914\n",
      " Test loss:\t0.05842744845610399\n",
      " Validation loss:\t0.06084469648507925\n",
      "R2:  0.9388841955729016\n",
      "#### Epoch: 95 of 1000 ####\n",
      " Training loss:\t0.05241237413014797\n",
      " Test loss:\t0.0584221940774184\n",
      " Validation loss:\t0.06083908447852501\n",
      "R2:  0.9388898691244004\n",
      "#### Epoch: 96 of 1000 ####\n",
      " Training loss:\t0.05240133194805699\n",
      " Test loss:\t0.058417668709388144\n",
      " Validation loss:\t0.06083353207661556\n",
      "R2:  0.9388947846304119\n",
      "#### Epoch: 97 of 1000 ####\n",
      " Training loss:\t0.05239071234531391\n",
      " Test loss:\t0.058412405160757214\n",
      " Validation loss:\t0.06082763121678279\n",
      "R2:  0.9389005275889654\n",
      "#### Epoch: 98 of 1000 ####\n",
      " Training loss:\t0.052380556661202775\n",
      " Test loss:\t0.058408031096825234\n",
      " Validation loss:\t0.06082276197580191\n",
      "R2:  0.9389051268885382\n",
      "#### Epoch: 99 of 1000 ####\n",
      " Training loss:\t0.0523708373362009\n",
      " Test loss:\t0.058403922961308405\n",
      " Validation loss:\t0.060817796450394854\n",
      "R2:  0.9389095181789588\n",
      "#### Epoch: 100 of 1000 ####\n",
      " Training loss:\t0.052361719727444404\n",
      " Test loss:\t0.05839964976677528\n",
      " Validation loss:\t0.060813459066244274\n",
      "R2:  0.9389138919098537\n",
      "#### Epoch: 101 of 1000 ####\n",
      " Training loss:\t0.052352919053198846\n",
      " Test loss:\t0.05839618811240563\n",
      " Validation loss:\t0.06080917670176579\n",
      "R2:  0.9389174305971989\n",
      "#### Epoch: 102 of 1000 ####\n",
      " Training loss:\t0.05234459148409279\n",
      " Test loss:\t0.05839276313781738\n",
      " Validation loss:\t0.060805247380183294\n",
      "R2:  0.9389210657692565\n",
      "#### Epoch: 103 of 1000 ####\n",
      " Training loss:\t0.05233653032370607\n",
      " Test loss:\t0.05838956741186289\n",
      " Validation loss:\t0.06080148311761709\n",
      "R2:  0.9389242561955938\n",
      "#### Epoch: 104 of 1000 ####\n",
      " Training loss:\t0.05232894686171263\n",
      " Test loss:\t0.05838643587552584\n",
      " Validation loss:\t0.060798012293302096\n",
      "R2:  0.9389274289648308\n",
      "#### Epoch: 105 of 1000 ####\n",
      " Training loss:\t0.052321597655662064\n",
      " Test loss:\t0.0583835244178772\n",
      " Validation loss:\t0.0607947982274569\n",
      "R2:  0.938930278839137\n",
      "#### Epoch: 106 of 1000 ####\n",
      " Training loss:\t0.05231473074125856\n",
      " Test loss:\t0.058381071457496055\n",
      " Validation loss:\t0.06079183633510883\n",
      "R2:  0.9389326712754898\n",
      "#### Epoch: 107 of 1000 ####\n",
      " Training loss:\t0.05230807511652901\n",
      " Test loss:\t0.05837852221268874\n",
      " Validation loss:\t0.06078896155724159\n",
      "R2:  0.9389351358545887\n",
      "#### Epoch: 108 of 1000 ####\n",
      " Training loss:\t0.052301708866787706\n",
      " Test loss:\t0.05837629850094135\n",
      " Validation loss:\t0.06078643523729765\n",
      "R2:  0.938937368444785\n",
      "#### Epoch: 109 of 1000 ####\n",
      " Training loss:\t0.05229567333131867\n",
      " Test loss:\t0.05837421233837421\n",
      " Validation loss:\t0.0607841656758235\n",
      "R2:  0.9389392402733538\n",
      "#### Epoch: 110 of 1000 ####\n",
      " Training loss:\t0.052289940950599224\n",
      " Test loss:\t0.058372208705315225\n",
      " Validation loss:\t0.060782006153693564\n",
      "R2:  0.9389412015765921\n",
      "#### Epoch: 111 of 1000 ####\n",
      " Training loss:\t0.05228440148653861\n",
      " Test loss:\t0.05837040681105394\n",
      " Validation loss:\t0.06077994750096248\n",
      "R2:  0.9389428442961327\n",
      "#### Epoch: 112 of 1000 ####\n",
      " Training loss:\t0.052279197330004044\n",
      " Test loss:\t0.05836885709028978\n",
      " Validation loss:\t0.06077830149577214\n",
      "R2:  0.9389442409349338\n",
      "#### Epoch: 113 of 1000 ####\n",
      " Training loss:\t0.05227406207227621\n",
      " Test loss:\t0.05836711938564594\n",
      " Validation loss:\t0.06077651335642888\n",
      "R2:  0.9389458007990579\n",
      "#### Epoch: 114 of 1000 ####\n",
      " Training loss:\t0.052269248342313085\n",
      " Test loss:\t0.05836574389384343\n",
      " Validation loss:\t0.06077506909003624\n",
      "R2:  0.938946921666324\n",
      "#### Epoch: 115 of 1000 ####\n",
      " Training loss:\t0.05226464130877013\n",
      " Test loss:\t0.058364455516521745\n",
      " Validation loss:\t0.06077362023867094\n",
      "R2:  0.938948006902369\n",
      "#### Epoch: 116 of 1000 ####\n",
      " Training loss:\t0.052260259344662476\n",
      " Test loss:\t0.05836317630914541\n",
      " Validation loss:\t0.06077235020123995\n",
      "R2:  0.9389491523163325\n",
      "#### Epoch: 117 of 1000 ####\n",
      " Training loss:\t0.05225614378927416\n",
      " Test loss:\t0.0583619291965778\n",
      " Validation loss:\t0.060771057238945596\n",
      "R2:  0.938950164621349\n",
      "#### Epoch: 118 of 1000 ####\n",
      " Training loss:\t0.052252133878722816\n",
      " Test loss:\t0.05836072334876427\n",
      " Validation loss:\t0.06076989724085881\n",
      "R2:  0.9389511644676497\n",
      "#### Epoch: 119 of 1000 ####\n",
      " Training loss:\t0.052248257172531135\n",
      " Test loss:\t0.058359742164611816\n",
      " Validation loss:\t0.060768906886761\n",
      "R2:  0.938952006571838\n",
      "#### Epoch: 120 of 1000 ####\n",
      " Training loss:\t0.0522446193155361\n",
      " Test loss:\t0.05835871054575993\n",
      " Validation loss:\t0.06076808617665218\n",
      "R2:  0.9389528328347017\n",
      "#### Epoch: 121 of 1000 ####\n",
      " Training loss:\t0.05224119734146881\n",
      " Test loss:\t0.05835767892690805\n",
      " Validation loss:\t0.06076716459714449\n",
      "R2:  0.9389535891428576\n",
      "#### Epoch: 122 of 1000 ####\n",
      " Training loss:\t0.05223786263922336\n",
      " Test loss:\t0.0583569361613347\n",
      " Validation loss:\t0.06076649519113394\n",
      "R2:  0.9389541808907129\n",
      "#### Epoch: 123 of 1000 ####\n",
      " Training loss:\t0.052234780565935904\n",
      " Test loss:\t0.05835607877144447\n",
      " Validation loss:\t0.060765752425560586\n",
      "R2:  0.9389548802039538\n",
      "#### Epoch: 124 of 1000 ####\n",
      " Training loss:\t0.05223178117121651\n",
      " Test loss:\t0.05835518928674551\n",
      " Validation loss:\t0.06076509218949538\n",
      "R2:  0.9389556121788021\n",
      "#### Epoch: 125 of 1000 ####\n",
      " Training loss:\t0.05222887823482653\n",
      " Test loss:\t0.05835447861598088\n",
      " Validation loss:\t0.0607643723487854\n",
      "R2:  0.938956131001013\n",
      "#### Epoch: 126 of 1000 ####\n",
      " Training loss:\t0.05222609931628866\n",
      " Test loss:\t0.0583536487359267\n",
      " Validation loss:\t0.06076375337747427\n",
      "R2:  0.9389567080127806\n",
      "#### Epoch: 127 of 1000 ####\n",
      " Training loss:\t0.05222352709417096\n",
      " Test loss:\t0.0583528601206266\n",
      " Validation loss:\t0.06076307021654569\n",
      "R2:  0.9389573735091189\n",
      "#### Epoch: 128 of 1000 ####\n",
      " Training loss:\t0.05222106511014402\n",
      " Test loss:\t0.058352144864889294\n",
      " Validation loss:\t0.060762497094961315\n",
      "R2:  0.9389578632543295\n",
      "#### Epoch: 129 of 1000 ####\n",
      " Training loss:\t0.0522185985328633\n",
      " Test loss:\t0.058351374589479886\n",
      " Validation loss:\t0.060761768084305987\n",
      "R2:  0.9389585128489706\n",
      "#### Epoch: 130 of 1000 ####\n",
      " Training loss:\t0.052216375398033\n",
      " Test loss:\t0.05835073269330538\n",
      " Validation loss:\t0.06076132792692918\n",
      "R2:  0.9389590430107634\n",
      "#### Epoch: 131 of 1000 ####\n",
      " Training loss:\t0.05221416604296405\n",
      " Test loss:\t0.05835008162718553\n",
      " Validation loss:\t0.0607606631058913\n",
      "R2:  0.9389595406843552\n",
      "#### Epoch: 132 of 1000 ####\n",
      " Training loss:\t0.052212094485508544\n",
      " Test loss:\t0.058349425976093\n",
      " Validation loss:\t0.06076014500397902\n",
      "R2:  0.9389600480192815\n",
      "#### Epoch: 133 of 1000 ####\n",
      " Training loss:\t0.05221022503121943\n",
      " Test loss:\t0.05834879324986385\n",
      " Validation loss:\t0.06075969109168419\n",
      "R2:  0.9389605435084136\n",
      "#### Epoch: 134 of 1000 ####\n",
      " Training loss:\t0.05220831883090006\n",
      " Test loss:\t0.05834818344849806\n",
      " Validation loss:\t0.06075918215971727\n",
      "R2:  0.9389610436028033\n",
      "#### Epoch: 135 of 1000 ####\n",
      " Training loss:\t0.052206600953985736\n",
      " Test loss:\t0.058347578232104964\n",
      " Validation loss:\t0.06075863654796894\n",
      "R2:  0.9389614810648715\n",
      "#### Epoch: 136 of 1000 ####\n",
      " Training loss:\t0.052204956569131926\n",
      " Test loss:\t0.05834702345041128\n",
      " Validation loss:\t0.06075813678594736\n",
      "R2:  0.938961993214799\n",
      "#### Epoch: 137 of 1000 ####\n",
      " Training loss:\t0.052203275438247855\n",
      " Test loss:\t0.058346491593580976\n",
      " Validation loss:\t0.060757692043597884\n",
      "R2:  0.9389624053575525\n",
      "#### Epoch: 138 of 1000 ####\n",
      " Training loss:\t0.052201732104977236\n",
      " Test loss:\t0.058345955151778\n",
      " Validation loss:\t0.0607572473012484\n",
      "R2:  0.9389628342546767\n",
      "#### Epoch: 139 of 1000 ####\n",
      " Training loss:\t0.05220038628161922\n",
      " Test loss:\t0.05834543704986572\n",
      " Validation loss:\t0.0607568254837623\n",
      "R2:  0.9389632763871518\n",
      "#### Epoch: 140 of 1000 ####\n",
      " Training loss:\t0.05219895318643936\n",
      " Test loss:\t0.05834502440232497\n",
      " Validation loss:\t0.060756371571467474\n",
      "R2:  0.9389636227254404\n",
      "#### Epoch: 141 of 1000 ####\n",
      " Training loss:\t0.05219765788887295\n",
      " Test loss:\t0.058344515470358044\n",
      " Validation loss:\t0.06075597726381742\n",
      "R2:  0.9389640056993057\n",
      "#### Epoch: 142 of 1000 ####\n",
      " Training loss:\t0.05219640852384435\n",
      " Test loss:\t0.058344130332653336\n",
      " Validation loss:\t0.060755642560812145\n",
      "R2:  0.9389643329732061\n",
      "#### Epoch: 143 of 1000 ####\n",
      " Training loss:\t0.05219530614293676\n",
      " Test loss:\t0.05834369017527653\n",
      " Validation loss:\t0.060755252838134766\n",
      "R2:  0.9389647265511715\n",
      "#### Epoch: 144 of 1000 ####\n",
      " Training loss:\t0.05219416701599893\n",
      " Test loss:\t0.05834326835779043\n",
      " Validation loss:\t0.06075493189004751\n",
      "R2:  0.9389650911794061\n",
      "#### Epoch: 145 of 1000 ####\n",
      " Training loss:\t0.05219306004183756\n",
      " Test loss:\t0.05834291072992178\n",
      " Validation loss:\t0.06075462011190561\n",
      "R2:  0.9389653610969433\n",
      "#### Epoch: 146 of 1000 ####\n",
      " Training loss:\t0.05219210464505099\n",
      " Test loss:\t0.05834252559221708\n",
      " Validation loss:\t0.06075434042857243\n",
      "R2:  0.9389656821415517\n",
      "#### Epoch: 147 of 1000 ####\n",
      " Training loss:\t0.052191080349457686\n",
      " Test loss:\t0.058342195474184476\n",
      " Validation loss:\t0.060754051575293906\n",
      "R2:  0.9389659819746056\n",
      "#### Epoch: 148 of 1000 ####\n",
      " Training loss:\t0.05219014791894002\n",
      " Test loss:\t0.05834188828101525\n",
      " Validation loss:\t0.060753813156714805\n",
      "R2:  0.9389662658196851\n",
      "#### Epoch: 149 of 1000 ####\n",
      " Training loss:\t0.052189256827706386\n",
      " Test loss:\t0.058341581087846026\n",
      " Validation loss:\t0.06075348303868221\n",
      "R2:  0.9389665402047183\n",
      "#### Epoch: 150 of 1000 ####\n",
      " Training loss:\t0.0521885035340862\n",
      " Test loss:\t0.05834127847964947\n",
      " Validation loss:\t0.060753313394693226\n",
      "R2:  0.9389668088294907\n",
      "#### Epoch: 151 of 1000 ####\n",
      " Training loss:\t0.052187722680943335\n",
      " Test loss:\t0.05834102172117967\n",
      " Validation loss:\t0.06075305205125075\n",
      "R2:  0.9389670353758047\n",
      "#### Epoch: 152 of 1000 ####\n",
      " Training loss:\t0.052186960200815584\n",
      " Test loss:\t0.058340764962709867\n",
      " Validation loss:\t0.06075288699223445\n",
      "R2:  0.938967270713581\n",
      "#### Epoch: 153 of 1000 ####\n",
      " Training loss:\t0.05218628039925591\n",
      " Test loss:\t0.05834053571407612\n",
      " Validation loss:\t0.0607527127632728\n",
      "R2:  0.938967477805368\n",
      "#### Epoch: 154 of 1000 ####\n",
      " Training loss:\t0.052185618970711355\n",
      " Test loss:\t0.05834032480533306\n",
      " Validation loss:\t0.060752506439502425\n",
      "R2:  0.9389676954092742\n",
      "#### Epoch: 155 of 1000 ####\n",
      " Training loss:\t0.0521848840501063\n",
      " Test loss:\t0.05834012306653536\n",
      " Validation loss:\t0.060752373475294844\n",
      "R2:  0.9389678910263448\n",
      "#### Epoch: 156 of 1000 ####\n",
      " Training loss:\t0.05218428692711469\n",
      " Test loss:\t0.058339912157792315\n",
      " Validation loss:\t0.060752235926114596\n",
      "R2:  0.9389680987056962\n",
      "#### Epoch: 157 of 1000 ####\n",
      " Training loss:\t0.05218377248269115\n",
      " Test loss:\t0.05833975626872136\n",
      " Validation loss:\t0.06075210296190702\n",
      "R2:  0.9389682656916073\n",
      "#### Epoch: 158 of 1000 ####\n",
      " Training loss:\t0.052183207512476015\n",
      " Test loss:\t0.05833960954959576\n",
      " Validation loss:\t0.060751960827754095\n",
      "R2:  0.9389684237575404\n",
      "#### Epoch: 159 of 1000 ####\n",
      " Training loss:\t0.05218268388154491\n",
      " Test loss:\t0.05833943073566143\n",
      " Validation loss:\t0.06075182327857384\n",
      "R2:  0.9389686200077184\n",
      "#### Epoch: 160 of 1000 ####\n",
      " Training loss:\t0.052182160250613806\n",
      " Test loss:\t0.05833923816680908\n",
      " Validation loss:\t0.06075174533403837\n",
      "R2:  0.9389687694364139\n",
      "#### Epoch: 161 of 1000 ####\n",
      " Training loss:\t0.052181677958966735\n",
      " Test loss:\t0.05833912812746488\n",
      " Validation loss:\t0.06075163987966684\n",
      "R2:  0.9389689132413926\n",
      "#### Epoch: 162 of 1000 ####\n",
      " Training loss:\t0.05218126915938017\n",
      " Test loss:\t0.05833899974822998\n",
      " Validation loss:\t0.06075153901026799\n",
      "R2:  0.938969064151999\n",
      "#### Epoch: 163 of 1000 ####\n",
      " Training loss:\t0.05218083280027092\n",
      " Test loss:\t0.05833888970888578\n",
      " Validation loss:\t0.06075142438595112\n",
      "R2:  0.9389691902474505\n",
      "#### Epoch: 164 of 1000 ####\n",
      " Training loss:\t0.05218037347489276\n",
      " Test loss:\t0.05833875215970553\n",
      " Validation loss:\t0.06075133727147029\n",
      "R2:  0.9389692996767772\n",
      "#### Epoch: 165 of 1000 ####\n",
      " Training loss:\t0.0521799876415751\n",
      " Test loss:\t0.05833865129030668\n",
      " Validation loss:\t0.06075125474196214\n",
      "R2:  0.9389694401265938\n",
      "#### Epoch: 166 of 1000 ####\n",
      " Training loss:\t0.05217955587571963\n",
      " Test loss:\t0.058338555005880505\n",
      " Validation loss:\t0.06075117221245399\n",
      "R2:  0.9389695434157056\n",
      "#### Epoch: 167 of 1000 ####\n",
      " Training loss:\t0.052179188415417105\n",
      " Test loss:\t0.0583384449665363\n",
      " Validation loss:\t0.060751075928027816\n",
      "R2:  0.9389696609384958\n",
      "#### Epoch: 168 of 1000 ####\n",
      " Training loss:\t0.052178765836069195\n",
      " Test loss:\t0.05833837619194618\n",
      " Validation loss:\t0.060750988813546986\n",
      "R2:  0.9389697639865553\n",
      "#### Epoch: 169 of 1000 ####\n",
      " Training loss:\t0.05217848564758852\n",
      " Test loss:\t0.058338275322547324\n",
      " Validation loss:\t0.06075090169906616\n",
      "R2:  0.9389698592059669\n",
      "#### Epoch: 170 of 1000 ####\n",
      " Training loss:\t0.0521781411535549\n",
      " Test loss:\t0.05833819279303917\n",
      " Validation loss:\t0.060750878774202786\n",
      "R2:  0.9389699261695342\n",
      "#### Epoch: 171 of 1000 ####\n",
      " Training loss:\t0.05217778287975993\n",
      " Test loss:\t0.058338119433476374\n",
      " Validation loss:\t0.06075076873485859\n",
      "R2:  0.9389700308749933\n",
      "#### Epoch: 172 of 1000 ####\n",
      " Training loss:\t0.05217751187778682\n",
      " Test loss:\t0.05833805065888625\n",
      " Validation loss:\t0.06075069537529579\n",
      "R2:  0.9389701250375494\n",
      "#### Epoch: 173 of 1000 ####\n",
      " Training loss:\t0.05217722709605236\n",
      " Test loss:\t0.05833800022418682\n",
      " Validation loss:\t0.060750649525569037\n",
      "R2:  0.9389701766854753\n",
      "#### Epoch: 174 of 1000 ####\n",
      " Training loss:\t0.05217698365360193\n",
      " Test loss:\t0.058337913109706\n",
      " Validation loss:\t0.06075057158103356\n",
      "R2:  0.9389702603067362\n",
      "#### Epoch: 175 of 1000 ####\n",
      " Training loss:\t0.05217668049885235\n",
      " Test loss:\t0.058337848920088545\n",
      " Validation loss:\t0.06075054407119751\n",
      "R2:  0.9389703355717122\n",
      "#### Epoch: 176 of 1000 ####\n",
      " Training loss:\t0.05217650136195486\n",
      " Test loss:\t0.0583377847304711\n",
      " Validation loss:\t0.06075047071163471\n",
      "R2:  0.9389703942489811\n",
      "#### Epoch: 177 of 1000 ####\n",
      " Training loss:\t0.052176234953235534\n",
      " Test loss:\t0.058337738880744346\n",
      " Validation loss:\t0.06075042486190796\n",
      "R2:  0.9389704685531212\n",
      "#### Epoch: 178 of 1000 ####\n",
      " Training loss:\t0.0521760190703078\n",
      " Test loss:\t0.0583376746911269\n",
      " Validation loss:\t0.060750374427208535\n",
      "R2:  0.9389705186980153\n",
      "#### Epoch: 179 of 1000 ####\n",
      " Training loss:\t0.05217581696714141\n",
      " Test loss:\t0.0583376380113455\n",
      " Validation loss:\t0.06075031940753643\n",
      "R2:  0.9389705826616483\n",
      "#### Epoch: 180 of 1000 ####\n",
      " Training loss:\t0.05217561486397501\n",
      " Test loss:\t0.05833757382172804\n",
      " Validation loss:\t0.06075025980289166\n",
      "R2:  0.938970642872093\n",
      "#### Epoch: 181 of 1000 ####\n",
      " Training loss:\t0.05217542654056997\n",
      " Test loss:\t0.058337537141946644\n",
      " Validation loss:\t0.06075020936819223\n",
      "R2:  0.9389706984387095\n",
      "#### Epoch: 182 of 1000 ####\n",
      " Training loss:\t0.052175265776687615\n",
      " Test loss:\t0.05833748670724722\n",
      " Validation loss:\t0.060750181858356185\n",
      "R2:  0.9389707485233081\n",
      "#### Epoch: 183 of 1000 ####\n",
      " Training loss:\t0.052175137165581725\n",
      " Test loss:\t0.058337463782383844\n",
      " Validation loss:\t0.06075010849879338\n",
      "R2:  0.938970795081059\n",
      "#### Epoch: 184 of 1000 ####\n",
      " Training loss:\t0.0521749442489229\n",
      " Test loss:\t0.05833742251762977\n",
      " Validation loss:\t0.060750099328848034\n",
      "R2:  0.938970845404673\n",
      "#### Epoch: 185 of 1000 ####\n",
      " Training loss:\t0.052174792671548106\n",
      " Test loss:\t0.05833738125287569\n",
      " Validation loss:\t0.06075008098895733\n",
      "R2:  0.9389708841983857\n",
      "#### Epoch: 186 of 1000 ####\n",
      " Training loss:\t0.05217468702671113\n",
      " Test loss:\t0.058337362912985\n",
      " Validation loss:\t0.0607500488941486\n",
      "R2:  0.9389709228644227\n",
      "#### Epoch: 187 of 1000 ####\n",
      " Training loss:\t0.052174563008859025\n",
      " Test loss:\t0.058337344573094294\n",
      " Validation loss:\t0.06075000762939453\n",
      "R2:  0.9389709669274511\n",
      "#### Epoch: 188 of 1000 ####\n",
      " Training loss:\t0.0521744435842607\n",
      " Test loss:\t0.05833729872336754\n",
      " Validation loss:\t0.06074997094961313\n",
      "R2:  0.9389710016890245\n",
      "#### Epoch: 189 of 1000 ####\n",
      " Training loss:\t0.05217428282037835\n",
      " Test loss:\t0.058337275798504167\n",
      " Validation loss:\t0.06074992968485905\n",
      "R2:  0.9389710336370924\n",
      "#### Epoch: 190 of 1000 ####\n",
      " Training loss:\t0.052174190955302714\n",
      " Test loss:\t0.058337243703695446\n",
      " Validation loss:\t0.060749906759995684\n",
      "R2:  0.9389710689771193\n",
      "#### Epoch: 191 of 1000 ####\n",
      " Training loss:\t0.05217408531046574\n",
      " Test loss:\t0.05833721619385939\n",
      " Validation loss:\t0.06074987008021428\n",
      "R2:  0.9389711000692033\n",
      "#### Epoch: 192 of 1000 ####\n",
      " Training loss:\t0.05217400263189767\n",
      " Test loss:\t0.05833720243894137\n",
      " Validation loss:\t0.06074984715535091\n",
      "R2:  0.9389711265350973\n",
      "#### Epoch: 193 of 1000 ####\n",
      " Training loss:\t0.05217393373309095\n",
      " Test loss:\t0.05833720243894137\n",
      " Validation loss:\t0.060749828815460205\n",
      "R2:  0.9389711545399939\n",
      "#### Epoch: 194 of 1000 ####\n",
      " Training loss:\t0.052173846461269095\n",
      " Test loss:\t0.05833717492910532\n",
      " Validation loss:\t0.06074982423048753\n",
      "R2:  0.9389711771757105\n",
      "#### Epoch: 195 of 1000 ####\n",
      " Training loss:\t0.05217375918944724\n",
      " Test loss:\t0.05833714741926927\n",
      " Validation loss:\t0.0607498104755695\n",
      "R2:  0.9389712056971091\n",
      "#### Epoch: 196 of 1000 ####\n",
      " Training loss:\t0.05217368569738674\n",
      " Test loss:\t0.05833712907937857\n",
      " Validation loss:\t0.06074979672065148\n",
      "R2:  0.9389712299147359\n",
      "#### Epoch: 197 of 1000 ####\n",
      " Training loss:\t0.05217360301881867\n",
      " Test loss:\t0.05833710156954252\n",
      " Validation loss:\t0.06074977838076078\n",
      "R2:  0.9389712539739794\n",
      "#### Epoch: 198 of 1000 ####\n",
      " Training loss:\t0.0521735203402506\n",
      " Test loss:\t0.05833709698456984\n",
      " Validation loss:\t0.06074973253103403\n",
      "R2:  0.9389712860331566\n",
      "#### Epoch: 199 of 1000 ####\n",
      " Training loss:\t0.052173433068428754\n",
      " Test loss:\t0.05833709239959717\n",
      " Validation loss:\t0.0607497371160067\n",
      "R2:  0.9389713079849631\n",
      "#### Epoch: 200 of 1000 ####\n",
      " Training loss:\t0.05217336876287581\n",
      " Test loss:\t0.05833706030478844\n",
      " Validation loss:\t0.06074970502119798\n",
      "R2:  0.9389713356135656\n",
      "#### Epoch: 201 of 1000 ####\n",
      " Training loss:\t0.05217330905057665\n",
      " Test loss:\t0.058337041964897744\n",
      " Validation loss:\t0.06074969126627995\n",
      "R2:  0.9389713668933405\n",
      "#### Epoch: 202 of 1000 ####\n",
      " Training loss:\t0.052173240151769926\n",
      " Test loss:\t0.05833702362500704\n",
      " Validation loss:\t0.0607496637564439\n",
      "R2:  0.9389713902595045\n",
      "#### Epoch: 203 of 1000 ####\n",
      " Training loss:\t0.05217318503272454\n",
      " Test loss:\t0.058337000700143665\n",
      " Validation loss:\t0.06074964541655321\n",
      "R2:  0.9389714111296494\n",
      "#### Epoch: 204 of 1000 ####\n",
      " Training loss:\t0.052173102354156474\n",
      " Test loss:\t0.05833697319030762\n",
      " Validation loss:\t0.06074962249168983\n",
      "R2:  0.9389714428601632\n",
      "#### Epoch: 205 of 1000 ####\n",
      " Training loss:\t0.05217303345534975\n",
      " Test loss:\t0.058336968605334945\n",
      " Validation loss:\t0.06074961790671715\n",
      "R2:  0.9389714616997089\n",
      "#### Epoch: 206 of 1000 ####\n",
      " Training loss:\t0.052172969149796806\n",
      " Test loss:\t0.058336931925553545\n",
      " Validation loss:\t0.06074960415179913\n",
      "R2:  0.938971488005524\n",
      "#### Epoch: 207 of 1000 ####\n",
      " Training loss:\t0.05217291403075143\n",
      " Test loss:\t0.05833690900069017\n",
      " Validation loss:\t0.060749594981853776\n",
      "R2:  0.9389715100770839\n",
      "#### Epoch: 208 of 1000 ####\n",
      " Training loss:\t0.05217284513194471\n",
      " Test loss:\t0.058336890660799466\n",
      " Validation loss:\t0.060749558302072376\n",
      "R2:  0.9389715289595657\n",
      "#### Epoch: 209 of 1000 ####\n",
      " Training loss:\t0.05217279460615311\n",
      " Test loss:\t0.05833687690588144\n",
      " Validation loss:\t0.060749526207263656\n",
      "R2:  0.9389715490692463\n",
      "#### Epoch: 210 of 1000 ####\n",
      " Training loss:\t0.05217274867361529\n",
      " Test loss:\t0.058336853981018066\n",
      " Validation loss:\t0.06074950786737295\n",
      "R2:  0.938971570872979\n",
      "#### Epoch: 211 of 1000 ####\n",
      " Training loss:\t0.05217268436806235\n",
      " Test loss:\t0.058336844811072715\n",
      " Validation loss:\t0.060749480357536904\n",
      "R2:  0.9389715891942274\n",
      "#### Epoch: 212 of 1000 ####\n",
      " Training loss:\t0.052172615469255626\n",
      " Test loss:\t0.058336808131291315\n",
      " Validation loss:\t0.06074945284770085\n",
      "R2:  0.9389716170747181\n",
      "#### Epoch: 213 of 1000 ####\n",
      " Training loss:\t0.052172537383941335\n",
      " Test loss:\t0.05833678979140062\n",
      " Validation loss:\t0.06074942992283748\n",
      "R2:  0.9389716424001323\n",
      "#### Epoch: 214 of 1000 ####\n",
      " Training loss:\t0.05217248226489596\n",
      " Test loss:\t0.058336762281564564\n",
      " Validation loss:\t0.060749402413001426\n",
      "R2:  0.938971664083447\n",
      "#### Epoch: 215 of 1000 ####\n",
      " Training loss:\t0.05217241795934301\n",
      " Test loss:\t0.058336734771728516\n",
      " Validation loss:\t0.060749356563274674\n",
      "R2:  0.9389716907229316\n",
      "#### Epoch: 216 of 1000 ####\n",
      " Training loss:\t0.05217233068752117\n",
      " Test loss:\t0.05833671643183781\n",
      " Validation loss:\t0.06074934280835665\n",
      "R2:  0.9389717165274217\n",
      "#### Epoch: 217 of 1000 ####\n",
      " Training loss:\t0.052172234229191754\n",
      " Test loss:\t0.058336693506974444\n",
      " Validation loss:\t0.060749319883493275\n",
      "R2:  0.9389717425040226\n",
      "#### Epoch: 218 of 1000 ####\n",
      " Training loss:\t0.05217216073713125\n",
      " Test loss:\t0.05833664765724769\n",
      " Validation loss:\t0.060749283203711875\n",
      "R2:  0.9389717718430126\n",
      "#### Epoch: 219 of 1000 ####\n",
      " Training loss:\t0.05217211939784721\n",
      " Test loss:\t0.058336624732384317\n",
      " Validation loss:\t0.060749251108903154\n",
      "R2:  0.9389717994741855\n",
      "#### Epoch: 220 of 1000 ####\n",
      " Training loss:\t0.0521720459057867\n",
      " Test loss:\t0.05833660639249361\n",
      " Validation loss:\t0.0607492235990671\n",
      "R2:  0.9389718284018188\n",
      "#### Epoch: 221 of 1000 ####\n",
      " Training loss:\t0.05217199537999511\n",
      " Test loss:\t0.058336578882657565\n",
      " Validation loss:\t0.06074919150425838\n",
      "R2:  0.9389718536636527\n",
      "#### Epoch: 222 of 1000 ####\n",
      " Training loss:\t0.05217194485420351\n",
      " Test loss:\t0.05833654678784884\n",
      " Validation loss:\t0.0607491869192857\n",
      "R2:  0.9389718795647594\n",
      "#### Epoch: 223 of 1000 ####\n",
      " Training loss:\t0.052171889735158135\n",
      " Test loss:\t0.05833652844795814\n",
      " Validation loss:\t0.06074915940944965\n",
      "R2:  0.9389719052747137\n",
      "#### Epoch: 224 of 1000 ####\n",
      " Training loss:\t0.05217184380262031\n",
      " Test loss:\t0.058336496353149414\n",
      " Validation loss:\t0.060749131899613604\n",
      "R2:  0.9389719278043541\n",
      "#### Epoch: 225 of 1000 ####\n",
      " Training loss:\t0.05217179327682872\n",
      " Test loss:\t0.05833647342828604\n",
      " Validation loss:\t0.06074910438977755\n",
      "R2:  0.9389719502907122\n",
      "#### Epoch: 226 of 1000 ####\n",
      " Training loss:\t0.05217176571730603\n",
      " Test loss:\t0.05833645508839534\n",
      " Validation loss:\t0.0607490768799415\n",
      "R2:  0.9389719719730671\n",
      "#### Epoch: 227 of 1000 ####\n",
      " Training loss:\t0.052171719784768214\n",
      " Test loss:\t0.05833643674850464\n",
      " Validation loss:\t0.06074906312502348\n",
      "R2:  0.9389719883248961\n",
      "#### Epoch: 228 of 1000 ####\n",
      " Training loss:\t0.0521716738522304\n",
      " Test loss:\t0.05833641382364126\n",
      " Validation loss:\t0.06074903103021475\n",
      "R2:  0.9389720102362459\n",
      "#### Epoch: 229 of 1000 ####\n",
      " Training loss:\t0.05217161413993124\n",
      " Test loss:\t0.05833640006872324\n",
      " Validation loss:\t0.06074898976546068\n",
      "R2:  0.9389720250198474\n",
      "#### Epoch: 230 of 1000 ####\n",
      " Training loss:\t0.0521715728006472\n",
      " Test loss:\t0.05833639089877789\n",
      " Validation loss:\t0.06074895767065195\n",
      "R2:  0.9389720429238206\n",
      "#### Epoch: 231 of 1000 ####\n",
      " Training loss:\t0.05217154064787073\n",
      " Test loss:\t0.05833637255888719\n",
      " Validation loss:\t0.060748943915733926\n",
      "R2:  0.9389720609802245\n",
      "#### Epoch: 232 of 1000 ####\n",
      " Training loss:\t0.052171471749064\n",
      " Test loss:\t0.05833635421899649\n",
      " Validation loss:\t0.06074892099087055\n",
      "R2:  0.9389720771301746\n",
      "#### Epoch: 233 of 1000 ####\n",
      " Training loss:\t0.052171416630018626\n",
      " Test loss:\t0.05833632670916044\n",
      " Validation loss:\t0.060748907235952526\n",
      "R2:  0.938972096381744\n",
      "#### Epoch: 234 of 1000 ####\n",
      " Training loss:\t0.05217136610422703\n",
      " Test loss:\t0.058336299199324385\n",
      " Validation loss:\t0.060748898066007175\n",
      "R2:  0.9389721143082028\n",
      "#### Epoch: 235 of 1000 ####\n",
      " Training loss:\t0.0521713477312119\n",
      " Test loss:\t0.058336299199324385\n",
      " Validation loss:\t0.06074887972611647\n",
      "R2:  0.9389721320083619\n",
      "#### Epoch: 236 of 1000 ####\n",
      " Training loss:\t0.05217129261216652\n",
      " Test loss:\t0.05833629002937904\n",
      " Validation loss:\t0.060748861386225775\n",
      "R2:  0.9389721507244407\n",
      "#### Epoch: 237 of 1000 ####\n",
      " Training loss:\t0.05217126045939005\n",
      " Test loss:\t0.058336276274461016\n",
      " Validation loss:\t0.06074885221628042\n",
      "R2:  0.9389721645207505\n",
      "#### Epoch: 238 of 1000 ####\n",
      " Training loss:\t0.05217122830661358\n",
      " Test loss:\t0.05833624417965229\n",
      " Validation loss:\t0.06074884304633507\n",
      "R2:  0.9389721797404484\n",
      "#### Epoch: 239 of 1000 ####\n",
      " Training loss:\t0.052171196153837106\n",
      " Test loss:\t0.05833623500970694\n",
      " Validation loss:\t0.06074881553649902\n",
      "R2:  0.9389721931738231\n",
      "#### Epoch: 240 of 1000 ####\n",
      " Training loss:\t0.05217112725503038\n",
      " Test loss:\t0.05833621666981624\n",
      " Validation loss:\t0.06074881553649902\n",
      "R2:  0.9389722075767574\n",
      "#### Epoch: 241 of 1000 ####\n",
      " Training loss:\t0.05217110888201526\n",
      " Test loss:\t0.05833620291489821\n",
      " Validation loss:\t0.06074879719660832\n",
      "R2:  0.938972221691068\n",
      "#### Epoch: 242 of 1000 ####\n",
      " Training loss:\t0.05217109969550769\n",
      " Test loss:\t0.05833618457500751\n",
      " Validation loss:\t0.060748778856717624\n",
      "R2:  0.9389722314068123\n",
      "#### Epoch: 243 of 1000 ####\n",
      " Training loss:\t0.05217107213598501\n",
      " Test loss:\t0.05833617082008949\n",
      " Validation loss:\t0.06074877427174495\n",
      "R2:  0.9389722417126726\n",
      "#### Epoch: 244 of 1000 ####\n",
      " Training loss:\t0.05217103998320853\n",
      " Test loss:\t0.05833616165014414\n",
      " Validation loss:\t0.060748742176936224\n",
      "R2:  0.93897225052967\n",
      "#### Epoch: 245 of 1000 ####\n",
      " Training loss:\t0.05217102161019341\n",
      " Test loss:\t0.058336157065171465\n",
      " Validation loss:\t0.06074871925207285\n",
      "R2:  0.9389722600628073\n",
      "#### Epoch: 246 of 1000 ####\n",
      " Training loss:\t0.052171003237178284\n",
      " Test loss:\t0.058336152480198786\n",
      " Validation loss:\t0.0607487100821275\n",
      "R2:  0.9389722695302836\n",
      "#### Epoch: 247 of 1000 ####\n",
      " Training loss:\t0.05217096189789425\n",
      " Test loss:\t0.05833614789522611\n",
      " Validation loss:\t0.060748700912182145\n",
      "R2:  0.9389722788398722\n",
      "#### Epoch: 248 of 1000 ####\n",
      " Training loss:\t0.0521709481181329\n",
      " Test loss:\t0.05833613872528076\n",
      " Validation loss:\t0.06074868715726412\n",
      "R2:  0.9389722889706124\n",
      "#### Epoch: 249 of 1000 ####\n",
      " Training loss:\t0.052170934338371555\n",
      " Test loss:\t0.05833613414030809\n",
      " Validation loss:\t0.06074868715726412\n",
      "R2:  0.9389722965902353\n",
      "#### Epoch: 250 of 1000 ####\n",
      " Training loss:\t0.05217092974511778\n",
      " Test loss:\t0.05833612497036274\n",
      " Validation loss:\t0.0607486734023461\n",
      "R2:  0.9389723032827271\n",
      "#### Epoch: 251 of 1000 ####\n",
      " Training loss:\t0.0521708975923413\n",
      " Test loss:\t0.058336111215444714\n",
      " Validation loss:\t0.060748664232400745\n",
      "R2:  0.9389723131664245\n",
      "#### Epoch: 252 of 1000 ####\n",
      " Training loss:\t0.052170892999087524\n",
      " Test loss:\t0.058336111215444714\n",
      " Validation loss:\t0.06074865047748272\n",
      "R2:  0.938972317159312\n",
      "#### Epoch: 253 of 1000 ####\n",
      " Training loss:\t0.05217088840583374\n",
      " Test loss:\t0.05833610204549936\n",
      " Validation loss:\t0.06074865047748272\n",
      "R2:  0.9389723228480558\n",
      "#### Epoch: 254 of 1000 ####\n",
      " Training loss:\t0.05217085625305727\n",
      " Test loss:\t0.05833608370560866\n",
      " Validation loss:\t0.06074864589251005\n",
      "R2:  0.9389723288221093\n",
      "#### Epoch: 255 of 1000 ####\n",
      " Training loss:\t0.05217084706654971\n",
      " Test loss:\t0.058336074535663314\n",
      " Validation loss:\t0.060748632137592025\n",
      "R2:  0.9389723339472099\n",
      "#### Epoch: 256 of 1000 ####\n",
      " Training loss:\t0.05217083788004214\n",
      " Test loss:\t0.058336069950690635\n",
      " Validation loss:\t0.060748618382673994\n",
      "R2:  0.9389723383100302\n",
      "#### Epoch: 257 of 1000 ####\n",
      " Training loss:\t0.05217081950702702\n",
      " Test loss:\t0.05833606536571796\n",
      " Validation loss:\t0.06074862296764667\n",
      "R2:  0.9389723422166264\n",
      "#### Epoch: 258 of 1000 ####\n",
      " Training loss:\t0.05217080572726567\n",
      " Test loss:\t0.05833606078074528\n",
      " Validation loss:\t0.060748632137592025\n",
      "R2:  0.9389723459920343\n",
      "#### Epoch: 259 of 1000 ####\n",
      " Training loss:\t0.05217077816774298\n",
      " Test loss:\t0.05833605161079994\n",
      " Validation loss:\t0.060748627552619346\n",
      "R2:  0.9389723492402448\n",
      "#### Epoch: 260 of 1000 ####\n",
      " Training loss:\t0.052170791947504326\n",
      " Test loss:\t0.05833605161079994\n",
      " Validation loss:\t0.060748627552619346\n",
      "R2:  0.9389723520802391\n",
      "#### Epoch: 261 of 1000 ####\n",
      " Training loss:\t0.05217078735425055\n",
      " Test loss:\t0.05833604702582726\n",
      " Validation loss:\t0.060748627552619346\n",
      "R2:  0.9389723550486786\n",
      "#### Epoch: 262 of 1000 ####\n",
      " Training loss:\t0.05217077816774298\n",
      " Test loss:\t0.05833604702582726\n",
      " Validation loss:\t0.06074862296764667\n",
      "R2:  0.9389723576766082\n",
      "#### Epoch: 263 of 1000 ####\n",
      " Training loss:\t0.0521707735744892\n",
      " Test loss:\t0.05833604702582726\n",
      " Validation loss:\t0.06074862296764667\n",
      "R2:  0.9389723601469897\n",
      "#### Epoch: 264 of 1000 ####\n",
      " Training loss:\t0.05217075520147407\n",
      " Test loss:\t0.05833605161079994\n",
      " Validation loss:\t0.06074861379770132\n",
      "R2:  0.938972362677188\n",
      "#### Epoch: 265 of 1000 ####\n",
      " Training loss:\t0.052170750608220295\n",
      " Test loss:\t0.05833604702582726\n",
      " Validation loss:\t0.06074861379770132\n",
      "R2:  0.9389723648313605\n",
      "#### Epoch: 266 of 1000 ####\n",
      " Training loss:\t0.052170750608220295\n",
      " Test loss:\t0.058336037855881914\n",
      " Validation loss:\t0.06074861379770132\n",
      "R2:  0.9389723663396055\n",
      "#### Epoch: 267 of 1000 ####\n",
      " Training loss:\t0.052170732235205165\n",
      " Test loss:\t0.058336037855881914\n",
      " Validation loss:\t0.06074861379770132\n",
      "R2:  0.9389723680181821\n",
      "#### Epoch: 268 of 1000 ####\n",
      " Training loss:\t0.05217073682845895\n",
      " Test loss:\t0.058336033270909235\n",
      " Validation loss:\t0.06074861379770132\n",
      "R2:  0.938972369535385\n",
      "#### Epoch: 269 of 1000 ####\n",
      " Training loss:\t0.05217071845544382\n",
      " Test loss:\t0.058336033270909235\n",
      " Validation loss:\t0.06074861379770132\n",
      "R2:  0.9389723706734519\n",
      "#### Epoch: 270 of 1000 ####\n",
      " Training loss:\t0.052170723048697604\n",
      " Test loss:\t0.05833602868593656\n",
      " Validation loss:\t0.06074861379770132\n",
      "R2:  0.9389723721885562\n",
      "#### Epoch: 271 of 1000 ####\n",
      " Training loss:\t0.052170723048697604\n",
      " Test loss:\t0.05833602868593656\n",
      " Validation loss:\t0.06074861379770132\n",
      "R2:  0.938972373262552\n",
      "#### Epoch: 272 of 1000 ####\n",
      " Training loss:\t0.05217071386219004\n",
      " Test loss:\t0.05833602868593656\n",
      " Validation loss:\t0.06074861379770132\n",
      "R2:  0.9389723741213415\n",
      "#### Epoch: 273 of 1000 ####\n",
      " Training loss:\t0.05217070926893626\n",
      " Test loss:\t0.05833602868593656\n",
      " Validation loss:\t0.060748618382673994\n",
      "R2:  0.9389723753643656\n",
      "#### Epoch: 274 of 1000 ####\n",
      " Training loss:\t0.052170704675682474\n",
      " Test loss:\t0.05833602410096388\n",
      " Validation loss:\t0.060748618382673994\n",
      "R2:  0.9389723761560107\n",
      "#### Epoch: 275 of 1000 ####\n",
      " Training loss:\t0.05217070926893626\n",
      " Test loss:\t0.05833602410096388\n",
      " Validation loss:\t0.06074861379770132\n",
      "R2:  0.9389723768653874\n",
      "#### Epoch: 276 of 1000 ####\n",
      " Training loss:\t0.05217070926893626\n",
      " Test loss:\t0.05833602868593656\n",
      " Validation loss:\t0.06074860462775597\n",
      "R2:  0.9389723776868101\n",
      "#### Epoch: 277 of 1000 ####\n",
      " Training loss:\t0.05217070926893626\n",
      " Test loss:\t0.05833602868593656\n",
      " Validation loss:\t0.06074860462775597\n",
      "R2:  0.93897237839294\n",
      "#### Epoch: 278 of 1000 ####\n",
      " Training loss:\t0.052170700082428696\n",
      " Test loss:\t0.05833602868593656\n",
      " Validation loss:\t0.06074860462775597\n",
      "R2:  0.938972378846143\n",
      "#### Epoch: 279 of 1000 ####\n",
      " Training loss:\t0.05217071386219004\n",
      " Test loss:\t0.05833602868593656\n",
      " Validation loss:\t0.060748595457810625\n",
      "R2:  0.9389723794079653\n",
      "#### Epoch: 280 of 1000 ####\n",
      " Training loss:\t0.05217070926893626\n",
      " Test loss:\t0.05833602868593656\n",
      " Validation loss:\t0.06074860462775597\n",
      "R2:  0.9389723800607184\n",
      "#### Epoch: 281 of 1000 ####\n",
      " Training loss:\t0.05217070926893626\n",
      " Test loss:\t0.05833602868593656\n",
      " Validation loss:\t0.060748595457810625\n",
      "R2:  0.9389723804693931\n",
      "#### Epoch: 282 of 1000 ####\n",
      " Training loss:\t0.05217068630266735\n",
      " Test loss:\t0.05833602868593656\n",
      " Validation loss:\t0.0607486000427833\n",
      "R2:  0.9389723807942952\n",
      "#### Epoch: 283 of 1000 ####\n",
      " Training loss:\t0.05217069548917491\n",
      " Test loss:\t0.05833602410096388\n",
      " Validation loss:\t0.060748595457810625\n",
      "R2:  0.9389723811051078\n",
      "#### Epoch: 284 of 1000 ####\n",
      " Training loss:\t0.052170700082428696\n",
      " Test loss:\t0.05833602410096388\n",
      " Validation loss:\t0.060748595457810625\n",
      "R2:  0.938972381645397\n",
      "#### Epoch: 285 of 1000 ####\n",
      " Training loss:\t0.05217068630266735\n",
      " Test loss:\t0.05833602868593656\n",
      " Validation loss:\t0.060748595457810625\n",
      "R2:  0.9389723821432528\n",
      "#### Epoch: 286 of 1000 ####\n",
      " Training loss:\t0.052170690895921135\n",
      " Test loss:\t0.05833602410096388\n",
      " Validation loss:\t0.060748595457810625\n",
      "R2:  0.9389723823577184\n",
      "#### Epoch: 287 of 1000 ####\n",
      " Training loss:\t0.052170704675682474\n",
      " Test loss:\t0.05833602868593656\n",
      " Validation loss:\t0.060748590872837946\n",
      "R2:  0.9389723826155708\n",
      "#### Epoch: 288 of 1000 ####\n",
      " Training loss:\t0.05217068630266735\n",
      " Test loss:\t0.05833602410096388\n",
      " Validation loss:\t0.060748590872837946\n",
      "R2:  0.9389723826807541\n",
      "#### Epoch: 289 of 1000 ####\n",
      " Training loss:\t0.052170681709413566\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748590872837946\n",
      "R2:  0.938972382997811\n",
      "#### Epoch: 290 of 1000 ####\n",
      " Training loss:\t0.05217068630266735\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748590872837946\n",
      "R2:  0.9389723830145399\n",
      "#### Epoch: 291 of 1000 ####\n",
      " Training loss:\t0.05217068630266735\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748590872837946\n",
      "R2:  0.938972383165009\n",
      "#### Epoch: 292 of 1000 ####\n",
      " Training loss:\t0.05217069548917491\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748586287865274\n",
      "R2:  0.9389723831886111\n",
      "#### Epoch: 293 of 1000 ####\n",
      " Training loss:\t0.05217068630266735\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748590872837946\n",
      "R2:  0.9389723832542024\n",
      "#### Epoch: 294 of 1000 ####\n",
      " Training loss:\t0.05217068630266735\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748590872837946\n",
      "R2:  0.9389723832892957\n",
      "#### Epoch: 295 of 1000 ####\n",
      " Training loss:\t0.05217069548917491\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748590872837946\n",
      "R2:  0.9389723833866681\n",
      "#### Epoch: 296 of 1000 ####\n",
      " Training loss:\t0.05217067711615979\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748590872837946\n",
      "R2:  0.9389723832711507\n",
      "#### Epoch: 297 of 1000 ####\n",
      " Training loss:\t0.052170690895921135\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748586287865274\n",
      "R2:  0.9389723831992719\n",
      "#### Epoch: 298 of 1000 ####\n",
      " Training loss:\t0.05217069548917491\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748586287865274\n",
      "R2:  0.9389723831516723\n",
      "#### Epoch: 299 of 1000 ####\n",
      " Training loss:\t0.052170700082428696\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748586287865274\n",
      "R2:  0.9389723831711829\n",
      "#### Epoch: 300 of 1000 ####\n",
      " Training loss:\t0.05217068630266735\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748586287865274\n",
      "R2:  0.9389723831408436\n",
      "#### Epoch: 301 of 1000 ####\n",
      " Training loss:\t0.052170672522906004\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748590872837946\n",
      "R2:  0.9389723831242099\n",
      "#### Epoch: 302 of 1000 ####\n",
      " Training loss:\t0.052170672522906004\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748586287865274\n",
      "R2:  0.9389723830163194\n",
      "#### Epoch: 303 of 1000 ####\n",
      " Training loss:\t0.05217068630266735\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748586287865274\n",
      "R2:  0.9389723829607088\n",
      "#### Epoch: 304 of 1000 ####\n",
      " Training loss:\t0.05217068630266735\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830073158\n",
      "#### Epoch: 305 of 1000 ####\n",
      " Training loss:\t0.052170681709413566\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748586287865274\n",
      "R2:  0.9389723830176504\n",
      "#### Epoch: 306 of 1000 ####\n",
      " Training loss:\t0.052170681709413566\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830068609\n",
      "#### Epoch: 307 of 1000 ####\n",
      " Training loss:\t0.05217067711615979\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748586287865274\n",
      "R2:  0.9389723830744335\n",
      "#### Epoch: 308 of 1000 ####\n",
      " Training loss:\t0.052170690895921135\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830444591\n",
      "#### Epoch: 309 of 1000 ####\n",
      " Training loss:\t0.052170672522906004\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830191892\n",
      "#### Epoch: 310 of 1000 ####\n",
      " Training loss:\t0.052170681709413566\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723828968399\n",
      "#### Epoch: 311 of 1000 ####\n",
      " Training loss:\t0.05217069548917491\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748586287865274\n",
      "R2:  0.9389723830419338\n",
      "#### Epoch: 312 of 1000 ####\n",
      " Training loss:\t0.05217068630266735\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830420696\n",
      "#### Epoch: 313 of 1000 ####\n",
      " Training loss:\t0.05217067711615979\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.938972383035191\n",
      "#### Epoch: 314 of 1000 ####\n",
      " Training loss:\t0.052170681709413566\n",
      " Test loss:\t0.05833601493101854\n",
      " Validation loss:\t0.060748586287865274\n",
      "R2:  0.9389723830889941\n",
      "#### Epoch: 315 of 1000 ####\n",
      " Training loss:\t0.052170681709413566\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748586287865274\n",
      "R2:  0.938972383006863\n",
      "#### Epoch: 316 of 1000 ####\n",
      " Training loss:\t0.052170672522906004\n",
      " Test loss:\t0.05833601493101854\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830706664\n",
      "#### Epoch: 317 of 1000 ####\n",
      " Training loss:\t0.05217068630266735\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723831189593\n",
      "#### Epoch: 318 of 1000 ####\n",
      " Training loss:\t0.052170690895921135\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723831956983\n",
      "#### Epoch: 319 of 1000 ####\n",
      " Training loss:\t0.05217068630266735\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723832147883\n",
      "#### Epoch: 320 of 1000 ####\n",
      " Training loss:\t0.05217068630266735\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.93897238320761\n",
      "#### Epoch: 321 of 1000 ####\n",
      " Training loss:\t0.052170690895921135\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723832204613\n",
      "#### Epoch: 322 of 1000 ####\n",
      " Training loss:\t0.05217068630266735\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723832227576\n",
      "#### Epoch: 323 of 1000 ####\n",
      " Training loss:\t0.052170681709413566\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723832112805\n",
      "#### Epoch: 324 of 1000 ####\n",
      " Training loss:\t0.052170681709413566\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723831943702\n",
      "#### Epoch: 325 of 1000 ####\n",
      " Training loss:\t0.05217068630266735\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.938972383182423\n",
      "#### Epoch: 326 of 1000 ####\n",
      " Training loss:\t0.052170681709413566\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748586287865274\n",
      "R2:  0.9389723831567444\n",
      "#### Epoch: 327 of 1000 ####\n",
      " Training loss:\t0.052170690895921135\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723831038899\n",
      "#### Epoch: 328 of 1000 ####\n",
      " Training loss:\t0.05217069548917491\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723831186032\n",
      "#### Epoch: 329 of 1000 ####\n",
      " Training loss:\t0.05217069548917491\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723831186962\n",
      "#### Epoch: 330 of 1000 ####\n",
      " Training loss:\t0.05217068630266735\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723831269078\n",
      "#### Epoch: 331 of 1000 ####\n",
      " Training loss:\t0.05217067711615979\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723831350614\n",
      "#### Epoch: 332 of 1000 ####\n",
      " Training loss:\t0.052170681709413566\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830841856\n",
      "#### Epoch: 333 of 1000 ####\n",
      " Training loss:\t0.052170681709413566\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830828657\n",
      "#### Epoch: 334 of 1000 ####\n",
      " Training loss:\t0.052170681709413566\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830487845\n",
      "#### Epoch: 335 of 1000 ####\n",
      " Training loss:\t0.05217068630266735\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830618837\n",
      "#### Epoch: 336 of 1000 ####\n",
      " Training loss:\t0.05217067711615979\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830552453\n",
      "#### Epoch: 337 of 1000 ####\n",
      " Training loss:\t0.052170690895921135\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830080624\n",
      "#### Epoch: 338 of 1000 ####\n",
      " Training loss:\t0.052170681709413566\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830236497\n",
      "#### Epoch: 339 of 1000 ####\n",
      " Training loss:\t0.052170690895921135\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830104957\n",
      "#### Epoch: 340 of 1000 ####\n",
      " Training loss:\t0.05217068630266735\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830223988\n",
      "#### Epoch: 341 of 1000 ####\n",
      " Training loss:\t0.05217068630266735\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830139439\n",
      "#### Epoch: 342 of 1000 ####\n",
      " Training loss:\t0.05217068630266735\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830285585\n",
      "#### Epoch: 343 of 1000 ####\n",
      " Training loss:\t0.052170672522906004\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830314203\n",
      "#### Epoch: 344 of 1000 ####\n",
      " Training loss:\t0.052170690895921135\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830480796\n",
      "#### Epoch: 345 of 1000 ####\n",
      " Training loss:\t0.05217069548917491\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.938972383071222\n",
      "#### Epoch: 346 of 1000 ####\n",
      " Training loss:\t0.05217066792965223\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830572432\n",
      "#### Epoch: 347 of 1000 ####\n",
      " Training loss:\t0.05217066792965223\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830566872\n",
      "#### Epoch: 348 of 1000 ####\n",
      " Training loss:\t0.052170672522906004\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.93897238304786\n",
      "#### Epoch: 349 of 1000 ####\n",
      " Training loss:\t0.05217067711615979\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830476205\n",
      "#### Epoch: 350 of 1000 ####\n",
      " Training loss:\t0.052170681709413566\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830445317\n",
      "#### Epoch: 351 of 1000 ####\n",
      " Training loss:\t0.052170681709413566\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830530302\n",
      "#### Epoch: 352 of 1000 ####\n",
      " Training loss:\t0.05217068630266735\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830561445\n",
      "#### Epoch: 353 of 1000 ####\n",
      " Training loss:\t0.05217068630266735\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830559205\n",
      "#### Epoch: 354 of 1000 ####\n",
      " Training loss:\t0.052170681709413566\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830551939\n",
      "#### Epoch: 355 of 1000 ####\n",
      " Training loss:\t0.05217067711615979\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.938972383060848\n",
      "#### Epoch: 356 of 1000 ####\n",
      " Training loss:\t0.052170690895921135\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830703247\n",
      "#### Epoch: 357 of 1000 ####\n",
      " Training loss:\t0.05217068630266735\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830682273\n",
      "#### Epoch: 358 of 1000 ####\n",
      " Training loss:\t0.05217067711615979\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830728862\n",
      "#### Epoch: 359 of 1000 ####\n",
      " Training loss:\t0.052170672522906004\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830818787\n",
      "#### Epoch: 360 of 1000 ####\n",
      " Training loss:\t0.05217067711615979\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830908528\n",
      "#### Epoch: 361 of 1000 ####\n",
      " Training loss:\t0.05217068630266735\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748586287865274\n",
      "R2:  0.938972383090281\n",
      "#### Epoch: 362 of 1000 ####\n",
      " Training loss:\t0.05217067711615979\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748586287865274\n",
      "R2:  0.9389723830872987\n",
      "#### Epoch: 363 of 1000 ####\n",
      " Training loss:\t0.05217068630266735\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748586287865274\n",
      "R2:  0.9389723830715984\n",
      "#### Epoch: 364 of 1000 ####\n",
      " Training loss:\t0.05217067711615979\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748586287865274\n",
      "R2:  0.9389723830655463\n",
      "#### Epoch: 365 of 1000 ####\n",
      " Training loss:\t0.052170681709413566\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748586287865274\n",
      "R2:  0.9389723830706346\n",
      "#### Epoch: 366 of 1000 ####\n",
      " Training loss:\t0.052170690895921135\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748586287865274\n",
      "R2:  0.9389723830674245\n",
      "#### Epoch: 367 of 1000 ####\n",
      " Training loss:\t0.052170681709413566\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748586287865274\n",
      "R2:  0.9389723830699019\n",
      "#### Epoch: 368 of 1000 ####\n",
      " Training loss:\t0.052170681709413566\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830676697\n",
      "#### Epoch: 369 of 1000 ####\n",
      " Training loss:\t0.05217068630266735\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830714185\n",
      "#### Epoch: 370 of 1000 ####\n",
      " Training loss:\t0.05217067711615979\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830667681\n",
      "#### Epoch: 371 of 1000 ####\n",
      " Training loss:\t0.052170681709413566\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830646608\n",
      "#### Epoch: 372 of 1000 ####\n",
      " Training loss:\t0.05217067711615979\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830666856\n",
      "#### Epoch: 373 of 1000 ####\n",
      " Training loss:\t0.05217066792965223\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830613109\n",
      "#### Epoch: 374 of 1000 ####\n",
      " Training loss:\t0.05217069548917491\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830643459\n",
      "#### Epoch: 375 of 1000 ####\n",
      " Training loss:\t0.052170681709413566\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830611459\n",
      "#### Epoch: 376 of 1000 ####\n",
      " Training loss:\t0.052170672522906004\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.938972383071371\n",
      "#### Epoch: 377 of 1000 ####\n",
      " Training loss:\t0.05217068630266735\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830734888\n",
      "#### Epoch: 378 of 1000 ####\n",
      " Training loss:\t0.05217068630266735\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830710199\n",
      "#### Epoch: 379 of 1000 ####\n",
      " Training loss:\t0.052170681709413566\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830696397\n",
      "#### Epoch: 380 of 1000 ####\n",
      " Training loss:\t0.05217067711615979\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830667193\n",
      "#### Epoch: 381 of 1000 ####\n",
      " Training loss:\t0.052170672522906004\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.938972383062812\n",
      "#### Epoch: 382 of 1000 ####\n",
      " Training loss:\t0.052170672522906004\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830587483\n",
      "#### Epoch: 383 of 1000 ####\n",
      " Training loss:\t0.052170672522906004\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830597\n",
      "#### Epoch: 384 of 1000 ####\n",
      " Training loss:\t0.05217068630266735\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.938972383061401\n",
      "#### Epoch: 385 of 1000 ####\n",
      " Training loss:\t0.05217069548917491\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830620896\n",
      "#### Epoch: 386 of 1000 ####\n",
      " Training loss:\t0.052170681709413566\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830635882\n",
      "#### Epoch: 387 of 1000 ####\n",
      " Training loss:\t0.052170681709413566\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830614758\n",
      "#### Epoch: 388 of 1000 ####\n",
      " Training loss:\t0.052170690895921135\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830626916\n",
      "#### Epoch: 389 of 1000 ####\n",
      " Training loss:\t0.052170690895921135\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830629818\n",
      "#### Epoch: 390 of 1000 ####\n",
      " Training loss:\t0.05217067711615979\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.938972383061943\n",
      "#### Epoch: 391 of 1000 ####\n",
      " Training loss:\t0.052170681709413566\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830611307\n",
      "#### Epoch: 392 of 1000 ####\n",
      " Training loss:\t0.052170681709413566\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830604433\n",
      "#### Epoch: 393 of 1000 ####\n",
      " Training loss:\t0.05217067711615979\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830611522\n",
      "#### Epoch: 394 of 1000 ####\n",
      " Training loss:\t0.052170681709413566\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830622848\n",
      "#### Epoch: 395 of 1000 ####\n",
      " Training loss:\t0.052170672522906004\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.938972383064417\n",
      "#### Epoch: 396 of 1000 ####\n",
      " Training loss:\t0.052170681709413566\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830648671\n",
      "#### Epoch: 397 of 1000 ####\n",
      " Training loss:\t0.052170681709413566\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830651169\n",
      "#### Epoch: 398 of 1000 ####\n",
      " Training loss:\t0.05217069548917491\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830651763\n",
      "#### Epoch: 399 of 1000 ####\n",
      " Training loss:\t0.05217069548917491\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830650631\n",
      "#### Epoch: 400 of 1000 ####\n",
      " Training loss:\t0.05217067711615979\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830651946\n",
      "#### Epoch: 401 of 1000 ####\n",
      " Training loss:\t0.052170690895921135\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830658754\n",
      "#### Epoch: 402 of 1000 ####\n",
      " Training loss:\t0.052170672522906004\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656748\n",
      "#### Epoch: 403 of 1000 ####\n",
      " Training loss:\t0.052170681709413566\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830657156\n",
      "#### Epoch: 404 of 1000 ####\n",
      " Training loss:\t0.052170690895921135\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.938972383065072\n",
      "#### Epoch: 405 of 1000 ####\n",
      " Training loss:\t0.05217067711615979\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830647088\n",
      "#### Epoch: 406 of 1000 ####\n",
      " Training loss:\t0.052170681709413566\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830644852\n",
      "#### Epoch: 407 of 1000 ####\n",
      " Training loss:\t0.052170700082428696\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830642258\n",
      "#### Epoch: 408 of 1000 ####\n",
      " Training loss:\t0.05217068630266735\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830644056\n",
      "#### Epoch: 409 of 1000 ####\n",
      " Training loss:\t0.05217068630266735\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830643942\n",
      "#### Epoch: 410 of 1000 ####\n",
      " Training loss:\t0.052170672522906004\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.938972383064156\n",
      "#### Epoch: 411 of 1000 ####\n",
      " Training loss:\t0.052170681709413566\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830642502\n",
      "#### Epoch: 412 of 1000 ####\n",
      " Training loss:\t0.05217068630266735\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830647875\n",
      "#### Epoch: 413 of 1000 ####\n",
      " Training loss:\t0.05217068630266735\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.938972383065006\n",
      "#### Epoch: 414 of 1000 ####\n",
      " Training loss:\t0.052170690895921135\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830650725\n",
      "#### Epoch: 415 of 1000 ####\n",
      " Training loss:\t0.05217069548917491\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.938972383065011\n",
      "#### Epoch: 416 of 1000 ####\n",
      " Training loss:\t0.052170672522906004\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830650021\n",
      "#### Epoch: 417 of 1000 ####\n",
      " Training loss:\t0.05217068630266735\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830653207\n",
      "#### Epoch: 418 of 1000 ####\n",
      " Training loss:\t0.05217068630266735\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830651244\n",
      "#### Epoch: 419 of 1000 ####\n",
      " Training loss:\t0.052170681709413566\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830651461\n",
      "#### Epoch: 420 of 1000 ####\n",
      " Training loss:\t0.05217067711615979\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.938972383065148\n",
      "#### Epoch: 421 of 1000 ####\n",
      " Training loss:\t0.052170672522906004\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830651548\n",
      "#### Epoch: 422 of 1000 ####\n",
      " Training loss:\t0.052170690895921135\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830651243\n",
      "#### Epoch: 423 of 1000 ####\n",
      " Training loss:\t0.052170681709413566\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830650804\n",
      "#### Epoch: 424 of 1000 ####\n",
      " Training loss:\t0.05217068630266735\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830650991\n",
      "#### Epoch: 425 of 1000 ####\n",
      " Training loss:\t0.05217068630266735\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.938972383064518\n",
      "#### Epoch: 426 of 1000 ####\n",
      " Training loss:\t0.052170690895921135\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.938972383064518\n",
      "#### Epoch: 427 of 1000 ####\n",
      " Training loss:\t0.05217068630266735\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830645175\n",
      "#### Epoch: 428 of 1000 ####\n",
      " Training loss:\t0.05217067711615979\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830645125\n",
      "#### Epoch: 429 of 1000 ####\n",
      " Training loss:\t0.05217068630266735\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830645125\n",
      "#### Epoch: 430 of 1000 ####\n",
      " Training loss:\t0.05217067711615979\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830646233\n",
      "#### Epoch: 431 of 1000 ####\n",
      " Training loss:\t0.05217067711615979\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830646233\n",
      "#### Epoch: 432 of 1000 ####\n",
      " Training loss:\t0.05217067711615979\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830646195\n",
      "#### Epoch: 433 of 1000 ####\n",
      " Training loss:\t0.052170681709413566\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830646843\n",
      "#### Epoch: 434 of 1000 ####\n",
      " Training loss:\t0.05217068630266735\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830652887\n",
      "#### Epoch: 435 of 1000 ####\n",
      " Training loss:\t0.052170681709413566\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830652887\n",
      "#### Epoch: 436 of 1000 ####\n",
      " Training loss:\t0.05217067711615979\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830652726\n",
      "#### Epoch: 437 of 1000 ####\n",
      " Training loss:\t0.052170700082428696\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830661096\n",
      "#### Epoch: 438 of 1000 ####\n",
      " Training loss:\t0.05217067711615979\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830661096\n",
      "#### Epoch: 439 of 1000 ####\n",
      " Training loss:\t0.05217067711615979\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830658159\n",
      "#### Epoch: 440 of 1000 ####\n",
      " Training loss:\t0.052170681709413566\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830658571\n",
      "#### Epoch: 441 of 1000 ####\n",
      " Training loss:\t0.05217068630266735\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830658571\n",
      "#### Epoch: 442 of 1000 ####\n",
      " Training loss:\t0.05217068630266735\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830658571\n",
      "#### Epoch: 443 of 1000 ####\n",
      " Training loss:\t0.052170681709413566\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830658571\n",
      "#### Epoch: 444 of 1000 ####\n",
      " Training loss:\t0.05217068630266735\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 445 of 1000 ####\n",
      " Training loss:\t0.05217067711615979\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 446 of 1000 ####\n",
      " Training loss:\t0.052170681709413566\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 447 of 1000 ####\n",
      " Training loss:\t0.05217069548917491\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 448 of 1000 ####\n",
      " Training loss:\t0.052170690895921135\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 449 of 1000 ####\n",
      " Training loss:\t0.05217068630266735\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 450 of 1000 ####\n",
      " Training loss:\t0.05217069548917491\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 451 of 1000 ####\n",
      " Training loss:\t0.052170681709413566\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 452 of 1000 ####\n",
      " Training loss:\t0.052170690895921135\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 453 of 1000 ####\n",
      " Training loss:\t0.052170681709413566\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 454 of 1000 ####\n",
      " Training loss:\t0.05217068630266735\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 455 of 1000 ####\n",
      " Training loss:\t0.052170681709413566\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 456 of 1000 ####\n",
      " Training loss:\t0.05217067711615979\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 457 of 1000 ####\n",
      " Training loss:\t0.052170681709413566\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 458 of 1000 ####\n",
      " Training loss:\t0.05217069548917491\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 459 of 1000 ####\n",
      " Training loss:\t0.05217067711615979\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 460 of 1000 ####\n",
      " Training loss:\t0.052170681709413566\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 461 of 1000 ####\n",
      " Training loss:\t0.052170681709413566\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 462 of 1000 ####\n",
      " Training loss:\t0.052170690895921135\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 463 of 1000 ####\n",
      " Training loss:\t0.052170690895921135\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 464 of 1000 ####\n",
      " Training loss:\t0.052170672522906004\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 465 of 1000 ####\n",
      " Training loss:\t0.05217068630266735\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 466 of 1000 ####\n",
      " Training loss:\t0.05217067711615979\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 467 of 1000 ####\n",
      " Training loss:\t0.052170690895921135\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 468 of 1000 ####\n",
      " Training loss:\t0.052170672522906004\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 469 of 1000 ####\n",
      " Training loss:\t0.05217068630266735\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 470 of 1000 ####\n",
      " Training loss:\t0.05217068630266735\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 471 of 1000 ####\n",
      " Training loss:\t0.052170681709413566\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 472 of 1000 ####\n",
      " Training loss:\t0.052170681709413566\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 473 of 1000 ####\n",
      " Training loss:\t0.052170681709413566\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 474 of 1000 ####\n",
      " Training loss:\t0.05217066792965223\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 475 of 1000 ####\n",
      " Training loss:\t0.052170681709413566\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 476 of 1000 ####\n",
      " Training loss:\t0.052170690895921135\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 477 of 1000 ####\n",
      " Training loss:\t0.052170690895921135\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 478 of 1000 ####\n",
      " Training loss:\t0.052170681709413566\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 479 of 1000 ####\n",
      " Training loss:\t0.05217068630266735\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 480 of 1000 ####\n",
      " Training loss:\t0.052170681709413566\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 481 of 1000 ####\n",
      " Training loss:\t0.052170681709413566\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 482 of 1000 ####\n",
      " Training loss:\t0.052170681709413566\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 483 of 1000 ####\n",
      " Training loss:\t0.05217067711615979\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 484 of 1000 ####\n",
      " Training loss:\t0.05217068630266735\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 485 of 1000 ####\n",
      " Training loss:\t0.05217068630266735\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 486 of 1000 ####\n",
      " Training loss:\t0.05217068630266735\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 487 of 1000 ####\n",
      " Training loss:\t0.052170690895921135\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 488 of 1000 ####\n",
      " Training loss:\t0.05217066792965223\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 489 of 1000 ####\n",
      " Training loss:\t0.05217068630266735\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 490 of 1000 ####\n",
      " Training loss:\t0.05217067711615979\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 491 of 1000 ####\n",
      " Training loss:\t0.052170690895921135\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 492 of 1000 ####\n",
      " Training loss:\t0.052170681709413566\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 493 of 1000 ####\n",
      " Training loss:\t0.052170681709413566\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 494 of 1000 ####\n",
      " Training loss:\t0.052170681709413566\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 495 of 1000 ####\n",
      " Training loss:\t0.052170681709413566\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 496 of 1000 ####\n",
      " Training loss:\t0.05217067711615979\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 497 of 1000 ####\n",
      " Training loss:\t0.052170672522906004\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 498 of 1000 ####\n",
      " Training loss:\t0.052170690895921135\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 499 of 1000 ####\n",
      " Training loss:\t0.052170672522906004\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 500 of 1000 ####\n",
      " Training loss:\t0.052170672522906004\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 501 of 1000 ####\n",
      " Training loss:\t0.052170700082428696\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 502 of 1000 ####\n",
      " Training loss:\t0.05217068630266735\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 503 of 1000 ####\n",
      " Training loss:\t0.052170690895921135\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 504 of 1000 ####\n",
      " Training loss:\t0.05217068630266735\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 505 of 1000 ####\n",
      " Training loss:\t0.052170672522906004\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 506 of 1000 ####\n",
      " Training loss:\t0.05217067711615979\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 507 of 1000 ####\n",
      " Training loss:\t0.05217068630266735\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 508 of 1000 ####\n",
      " Training loss:\t0.05217067711615979\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 509 of 1000 ####\n",
      " Training loss:\t0.05217069548917491\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 510 of 1000 ####\n",
      " Training loss:\t0.05217068630266735\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 511 of 1000 ####\n",
      " Training loss:\t0.052170700082428696\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 512 of 1000 ####\n",
      " Training loss:\t0.05217067711615979\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 513 of 1000 ####\n",
      " Training loss:\t0.052170690895921135\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 514 of 1000 ####\n",
      " Training loss:\t0.052170690895921135\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 515 of 1000 ####\n",
      " Training loss:\t0.05217067711615979\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 516 of 1000 ####\n",
      " Training loss:\t0.052170690895921135\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 517 of 1000 ####\n",
      " Training loss:\t0.052170681709413566\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 518 of 1000 ####\n",
      " Training loss:\t0.052170690895921135\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 519 of 1000 ####\n",
      " Training loss:\t0.052170681709413566\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 520 of 1000 ####\n",
      " Training loss:\t0.052170672522906004\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 521 of 1000 ####\n",
      " Training loss:\t0.05217067711615979\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 522 of 1000 ####\n",
      " Training loss:\t0.052170681709413566\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 523 of 1000 ####\n",
      " Training loss:\t0.05217069548917491\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 524 of 1000 ####\n",
      " Training loss:\t0.05217067711615979\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 525 of 1000 ####\n",
      " Training loss:\t0.05217068630266735\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 526 of 1000 ####\n",
      " Training loss:\t0.05217069548917491\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 527 of 1000 ####\n",
      " Training loss:\t0.05217067711615979\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 528 of 1000 ####\n",
      " Training loss:\t0.052170672522906004\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 529 of 1000 ####\n",
      " Training loss:\t0.052170681709413566\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 530 of 1000 ####\n",
      " Training loss:\t0.052170681709413566\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 531 of 1000 ####\n",
      " Training loss:\t0.052170681709413566\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 532 of 1000 ####\n",
      " Training loss:\t0.05217068630266735\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 533 of 1000 ####\n",
      " Training loss:\t0.05217067711615979\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 534 of 1000 ####\n",
      " Training loss:\t0.052170681709413566\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 535 of 1000 ####\n",
      " Training loss:\t0.05217067711615979\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 536 of 1000 ####\n",
      " Training loss:\t0.05217068630266735\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 537 of 1000 ####\n",
      " Training loss:\t0.05217068630266735\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 538 of 1000 ####\n",
      " Training loss:\t0.05217067711615979\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 539 of 1000 ####\n",
      " Training loss:\t0.052170681709413566\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 540 of 1000 ####\n",
      " Training loss:\t0.05217068630266735\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 541 of 1000 ####\n",
      " Training loss:\t0.052170690895921135\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 542 of 1000 ####\n",
      " Training loss:\t0.05217067711615979\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 543 of 1000 ####\n",
      " Training loss:\t0.05217068630266735\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 544 of 1000 ####\n",
      " Training loss:\t0.05217068630266735\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 545 of 1000 ####\n",
      " Training loss:\t0.052170681709413566\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 546 of 1000 ####\n",
      " Training loss:\t0.052170672522906004\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 547 of 1000 ####\n",
      " Training loss:\t0.052170681709413566\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 548 of 1000 ####\n",
      " Training loss:\t0.05217069548917491\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 549 of 1000 ####\n",
      " Training loss:\t0.052170690895921135\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 550 of 1000 ####\n",
      " Training loss:\t0.05217068630266735\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 551 of 1000 ####\n",
      " Training loss:\t0.052170681709413566\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 552 of 1000 ####\n",
      " Training loss:\t0.05217067711615979\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 553 of 1000 ####\n",
      " Training loss:\t0.052170690895921135\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 554 of 1000 ####\n",
      " Training loss:\t0.05217067711615979\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 555 of 1000 ####\n",
      " Training loss:\t0.052170681709413566\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 556 of 1000 ####\n",
      " Training loss:\t0.05217069548917491\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 557 of 1000 ####\n",
      " Training loss:\t0.05217068630266735\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 558 of 1000 ####\n",
      " Training loss:\t0.05217069548917491\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 559 of 1000 ####\n",
      " Training loss:\t0.052170681709413566\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 560 of 1000 ####\n",
      " Training loss:\t0.05217067711615979\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 561 of 1000 ####\n",
      " Training loss:\t0.052170681709413566\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 562 of 1000 ####\n",
      " Training loss:\t0.052170681709413566\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 563 of 1000 ####\n",
      " Training loss:\t0.05217067711615979\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 564 of 1000 ####\n",
      " Training loss:\t0.052170681709413566\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 565 of 1000 ####\n",
      " Training loss:\t0.05217066792965223\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 566 of 1000 ####\n",
      " Training loss:\t0.052170690895921135\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 567 of 1000 ####\n",
      " Training loss:\t0.052170690895921135\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 568 of 1000 ####\n",
      " Training loss:\t0.05217068630266735\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 569 of 1000 ####\n",
      " Training loss:\t0.052170700082428696\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 570 of 1000 ####\n",
      " Training loss:\t0.05217068630266735\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 571 of 1000 ####\n",
      " Training loss:\t0.052170672522906004\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n",
      "#### Epoch: 572 of 1000 ####\n",
      " Training loss:\t0.052170690895921135\n",
      " Test loss:\t0.05833601951599121\n",
      " Validation loss:\t0.060748581702892594\n",
      "R2:  0.9389723830656812\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 48\u001b[0m\n\u001b[0;32m     46\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     47\u001b[0m \u001b[39mfor\u001b[39;00m \u001b[39minput\u001b[39m,targets,descriptor,fingerprint \u001b[39min\u001b[39;00m trainingData:\n\u001b[1;32m---> 48\u001b[0m       loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m train(\u001b[39minput\u001b[39;49m,targets,descriptor,fingerprint)\u001b[39m.\u001b[39mdetach()\n\u001b[0;32m     50\u001b[0m test \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     51\u001b[0m r2 \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "Cell \u001b[1;32mIn[14], line 25\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(data_in, targets, descriptors, fingerprints)\u001b[0m\n\u001b[0;32m     22\u001b[0m out \u001b[39m=\u001b[39m model(data_in,descriptors,fingerprints)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     23\u001b[0m loss \u001b[39m=\u001b[39m criterion(out, targets)\n\u001b[1;32m---> 25\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()  \u001b[39m# Derive gradients.\u001b[39;00m\n\u001b[0;32m     26\u001b[0m optimizer\u001b[39m.\u001b[39mstep()  \u001b[39m# Update parameters based on gradients.\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\j031m\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\j031m\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "hidden_channels = 1024\n",
    "embedding_dim = 128\n",
    "d_target = 15\n",
    "nHeads = 8\n",
    "learningRate = 0.0005\n",
    "nDescriptors = len(mol_descriptor[0])\n",
    "nFingerprints = len(mol_fingerprint[0])\n",
    "vocab_size = len(tokenDict)+1\n",
    "torch.manual_seed(12345)\n",
    "\n",
    "model = TransformerNetwork(hidden_channels,d_target,vocab_size, embedding_dim, nHeads,maxTokenLength,nDescriptors,nFingerprints).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learningRate, weight_decay=5e-4)\n",
    "# decay learning rate\n",
    "decayRate = 0.95\n",
    "lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=decayRate)\n",
    "criterion = torch.nn.MSELoss().to(device)\n",
    "\n",
    "def train(data_in, targets, descriptors,fingerprints):\n",
    "      model.train()\n",
    "      optimizer.zero_grad()  # Clear gradients.\n",
    "      out = model(data_in,descriptors,fingerprints).to(device)\n",
    "      loss = criterion(out, targets)\n",
    "      \n",
    "      loss.backward()  # Derive gradients.\n",
    "      optimizer.step()  # Update parameters based on gradients.\n",
    "      return loss\n",
    "\n",
    "def infer(data_in, targets, descriptors, fingerprints):\n",
    "      with torch.no_grad():\n",
    "            model.eval()\n",
    "            out = model(data_in, descriptors,fingerprints)\n",
    "            loss = criterion(out, targets)\n",
    "            r2 = r2_score(targets.cpu().numpy(), out.cpu().numpy())\n",
    "      return loss, r2\n",
    "\n",
    "\n",
    "earlyStop = False\n",
    "stopTolerance = 3\n",
    "minDiff = 0.01\n",
    "counter = 0\n",
    "epoch = 0\n",
    "lossList = []\n",
    "r2List = []\n",
    "while not earlyStop and epoch < 1000:\n",
    "      loss = 0\n",
    "      for input,targets,descriptor,fingerprint in trainingData:\n",
    "            loss += train(input,targets,descriptor,fingerprint).detach()\n",
    "      \n",
    "      test = 0\n",
    "      r2 = 0\n",
    "      for input, targets, descriptor, fingerprint in testData:\n",
    "            tempTest, tempR2 = infer(input,targets,descriptor,fingerprint)\n",
    "            test += tempTest\n",
    "            r2 += tempR2\n",
    "      r2List.append(r2.item()/len(testData))\n",
    "\n",
    "      \n",
    "      val = 0\n",
    "      for input,targets,descriptor,fingerprint in validationData:\n",
    "            tempVal,tempR2 = infer(input,targets, descriptor, fingerprint)\n",
    "            val += tempVal\n",
    "      \n",
    "      lr_scheduler.step()\n",
    "\n",
    "      # Save loss AS PYTHON NUMBER (not tensor) in list\n",
    "      lossList.append([loss.item()/len(trainingData), test.item()/len(testData), val.item()/len(validationData)])\n",
    "\n",
    "      # Early stopping\n",
    "      if (lossList[epoch][2]-lossList[epoch][0]) > minDiff:\n",
    "            counter += 1\n",
    "      else:\n",
    "            counter = 0\n",
    "            if counter >= stopTolerance:\n",
    "                  earlyStop = True\n",
    "\n",
    "      print(f\"#### Epoch: {epoch} of 1000 ####\\n Training loss:\\t{lossList[epoch][0]}\\n Test loss:\\t{lossList[epoch][1]}\\n Validation loss:\\t{lossList[epoch][2]}\")\n",
    "      print('R2: ',r2List[epoch])\n",
    "\n",
    "      epoch+=1 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
