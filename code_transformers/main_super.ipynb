{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import atomInSmiles\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from code_graphs.utility_functions import get_num_parameters , get_data_split_indices, scale_targets\n",
    "from classes import create_encoded_vector, PositionalEncoding, TransformerLayer, TransformerNetwork\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will attempt to gather all the variant in this file then changing if descriptors are used etc by only changing bools instead of having four different files.\n",
    "Alot of redundancy this way bit the program is much shorter, we do all calcualtion as if we are using both descriptors and fingerprints. But do not concat them in the transformerencoder if their bool is set to False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "useDescriptors = False\n",
    "useFingerprints = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available: True\n",
      "device: cuda\n",
      "cuda version: 11.7\n",
      "gpu: NVIDIA GeForce GTX 970\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# check if cuda is available\n",
    "print('cuda available:', torch.cuda.is_available())\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('device:', \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    print('cuda version:', torch.version.cuda)\n",
    "    print('gpu:', torch.cuda.get_device_name(0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(132820, 21)\n",
      "(132820, 179)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "df = pd.read_csv(\"../data/smiles_and_targets.csv\")\n",
    "print(np.shape(df))\n",
    "\n",
    "mol_descriptor = np.load(\"../data/mol_descriptors.npy\")\n",
    "mol_fingerprint = np.load(\"../data/mol_morgan_fingerprints.npy\")\n",
    "print(mol_descriptor.shape)\n",
    "\n",
    "\n",
    "properties_names = ['A', 'B', 'C', 'mu', 'alfa', 'homo', 'lumo', 'gap', 'RÂ²', 'zpve', 'U0', 'U', 'H', 'G', 'Cv']\n",
    "\n",
    "x_smiles = df.smiles.values\n",
    "y_targets = df.loc[:, properties_names].values # shape = (n_samples, n_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest word (max amount of tokens): 22\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenList = []\n",
    "for smile in x_smiles:\n",
    "    tokenList.append(atomInSmiles.encode(smile).split())\n",
    "#tokenList is target\n",
    "\n",
    "# find longest word (max tokens from one smile+1)\n",
    "maxTokenLength = 0\n",
    "for token in tokenList:\n",
    "    if len(token)>maxTokenLength:\n",
    "        maxTokenLength = len(token)\n",
    "print('Longest word (max amount of tokens):', maxTokenLength)\n",
    "\n",
    "# Give each token a index in a dictionary\n",
    "tokenDict = {}\n",
    "count = 1\n",
    "\n",
    "dictList = []\n",
    "for itokens in tokenList:\n",
    "    for itoke in itokens:\n",
    "        #print(itoke)\n",
    "        if tokenDict.get(itoke) == None:\n",
    "            tokenDict[itoke] = count\n",
    "        \n",
    "            # current = [itoke, count]\n",
    "            # dictList.append(current)\n",
    "            count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodedTokens = []\n",
    "for token in tokenList:\n",
    "    encodedTokens.append(create_encoded_vector(token,tokenDict,maxTokenLength))\n",
    "    \n",
    "encodedTokens = np.array(encodedTokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Divide into splits\n",
    "trainSplit, validationSplit, testSplit = get_data_split_indices(len(encodedTokens), 0.1, 0.1)\n",
    "\n",
    "# Training\n",
    "encodedTrainData = torch.tensor(encodedTokens[trainSplit], dtype=torch.long, device=device)\n",
    "trainTargets = y_targets[trainSplit]\n",
    "\n",
    "# Validation\n",
    "encodedValidationData = torch.tensor(encodedTokens[validationSplit], dtype=torch.long, device=device)\n",
    "validationTargets = y_targets[validationSplit]\n",
    "\n",
    "# Test\n",
    "encodedTestData = torch.tensor(encodedTokens[testSplit], dtype=torch.long, device=device)\n",
    "testTargets = y_targets[testSplit]\n",
    "\n",
    "trainTargets, validationTargets, testTargets, scalerTargets = scale_targets(trainTargets, validationTargets, testTargets)\n",
    "\n",
    "minmax_scaler = MinMaxScaler()\n",
    "\n",
    "# Descriptors\n",
    "trainDescriptors = mol_descriptor[trainSplit]\n",
    "valDescriptors = mol_descriptor[validationSplit]\n",
    "testDescriptors = mol_descriptor[testSplit]\n",
    "# Normalize\n",
    "trainDescriptors = minmax_scaler.fit_transform(trainDescriptors)\n",
    "valDescriptors = minmax_scaler.transform(valDescriptors)\n",
    "testDescriptors = minmax_scaler.transform(testDescriptors)\n",
    "# Convert descriptors and fingerprints to tensors\n",
    "trainDescriptors = torch.from_numpy(trainDescriptors).float().to(device)\n",
    "valDescriptors = torch.from_numpy(valDescriptors).float().to(device)\n",
    "testDescriptors = torch.from_numpy(testDescriptors).float().to(device)\n",
    "\n",
    "# Fingerprints\n",
    "trainFingerprints = mol_fingerprint[trainSplit]\n",
    "valFingerprints = mol_fingerprint[validationSplit]\n",
    "testFingerprints = mol_fingerprint[testSplit]\n",
    "# Normalize\n",
    "trainFingerprints = minmax_scaler.fit_transform(trainFingerprints)\n",
    "valFingerprints = minmax_scaler.transform(valFingerprints)\n",
    "testFingerprints = minmax_scaler.transform(testFingerprints)\n",
    "# Convert to tensor\n",
    "trainFingerprints = torch.from_numpy(trainFingerprints).float().to(device)\n",
    "valFingerprints = torch.from_numpy(valFingerprints).float().to(device)\n",
    "testFingerprints = torch.from_numpy(testFingerprints).float().to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Convert targets to tensors\n",
    "trainTargets = torch.tensor(trainTargets, dtype=torch.float, device=device)\n",
    "validationTargets = torch.tensor(validationTargets, dtype=torch.float, device=device)\n",
    "testTargets = torch.tensor(testTargets, dtype=torch.float, device=device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class datasetObject(Dataset):\n",
    "    def __init__(self,data,targets,descriptors,fingerprints):\n",
    "        self.data = data\n",
    "        self.descriptors = descriptors\n",
    "        self.targets = targets\n",
    "        self.fingerprints = fingerprints\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.data[index]\n",
    "        target = self.targets[index]\n",
    "        descriptor = self.descriptors[index]\n",
    "        fingerprint = self.fingerprints[index]\n",
    "        return (sample), (target), (descriptor), (fingerprint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData = DataLoader(datasetObject(encodedTrainData, trainTargets, trainDescriptors, trainFingerprints), batch_size)\n",
    "testData = DataLoader(datasetObject(encodedTestData, testTargets,  testDescriptors, testFingerprints), batch_size)\n",
    "validationData = DataLoader(datasetObject(encodedValidationData, validationTargets, valDescriptors, valFingerprints), batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Epoch: 0 of 100 ####\n",
      " Training loss:\t0.380817668255663\n",
      " Test loss:\t0.1615614340855525\n",
      " Validation loss:\t0.16400819558363694\n",
      "R2: [0.5089178397889479, 0.7255251138258012, 0.8029658810922542, 0.447592706682468, 0.9353581707253813, 0.7283268139274488, 0.8721544657626086, 0.8503837864739473, 0.8659699591898343, 0.9542576647658447, 0.9637948905734687, 0.9650740479419557, 0.9636901503940941, 0.9630483260102161, 0.9304794362470131]\n",
      "avg R2 0.8318359502267523 \n",
      "\n",
      "#### Epoch: 1 of 100 ####\n",
      " Training loss:\t0.15439043039302378\n",
      " Test loss:\t0.1260609351671659\n",
      " Validation loss:\t0.12879112133613\n",
      "R2: [0.5821497308857393, 0.7704893510414424, 0.8489197311407194, 0.49354308892378557, 0.9618911554816818, 0.7784392558079687, 0.9062868586221637, 0.8824718262518764, 0.8955157425832148, 0.9840424322520285, 0.9913233727970368, 0.9910137344661815, 0.9914185634612964, 0.9908391059346737, 0.9564706815410694]\n",
      "avg R2 0.8683209754127253 \n",
      "\n",
      "#### Epoch: 2 of 100 ####\n",
      " Training loss:\t0.12814113334458252\n",
      " Test loss:\t0.1109160643357497\n",
      " Validation loss:\t0.11380461546090934\n",
      "R2: [0.6448583572791649, 0.8013023810315466, 0.8658624732900688, 0.5223086392401022, 0.9703186767524564, 0.7965612933956159, 0.9193461579883585, 0.893070961740716, 0.9082148490287262, 0.9910663892894915, 0.993447965485913, 0.9931002974857329, 0.9932529378551848, 0.9926454124018714, 0.9709751875070166]\n",
      "avg R2 0.8837554653181312 \n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 67\u001b[0m\n\u001b[0;32m     65\u001b[0m r2 \u001b[39m=\u001b[39m [\u001b[39m0\u001b[39m]\u001b[39m*\u001b[39md_target\n\u001b[0;32m     66\u001b[0m \u001b[39mfor\u001b[39;00m \u001b[39minput\u001b[39m, targets, descriptor, fingerprint \u001b[39min\u001b[39;00m testData:\n\u001b[1;32m---> 67\u001b[0m       tempTest, tempR2 \u001b[39m=\u001b[39m inferNew(\u001b[39minput\u001b[39;49m,targets,descriptor,fingerprint)\n\u001b[0;32m     68\u001b[0m       test \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tempTest\n\u001b[0;32m     69\u001b[0m       \u001b[39m# Add r2 for each variable\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[10], line 46\u001b[0m, in \u001b[0;36minferNew\u001b[1;34m(data_in, targets, descriptors, fingerprints)\u001b[0m\n\u001b[0;32m     44\u001b[0m         r2_scores \u001b[39m=\u001b[39m []\n\u001b[0;32m     45\u001b[0m         \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(targets\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]):\n\u001b[1;32m---> 46\u001b[0m               r2 \u001b[39m=\u001b[39m r2_score(targets[:, item]\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy(), out[:, item]\u001b[39m.\u001b[39;49mcpu()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy())\n\u001b[0;32m     47\u001b[0m               r2_scores\u001b[39m.\u001b[39mappend(r2)\n\u001b[0;32m     49\u001b[0m \u001b[39mreturn\u001b[39;00m loss, r2_scores\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "hidden_channels = 512\n",
    "embedding_dim = 128\n",
    "d_target = 15\n",
    "nHeads = 4\n",
    "learningRate = 0.0005\n",
    "if useDescriptors:\n",
    "      nDescriptors = len(mol_descriptor[0])\n",
    "else:\n",
    "      nDescriptors = 0\n",
    "\n",
    "if useFingerprints:\n",
    "      nFingerprints = len(mol_fingerprint[0])\n",
    "else:\n",
    "      nFingerprints = 0\n",
    "\n",
    "vocab_size = len(tokenDict)+1\n",
    "torch.manual_seed(12345)\n",
    "\n",
    "model = TransformerNetwork(hidden_channels,d_target,vocab_size, embedding_dim, nHeads,maxTokenLength,\n",
    "                           nDescriptors,nFingerprints,useDescriptors,useFingerprints).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learningRate, weight_decay=0)\n",
    "# decay learning rate\n",
    "decayRate = 1\n",
    "lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=decayRate)\n",
    "criterion = torch.nn.MSELoss().to(device)\n",
    "\n",
    "def train(data_in, targets, descriptors, fingerprints):\n",
    "      model.train()\n",
    "      optimizer.zero_grad()  # Clear gradients.\n",
    "      out = model(data_in,descriptors,fingerprints).to(device)\n",
    "      loss = criterion(out, targets)\n",
    "      \n",
    "      loss.backward()  # Derive gradients.\n",
    "      optimizer.step()  # Update parameters based on gradients.\n",
    "      return loss\n",
    "\n",
    "# This calculates r2 for each target separately \n",
    "def inferNew(data_in, targets, descriptors, fingerprints):\n",
    "    with torch.no_grad():\n",
    "            model.eval()\n",
    "            out = model(data_in, descriptors, fingerprints).to(device)\n",
    "            loss = criterion(out, targets)\n",
    "\n",
    "            r2_scores = []\n",
    "            for item in range(targets.shape[1]):\n",
    "                  r2 = r2_score(targets[:, item].cpu().detach().numpy(), out[:, item].cpu().detach().numpy())\n",
    "                  r2_scores.append(r2)\n",
    "\n",
    "    return loss, r2_scores\n",
    "\n",
    "\n",
    "earlyStop = False\n",
    "stopTolerance = 3\n",
    "minDiff = 0.01\n",
    "counter = 0\n",
    "epoch = 0\n",
    "lossList = []\n",
    "r2List = []\n",
    "while not earlyStop and epoch < 100:\n",
    "      loss = 0\n",
    "      for input,targets,descriptor,fingerprint in trainingData:\n",
    "            loss += train(input,targets,descriptor,fingerprint).detach()\n",
    "      \n",
    "      test = 0\n",
    "      r2 = [0]*d_target\n",
    "      for input, targets, descriptor, fingerprint in testData:\n",
    "            tempTest, tempR2 = inferNew(input,targets,descriptor,fingerprint)\n",
    "            test += tempTest\n",
    "            # Add r2 for each variable\n",
    "            r2 = [x + y for x, y in zip(r2, tempR2)]\n",
    "      # Average over batch\n",
    "      r2 = [x / len(testData) for x in r2]\n",
    "      r2List.append(r2)\n",
    "\n",
    "      val = 0\n",
    "      for input,targets,descriptor,fingerprint in validationData:\n",
    "            tempVal,tempR2 = inferNew(input,targets, descriptor, fingerprint)\n",
    "            val += tempVal\n",
    "      \n",
    "      lr_scheduler.step()\n",
    "\n",
    "      # Save loss AS PYTHON NUMBER (not tensor) in list\n",
    "      lossList.append([loss.item()/len(trainingData), test.item()/len(testData), val.item()/len(validationData)])\n",
    "\n",
    "      # Early stopping\n",
    "      if (lossList[epoch][2]-lossList[epoch][0]) > minDiff:\n",
    "            counter += 1\n",
    "      else:\n",
    "            counter = 0\n",
    "            if counter >= stopTolerance:\n",
    "                  earlyStop = True\n",
    "\n",
    "      print(f\"#### Epoch: {epoch} of 100 ####\\n Training loss:\\t{lossList[epoch][0]}\\n Test loss:\\t{lossList[epoch][1]}\\n Validation loss:\\t{lossList[epoch][2]}\")\n",
    "      print('R2:', r2List[epoch])\n",
    "      print('avg R2',np.mean(r2List[epoch]),'\\n')\n",
    "      epoch+=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7374174639688904, 0.8468307442783691, 0.8926509429882702, 0.575811530605992, 0.9741914082581106, 0.8291913662528797, 0.9386550269223121, 0.9162573986517647, 0.9298462251219253, 0.9952611402526274, 0.9853929708954238, 0.9857257772646325, 0.9857025497125973, 0.98580312773329, 0.9780792388581795]\n",
      "0.9037877941176842\n",
      "[0.7374174639688904, 0.8468307442783691, 0.8926509429882702, 0.9741914082581106, 0.8291913662528797, 0.9386550269223121, 0.9162573986517647, 0.9298462251219253, 0.9952611402526274, 0.9853929708954238, 0.9857257772646325, 0.9857025497125973, 0.98580312773329, 0.9780792388581795]\n",
      "0.9272146700828049\n"
     ]
    }
   ],
   "source": [
    "lastRow = r2List[-1]\n",
    "print(lastRow)\n",
    "print(np.mean(lastRow))\n",
    "\n",
    "del(lastRow[3])\n",
    "print(lastRow)\n",
    "print(np.mean(lastRow))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
