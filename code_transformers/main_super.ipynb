{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import atomInSmiles\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from code_graphs.utility_functions import get_num_parameters , get_data_split_indices, scale_targets\n",
    "from classes import create_encoded_vector, PositionalEncoding, TransformerLayer, TransformerNetwork\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will attempt to gather all the variant in this file then changing if descriptors are used etc by only changing bools instead of having four different files.\n",
    "Alot of redundancy this way bit the program is much shorter, we do all calcualtion as if we are using both descriptors and fingerprints. But do not concat them in the transformerencoder if their bool is set to False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "useDescriptors = True\n",
    "\n",
    "# rdkitDescriptor == True uses rdKit descriptors otherwise mordred is used\n",
    "rdkitDescriptor = False\n",
    "\n",
    "useFingerprints = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available: True\n",
      "device: cuda\n",
      "cuda version: 11.7\n",
      "gpu: NVIDIA GeForce GTX 970\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# check if cuda is available\n",
    "print('cuda available:', torch.cuda.is_available())\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('device:', \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    print('cuda version:', torch.version.cuda)\n",
    "    print('gpu:', torch.cuda.get_device_name(0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(132820, 21)\n",
      "(132820, 985)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "df = pd.read_csv(\"../data/smiles_and_targets.csv\")\n",
    "print(np.shape(df))\n",
    "\n",
    "if rdkitDescriptor:\n",
    "    mol_descriptor = np.load(\"../data/mol_descriptors.npy\")\n",
    "else :\n",
    "    mol_descriptor = np.load(\"../data/Mordred_mol_descriptors.npy\")\n",
    "\n",
    "\n",
    "\n",
    "mol_fingerprint = np.load(\"../data/mol_morgan_fingerprints.npy\")\n",
    "print(mol_descriptor.shape)\n",
    "\n",
    "\n",
    "properties_names = ['A', 'B', 'C', 'mu', 'alfa', 'homo', 'lumo', 'gap', 'RÂ²', 'zpve', 'U0', 'U', 'H', 'G', 'Cv']\n",
    "\n",
    "x_smiles = df.smiles.values\n",
    "y_targets = df.loc[:, properties_names].values # shape = (n_samples, n_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest word (max amount of tokens): 22\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenList = []\n",
    "for smile in x_smiles:\n",
    "    tokenList.append(atomInSmiles.encode(smile).split())\n",
    "#tokenList is target\n",
    "\n",
    "# find longest word (max tokens from one smile+1)\n",
    "maxTokenLength = 0\n",
    "for token in tokenList:\n",
    "    if len(token)>maxTokenLength:\n",
    "        maxTokenLength = len(token)\n",
    "print('Longest word (max amount of tokens):', maxTokenLength)\n",
    "\n",
    "# Give each token a index in a dictionary\n",
    "tokenDict = {}\n",
    "count = 1\n",
    "\n",
    "dictList = []\n",
    "for itokens in tokenList:\n",
    "    for itoke in itokens:\n",
    "        #print(itoke)\n",
    "        if tokenDict.get(itoke) == None:\n",
    "            tokenDict[itoke] = count\n",
    "        \n",
    "            # current = [itoke, count]\n",
    "            # dictList.append(current)\n",
    "            count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodedTokens = []\n",
    "for token in tokenList:\n",
    "    encodedTokens.append(create_encoded_vector(token,tokenDict,maxTokenLength))\n",
    "    \n",
    "encodedTokens = np.array(encodedTokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "\n",
    "# Divide into splits\n",
    "trainSplit, validationSplit, testSplit = get_data_split_indices(len(encodedTokens), 0.1, 0.1)\n",
    "\n",
    "# Training\n",
    "encodedTrainData = torch.tensor(encodedTokens[trainSplit], dtype=torch.long, device=device)\n",
    "trainTargets = y_targets[trainSplit]\n",
    "\n",
    "# Validation\n",
    "encodedValidationData = torch.tensor(encodedTokens[validationSplit], dtype=torch.long, device=device)\n",
    "validationTargets = y_targets[validationSplit]\n",
    "\n",
    "# Test\n",
    "encodedTestData = torch.tensor(encodedTokens[testSplit], dtype=torch.long, device=device)\n",
    "testTargets = y_targets[testSplit]\n",
    "\n",
    "trainTargets, validationTargets, testTargets, scalerTargets = scale_targets(trainTargets, validationTargets, testTargets)\n",
    "\n",
    "minmax_scaler = MinMaxScaler()\n",
    "\n",
    "# Descriptors\n",
    "trainDescriptors = mol_descriptor[trainSplit]\n",
    "valDescriptors = mol_descriptor[validationSplit]\n",
    "testDescriptors = mol_descriptor[testSplit]\n",
    "# Normalize\n",
    "trainDescriptors = minmax_scaler.fit_transform(trainDescriptors)\n",
    "valDescriptors = minmax_scaler.transform(valDescriptors)\n",
    "testDescriptors = minmax_scaler.transform(testDescriptors)\n",
    "# Convert descriptors and fingerprints to tensors\n",
    "trainDescriptors = torch.from_numpy(trainDescriptors).float().to(device)\n",
    "valDescriptors = torch.from_numpy(valDescriptors).float().to(device)\n",
    "testDescriptors = torch.from_numpy(testDescriptors).float().to(device)\n",
    "\n",
    "# Fingerprints\n",
    "trainFingerprints = mol_fingerprint[trainSplit]\n",
    "valFingerprints = mol_fingerprint[validationSplit]\n",
    "testFingerprints = mol_fingerprint[testSplit]\n",
    "# Normalize\n",
    "trainFingerprints = minmax_scaler.fit_transform(trainFingerprints)\n",
    "valFingerprints = minmax_scaler.transform(valFingerprints)\n",
    "testFingerprints = minmax_scaler.transform(testFingerprints)\n",
    "# Convert to tensor\n",
    "trainFingerprints = torch.from_numpy(trainFingerprints).float().to(device)\n",
    "valFingerprints = torch.from_numpy(valFingerprints).float().to(device)\n",
    "testFingerprints = torch.from_numpy(testFingerprints).float().to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Convert targets to tensors\n",
    "trainTargets = torch.tensor(trainTargets, dtype=torch.float, device=device)\n",
    "validationTargets = torch.tensor(validationTargets, dtype=torch.float, device=device)\n",
    "testTargets = torch.tensor(testTargets, dtype=torch.float, device=device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class datasetObject(Dataset):\n",
    "    def __init__(self,data,targets,descriptors,fingerprints):\n",
    "        self.data = data\n",
    "        self.descriptors = descriptors\n",
    "        self.targets = targets\n",
    "        self.fingerprints = fingerprints\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.data[index]\n",
    "        target = self.targets[index]\n",
    "        descriptor = self.descriptors[index]\n",
    "        fingerprint = self.fingerprints[index]\n",
    "        return (sample), (target), (descriptor), (fingerprint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData = DataLoader(datasetObject(encodedTrainData, trainTargets, trainDescriptors, trainFingerprints), batch_size)\n",
    "testData = DataLoader(datasetObject(encodedTestData, testTargets,  testDescriptors, testFingerprints), batch_size)\n",
    "validationData = DataLoader(datasetObject(encodedValidationData, validationTargets, valDescriptors, valFingerprints), batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Epoch: 0 of 100 ####\n",
      " Training loss:\t0.11326718545692072\n",
      " Test loss:\t0.07604318857192993\n",
      " Validation loss:\t0.07850024333366981\n",
      "R2: [0.783591678493746, 0.89572665505139, 0.9284220834424693, 0.5886612753829072, 0.9825314702378869, 0.8413522256175303, 0.9380325515476446, 0.911648101881167, 0.9567906537191521, 0.9961291634991037, 0.9982971458955359, 0.9972544351027735, 0.9982218435165838, 0.9983499138366236, 0.9872465214790934]\n",
      "avg R2 0.9201503812469072 \n",
      "\n",
      "#### Epoch: 1 of 100 ####\n",
      " Training loss:\t0.06819705859786555\n",
      " Test loss:\t0.06282208057550284\n",
      " Validation loss:\t0.06528549927931565\n",
      "R2: [0.8294427945468696, 0.9106603594504119, 0.9379708340850171, 0.6374202099736769, 0.9829417869532081, 0.8774465023367102, 0.9575792218507957, 0.9345619958173035, 0.9620736590520171, 0.9956346621056735, 0.9979794627685004, 0.9980755671475119, 0.9983012492889878, 0.9983228837050159, 0.9876945696656517]\n",
      "avg R2 0.93374038391649 \n",
      "\n",
      "#### Epoch: 2 of 100 ####\n",
      " Training loss:\t0.057189119213823095\n",
      " Test loss:\t0.05696764817604652\n",
      " Validation loss:\t0.05944155271236713\n",
      "R2: [0.8448467595805698, 0.9210767190417196, 0.9393667662144571, 0.6634922175264238, 0.9853890214810677, 0.8947599007835323, 0.9622894178932626, 0.9453188361132547, 0.9646986934383579, 0.9940584943950636, 0.9981861694739111, 0.9986695245043803, 0.9982832852175216, 0.9985262182090425, 0.9881398670390646]\n",
      "avg R2 0.9398067927274419 \n",
      "\n",
      "#### Epoch: 3 of 100 ####\n",
      " Training loss:\t0.050213808614327775\n",
      " Test loss:\t0.05308003150499784\n",
      " Validation loss:\t0.05539480080971351\n",
      "R2: [0.8621897579575534, 0.925784282851632, 0.9433501652409314, 0.6779434226759287, 0.9873186904642656, 0.9042453111655723, 0.9652087974832768, 0.9497410172124301, 0.9651238900664213, 0.9955130072392173, 0.9977180721139499, 0.9983225456915695, 0.997991064685359, 0.99804947013184, 0.9895843898866485]\n",
      "avg R2 0.9438722589911063 \n",
      "\n",
      "#### Epoch: 4 of 100 ####\n",
      " Training loss:\t0.04521156232949842\n",
      " Test loss:\t0.050864806542029746\n",
      " Validation loss:\t0.053167980450850263\n",
      "R2: [0.8737802447632391, 0.9281901953349383, 0.9473953108284789, 0.6858642071530523, 0.987725955657801, 0.9074000213933945, 0.9658687622766511, 0.9516686274649833, 0.9654964218013189, 0.9937704370655067, 0.998828739303142, 0.9988770007517574, 0.9988802229483107, 0.9988698265032696, 0.9901587778655416]\n",
      "avg R2 0.9461849834074257 \n",
      "\n",
      "#### Epoch: 5 of 100 ####\n",
      " Training loss:\t0.04150125134931423\n",
      " Test loss:\t0.049386340838212237\n",
      " Validation loss:\t0.05161410111647386\n",
      "R2: [0.8824640358031286, 0.9318860636881673, 0.95164856907075, 0.692971021247705, 0.9874689136777182, 0.9065686353039991, 0.9681659687872821, 0.9554332803979739, 0.9677035723871766, 0.9880836428623, 0.9987692202541502, 0.9987180561852278, 0.9987457331204518, 0.9987306246708604, 0.9891236800790993]\n",
      "avg R2 0.947765401169066 \n",
      "\n",
      "#### Epoch: 6 of 100 ####\n",
      " Training loss:\t0.0384787939026871\n",
      " Test loss:\t0.050398519405951865\n",
      " Validation loss:\t0.05264213451972374\n",
      "R2: [0.8771983555677655, 0.9352003392875802, 0.9531753436235936, 0.6953579455801226, 0.9859627249819697, 0.9081558857166309, 0.9710242536677792, 0.956759208611736, 0.9700631694959276, 0.9857245797118848, 0.9936884086579253, 0.994218993228063, 0.9941391759923549, 0.9942473232014558, 0.9845575656318258]\n",
      "avg R2 0.9466315515304412 \n",
      "\n",
      "#### Epoch: 7 of 100 ####\n",
      " Training loss:\t0.03605786897129929\n",
      " Test loss:\t0.04765618305939894\n",
      " Validation loss:\t0.04968510682766254\n",
      "R2: [0.8864260780589434, 0.9364916718506724, 0.9536957857790302, 0.6985507557025697, 0.9874705989889363, 0.9120422588894509, 0.9738316387846625, 0.9583418428652807, 0.9708513899231899, 0.9928727071194116, 0.9957821277742945, 0.995898141621254, 0.9959308383069576, 0.9957470251568428, 0.9893003814723312]\n",
      "avg R2 0.9495488828195885 \n",
      "\n",
      "#### Epoch: 8 of 100 ####\n",
      " Training loss:\t0.03399651313244328\n",
      " Test loss:\t0.04591527351966271\n",
      " Validation loss:\t0.04781888081477238\n",
      "R2: [0.8935196639556706, 0.9389067599446551, 0.9551981636665802, 0.7029248862078307, 0.9883655923164615, 0.9130883035380903, 0.9720088470445758, 0.9587454520128194, 0.9711506648066709, 0.9953376671610568, 0.9974499285986662, 0.9974929843929563, 0.9975015312870373, 0.9974099790133001, 0.9919842776741193]\n",
      "avg R2 0.9514056467746993 \n",
      "\n",
      "#### Epoch: 9 of 100 ####\n",
      " Training loss:\t0.032430546603814295\n",
      " Test loss:\t0.045068662900191084\n",
      " Validation loss:\t0.04696096823765682\n",
      "R2: [0.8957928480778181, 0.9397079475720479, 0.9558513476205684, 0.7034599734892674, 0.9898830153099196, 0.913621609669309, 0.9728046503773498, 0.9579460077538408, 0.971346869334059, 0.9979412709749036, 0.998272641289385, 0.9983051027723746, 0.9983579514711067, 0.9983676667907695, 0.9926843587490719]\n",
      "avg R2 0.9522895507501195 \n",
      "\n",
      "#### Epoch: 10 of 100 ####\n",
      " Training loss:\t0.031229367104013018\n",
      " Test loss:\t0.04568613492525541\n",
      " Validation loss:\t0.04742874548985408\n",
      "R2: [0.8938054829227279, 0.938179487164575, 0.9529050781306657, 0.6988996730154103, 0.9894477139851043, 0.912961634948249, 0.973279996526266, 0.9582783177738091, 0.9709619147469326, 0.9985189633116816, 0.9986613069585749, 0.9986555085045397, 0.9987361781089565, 0.9987020498526952, 0.9921398234677312]\n",
      "avg R2 0.951608875294528 \n",
      "\n",
      "#### Epoch: 11 of 100 ####\n",
      " Training loss:\t0.02964142415266594\n",
      " Test loss:\t0.04571912838862492\n",
      " Validation loss:\t0.04740949318959163\n",
      "R2: [0.8953399681241346, 0.9397297221428733, 0.9530857354182575, 0.6966200141205513, 0.9893972319659775, 0.9120726834730767, 0.9733401296199957, 0.9588243719700145, 0.9710805697156233, 0.9983065972294554, 0.9983430768801885, 0.998322264554742, 0.9984237234589504, 0.9983891247731518, 0.9921437672695279]\n",
      "avg R2 0.9515612653811013 \n",
      "\n",
      "#### Epoch: 12 of 100 ####\n",
      " Training loss:\t0.02748351122943986\n",
      " Test loss:\t0.04471742648344774\n",
      " Validation loss:\t0.04641534731938289\n",
      "R2: [0.8969810705499556, 0.9422011033052566, 0.9557795754843339, 0.6982829093256744, 0.9889525381294608, 0.9145229377507416, 0.9744643468422155, 0.9598787474594364, 0.9717281713362139, 0.9981978705517451, 0.9988865048517195, 0.998964099074398, 0.9988538010868117, 0.9989660253307066, 0.9926124939457839]\n",
      "avg R2 0.9526181463349634 \n",
      "\n",
      "#### Epoch: 13 of 100 ####\n",
      " Training loss:\t0.025574972943897926\n",
      " Test loss:\t0.04528750823094295\n",
      " Validation loss:\t0.047008477724515475\n",
      "R2: [0.8937200765597934, 0.9407284102325154, 0.9569237573874877, 0.6959281708774703, 0.9892883561096041, 0.9135859728112445, 0.9747447380862965, 0.9596705723342247, 0.9708316058982155, 0.9952301015059132, 0.9992977406749441, 0.999326538128478, 0.9992798967334764, 0.9993257904722868, 0.9923360390948992]\n",
      "avg R2 0.95201451779379 \n",
      "\n",
      "#### Epoch: 14 of 100 ####\n",
      " Training loss:\t0.024365337263884707\n",
      " Test loss:\t0.04586792909182035\n",
      " Validation loss:\t0.047524869441986084\n",
      "R2: [0.8919223708910589, 0.9401721165746899, 0.9566390398992213, 0.6905975650502992, 0.9888133648634374, 0.9136158009932954, 0.9750726426963013, 0.9599336812674094, 0.9706860665996022, 0.9938706438765076, 0.999370743151018, 0.9993832713798981, 0.9993697313724825, 0.9993858762708719, 0.9923676678891512]\n",
      "avg R2 0.9514133721850163 \n",
      "\n",
      "#### Epoch: 15 of 100 ####\n",
      " Training loss:\t0.023430056632246905\n",
      " Test loss:\t0.046372995926783636\n",
      " Validation loss:\t0.04807572639905489\n",
      "R2: [0.8919311609584953, 0.938171098464238, 0.9563894344064623, 0.6855064407595527, 0.9898136606838507, 0.9125441524824609, 0.9745275794569217, 0.9589601274255747, 0.9703162649806091, 0.99492910697979, 0.9993345315298152, 0.9993366420029298, 0.9993443183093882, 0.9993448851924932, 0.9923983332562545]\n",
      "avg R2 0.950856515792589 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(12345)\n",
    "hidden_channels = 512\n",
    "embedding_dim = 128\n",
    "d_target = 15\n",
    "nHeads = 8\n",
    "learningRate = 0.0005\n",
    "if useDescriptors:\n",
    "      nDescriptors = len(mol_descriptor[0])\n",
    "else:\n",
    "      nDescriptors = 0\n",
    "\n",
    "if useFingerprints:\n",
    "      nFingerprints = len(mol_fingerprint[0])\n",
    "else:\n",
    "      nFingerprints = 0\n",
    "\n",
    "vocab_size = len(tokenDict)+1\n",
    "\n",
    "model = TransformerNetwork(hidden_channels,d_target,vocab_size, embedding_dim, nHeads,maxTokenLength,\n",
    "                           nDescriptors,nFingerprints,useDescriptors,useFingerprints).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learningRate, weight_decay=0)\n",
    "# decay learning rate\n",
    "decayRate = 1\n",
    "lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=decayRate)\n",
    "criterion = torch.nn.MSELoss().to(device)\n",
    "\n",
    "def train(data_in, targets, descriptors, fingerprints):\n",
    "      model.train()\n",
    "      optimizer.zero_grad()  # Clear gradients.\n",
    "      out = model(data_in,descriptors,fingerprints).to(device)\n",
    "      loss = criterion(out, targets)\n",
    "      \n",
    "      loss.backward()  # Derive gradients.\n",
    "      optimizer.step()  # Update parameters based on gradients.\n",
    "      return loss\n",
    "\n",
    "# This calculates r2 for each target separately \n",
    "def inferNew(data_in, targets, descriptors, fingerprints):\n",
    "    with torch.no_grad():\n",
    "            model.eval()\n",
    "            out = model(data_in, descriptors, fingerprints).to(device)\n",
    "            loss = criterion(out, targets)\n",
    "\n",
    "            r2_scores = []\n",
    "            for item in range(targets.shape[1]):\n",
    "                  r2 = r2_score(targets[:, item].cpu().detach().numpy(), out[:, item].cpu().detach().numpy())\n",
    "                  r2_scores.append(r2)\n",
    "\n",
    "    return loss, r2_scores\n",
    "\n",
    "\n",
    "earlyStop = False\n",
    "stopTolerance = 3\n",
    "minDiff = 0.01\n",
    "counter = 0\n",
    "epoch = 0\n",
    "bestValLoss = float('inf')\n",
    "lossList = []\n",
    "r2List = []\n",
    "while not earlyStop and epoch < 100:\n",
    "      loss = 0\n",
    "      for input,targets,descriptor,fingerprint in trainingData:\n",
    "            loss += train(input,targets,descriptor,fingerprint).detach()\n",
    "      \n",
    "      test = 0\n",
    "      r2 = [0]*d_target\n",
    "      for input, targets, descriptor, fingerprint in testData:\n",
    "            tempTest, tempR2 = inferNew(input,targets,descriptor,fingerprint)\n",
    "            test += tempTest\n",
    "            # Add r2 for each variable\n",
    "            r2 = [x + y for x, y in zip(r2, tempR2)]\n",
    "      # Average over batch\n",
    "      r2 = [x / len(testData) for x in r2]\n",
    "      r2List.append(r2)\n",
    "\n",
    "      val = 0\n",
    "      for input,targets,descriptor,fingerprint in validationData:\n",
    "            tempVal,tempR2 = inferNew(input,targets, descriptor, fingerprint)\n",
    "            val += tempVal\n",
    "      \n",
    "      lr_scheduler.step()\n",
    "\n",
    "      # Save loss AS PYTHON NUMBER (not tensor) in list\n",
    "      lossList.append([loss.item()/len(trainingData), test.item()/len(testData), val.item()/len(validationData)])\n",
    "\n",
    "      # Early stopping\n",
    "      if val < bestValLoss - minDiff:\n",
    "            bestValLoss = val\n",
    "            counter = 0\n",
    "      else:\n",
    "            counter += 1\n",
    "            if counter >= stopTolerance:\n",
    "                  earlyStop = True\n",
    "\n",
    "      print(f\"#### Epoch: {epoch} of 100 ####\\n Training loss:\\t{lossList[epoch][0]}\\n Test loss:\\t{lossList[epoch][1]}\\n Validation loss:\\t{lossList[epoch][2]}\")\n",
    "      print('R2:', r2List[epoch])\n",
    "      print('avg R2',np.mean(r2List[epoch]),'\\n')\n",
    "      epoch+=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2ResultsList = []\n",
    "lossResultList = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8919311609584953, 0.938171098464238, 0.9563894344064623, 0.6855064407595527, 0.9898136606838507, 0.9125441524824609, 0.9745275794569217, 0.9589601274255747, 0.9703162649806091, 0.99492910697979, 0.9993345315298152, 0.9993366420029298, 0.9993443183093882, 0.9993448851924932, 0.9923983332562545]\n",
      "0.950856515792589\n"
     ]
    }
   ],
   "source": [
    "print(r2List[-1])\n",
    "print(np.mean(r2List[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DO NOT RUN THIS\n",
    "r2ResultsList.append(r2List)\n",
    "lossResultList.append(lossList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8646386301877237, 0.9283815679475169, 0.9450812473918203, 0.6910083686443527, 0.9874655047743294, 0.887775569127897, 0.9688351197757891, 0.9500867504338024, 0.9602256016829921, 0.997828370056202, 0.9988321801383298, 0.9988337075067906, 0.9988341061461555, 0.9988320998042559, 0.9887341839176582]\n",
      "[0.86625421074764, 0.9316138929959028, 0.951153619443174, 0.6623161665708791, 0.9865322779509239, 0.9105202891101809, 0.969592211067584, 0.9567827183751209, 0.963917258810526, 0.998534796341051, 0.9993756071436712, 0.9993733523845045, 0.9993736456839012, 0.9993740816892989, 0.9911018717164113]\n",
      "[0.8495609824870629, 0.904121631666355, 0.9326948743376868, 0.6828483713957751, 0.9766347721411918, 0.8780140030682374, 0.961413228732725, 0.9442646982895891, 0.9560375734224605, 0.9874243764658291, 0.989024773054134, 0.9890203187716863, 0.9890006606720448, 0.9890212987270223, 0.9763105790164521]\n",
      "[0.8657691711963978, 0.9064977822145035, 0.940750552133279, 0.6707883639398146, 0.987226978825102, 0.9001408264284956, 0.969744069602104, 0.9530963098946572, 0.9640605814910246, 0.9968912076438604, 0.9980049985906431, 0.998003851149782, 0.998006817153342, 0.9979965877037286, 0.9869581444213741]\n",
      "0.9422624161592073\n"
     ]
    }
   ],
   "source": [
    "print(r2ResultsList[0][-1])\n",
    "print(r2ResultsList[1][-1])\n",
    "print(r2ResultsList[2][-1])\n",
    "print(r2ResultsList[3][-1])\n",
    "print(np.mean(r2ResultsList[3][-1]))\n",
    "# First both false\n",
    "# Second descriptor true, fingerprints false\n",
    "# Third descriptor false, fingerprints true\n",
    "# fourth all true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The values have been saved to 'r2Results.csv'.\n",
      "The values have been saved to 'lossResults.csv'.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Extract specific values from the three-dimensional list\n",
    "values_to_save = [r2ResultsList[i][-1] for i in range(4)]\n",
    "\n",
    "# Create a DataFrame from the extracted values\n",
    "df = pd.DataFrame(values_to_save)\n",
    "\n",
    "# Specify the file name\n",
    "file_name = 'r2Results.csv'\n",
    "\n",
    "# Write the DataFrame to a CSV file\n",
    "df.to_csv(file_name, index=False)\n",
    "\n",
    "print(f\"The values have been saved to '{file_name}'.\")\n",
    "\n",
    "\n",
    "\n",
    "# Create a DataFrame from the extracted values\n",
    "df = pd.DataFrame(lossResultList)\n",
    "\n",
    "# Specify the file name\n",
    "file_name = 'lossResults.csv'\n",
    "\n",
    "# Write the DataFrame to a CSV file\n",
    "df.to_csv(file_name, index=False)\n",
    "\n",
    "print(f\"The values have been saved to '{file_name}'.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
