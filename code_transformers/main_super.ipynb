{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import atomInSmiles\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from code_graphs.utility_functions import get_num_parameters , get_data_split_indices, scale_targets\n",
    "from classes import create_encoded_vector, PositionalEncoding, TransformerLayer\n",
    "\n",
    "\n",
    "import random\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will attempt to gather all the variant in this file then changing if descriptors are used etc by only changing bools instead of having four different files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "useRdkitDescriptors = True\n",
    "useFingerprints = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available: True\n",
      "device: cuda\n",
      "cuda version: 11.7\n",
      "gpu: NVIDIA GeForce GTX 970\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# check if cuda is available\n",
    "print('cuda available:', torch.cuda.is_available())\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('device:', \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    print('cuda version:', torch.version.cuda)\n",
    "    print('gpu:', torch.cuda.get_device_name(0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(132820, 21)\n",
      "(132820, 179)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "df = pd.read_csv(\"../data/smiles_and_targets.csv\")\n",
    "print(np.shape(df))\n",
    "\n",
    "if useRdkitDescriptors:\n",
    "    mol_descriptor = np.load(\"../data/mol_descriptors.npy\")\n",
    "if useFingerprints:\n",
    "    mol_fingerprint = np.load(\"../data/mol_morgan_fingerprints.npy\")\n",
    "print(mol_descriptor.shape)\n",
    "\n",
    "\n",
    "properties_names = ['A', 'B', 'C', 'mu', 'alfa', 'homo', 'lumo', 'gap', 'RÂ²', 'zpve', 'U0', 'U', 'H', 'G', 'Cv']\n",
    "\n",
    "x_smiles = df.smiles.values\n",
    "targetTensor = df.loc[:, properties_names].values # shape = (n_samples, n_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest word (max amount of tokens): 22\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenList = []\n",
    "for smile in x_smiles:\n",
    "    tokenList.append(atomInSmiles.encode(smile).split())\n",
    "#tokenList is target\n",
    "\n",
    "# find longest word (max tokens from one smile+1)\n",
    "maxTokenLength = 0\n",
    "for token in tokenList:\n",
    "    if len(token)>maxTokenLength:\n",
    "        maxTokenLength = len(token)\n",
    "print('Longest word (max amount of tokens):', maxTokenLength)\n",
    "\n",
    "# Give each token a index in a dictionary\n",
    "tokenDict = {}\n",
    "count = 1\n",
    "\n",
    "dictList = []\n",
    "for itokens in tokenList:\n",
    "    for itoke in itokens:\n",
    "        #print(itoke)\n",
    "        if tokenDict.get(itoke) == None:\n",
    "            tokenDict[itoke] = count\n",
    "        \n",
    "            # current = [itoke, count]\n",
    "            # dictList.append(current)\n",
    "            count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodedTokens = []\n",
    "for token in tokenList:\n",
    "    encodedTokens.append(create_encoded_vector(token,tokenDict,maxTokenLength))\n",
    "    \n",
    "encodedTokens = np.array(encodedTokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerNetwork(torch.nn.Module):\n",
    "    def __init__(self,hidden_channels,output_dim, vocab_size, embedding_dim, num_heads,maxTokenLength, nDescriptors,nFingerprints):\n",
    "        super().__init__()\n",
    "        # Embedd and add pos encoding to input\n",
    "        self.dropout_rate = 0.15\n",
    "        self.EmbeddingLayer = torch.nn.Embedding(num_embeddings=vocab_size,embedding_dim = embedding_dim , max_norm=True)\n",
    "        self.PositionalEncoding = PositionalEncoding(embedding_dim, maxTokenLength, dropout = self.dropout_rate)\n",
    "\n",
    "        self.TransEnc1 = TransformerLayer(embedding_dim,hidden_channels, num_heads, self.dropout_rate)\n",
    "        self.TransEnc2 = TransformerLayer(embedding_dim,hidden_channels, num_heads, self.dropout_rate)\n",
    "        self.TransEnc3 = TransformerLayer(embedding_dim,hidden_channels, num_heads, self.dropout_rate)\n",
    "        self.Pooling = torch.nn.AvgPool1d(kernel_size= 22)\n",
    "\n",
    "        self.DenseOut1 = torch.nn.Linear(embedding_dim+nDescriptors+nFingerprints,hidden_channels)\n",
    "        self.DenseOut2 = torch.nn.Linear(hidden_channels,output_dim)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "\n",
    "    def forward(self,x,descriptors,fingerprints):\n",
    "        x = self.EmbeddingLayer(x)\n",
    "        x = self.PositionalEncoding(x)\n",
    "        x = self.TransEnc1(x)\n",
    "        x = self.TransEnc2(x)\n",
    "        x = self.TransEnc3(x)\n",
    "        x = self.Pooling(x.permute((0,2,1))).permute((0,2,1))\n",
    "        x = torch.squeeze(x,axis=1)\n",
    "        x = torch.cat((x,descriptors,fingerprints),1)\n",
    "        #x = x[:,-1,:]\n",
    "\n",
    "\n",
    "        x = self.DenseOut1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.DenseOut2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 64\n",
    "## Create iterable dataset class:\n",
    "\n",
    "class datasetObject(Dataset):\n",
    "    def __init__(self,data,targets,descriptors,fingerprints):\n",
    "        self.data = data\n",
    "        self.descriptors = descriptors\n",
    "        self.targets = targets\n",
    "        self.fingerprints = fingerprints\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.data[index]\n",
    "        target = self.targets[index]\n",
    "        descriptor = self.descriptors[index]\n",
    "        fingerprint = self.fingerprints[index]\n",
    "        return (sample), (target), (descriptor), (fingerprint)\n",
    "\n",
    "trainSplit, validationSplit, testSplit = get_data_split_indices(len(encodedTokens),0.1,0.1)\n",
    "\n",
    "# Training\n",
    "encodedTrainData = torch.tensor(encodedTokens[trainSplit], dtype=torch.long, device=device)\n",
    "trainTargets = targetTensor[trainSplit]\n",
    "\n",
    "# Validation\n",
    "encodedValidationData = torch.tensor(encodedTokens[validationSplit], dtype=torch.long, device=device)\n",
    "validationTargets = targetTensor[validationSplit]\n",
    "\n",
    "# Test\n",
    "encodedTestData = torch.tensor(encodedTokens[testSplit], dtype=torch.long, device=device)\n",
    "testTargets = targetTensor[testSplit]\n",
    "\n",
    "# Descriptors\n",
    "trainDescriptors = mol_descriptor[trainSplit]\n",
    "valDescriptors = mol_descriptor[validationSplit]\n",
    "testDescriptors = mol_descriptor[testSplit]\n",
    "\n",
    "trainFingerprints = mol_fingerprint[trainSplit]\n",
    "valFingerprints = mol_fingerprint[validationSplit]\n",
    "testFingerprints = mol_fingerprint[testSplit]\n",
    "\n",
    "\n",
    "trainTargets, validationTargets, testTargets, scalerTargets = scale_targets(trainTargets,validationTargets,testTargets)\n",
    "\n",
    "\n",
    "# Normalize descriptors and fingerprints using minmax scaler\n",
    "minmax_scaler = MinMaxScaler()\n",
    "trainDescriptors = minmax_scaler.fit_transform(trainDescriptors)\n",
    "valDescriptors = minmax_scaler.transform(valDescriptors)\n",
    "testDescriptors = minmax_scaler.transform(testDescriptors)\n",
    "\n",
    "trainFingerprints = minmax_scaler.fit_transform(trainFingerprints)\n",
    "valFingerprints = minmax_scaler.transform(valFingerprints)\n",
    "testFingerprints = minmax_scaler.transform(testFingerprints)\n",
    "\n",
    "\n",
    "# Make into tensors\n",
    "#print(np.dtype(trainTargets[0,0]))\n",
    "trainTargets = torch.tensor(trainTargets,dtype=torch.float, device=device)\n",
    "validationTargets = torch.tensor(validationTargets,dtype=torch.float, device=device)\n",
    "testTargets = torch.tensor(testTargets,dtype=torch.float, device=device)\n",
    "\n",
    "# Decriptors to tensor\n",
    "trainDescriptors = torch.from_numpy(trainDescriptors).float().to(device)\n",
    "valDescriptors = torch.from_numpy(valDescriptors).float().to(device)\n",
    "testDescriptors = torch.from_numpy(testDescriptors).float().to(device)\n",
    "\n",
    "trainFingerprints = torch.from_numpy(trainFingerprints).float().to(device)\n",
    "valFingerprints = torch.from_numpy(valFingerprints).float().to(device)\n",
    "testFingerprints = torch.from_numpy(testFingerprints).float().to(device)\n",
    "\n",
    "trainingData = DataLoader(datasetObject(encodedTrainData,trainTargets,trainDescriptors,trainFingerprints),batch_size)\n",
    "testData = DataLoader(datasetObject(encodedTestData,testTargets,testDescriptors,testFingerprints),batch_size)\n",
    "validationData = DataLoader(datasetObject(encodedValidationData,validationTargets,valDescriptors,valFingerprints),batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Epoch: 0 of 1000 ####\n",
      " Training loss:\t0.1516107034424857\n",
      " Test loss:\t0.10946620427645169\n",
      " Validation loss:\t0.11236062416663536\n",
      "R2: [0.6751535254068515, 0.7980330020112607, 0.8704116243382779, 0.5803614569294574, 0.9593750554261404, 0.8023212046593694, 0.9323044439580188, 0.9020609109652724, 0.9137839223855866, 0.9821075226906368, 0.9775182401391977, 0.9769764671381153, 0.9755606283081092, 0.9772325479518412, 0.9630062577962136]\n",
      "avg R2 0.8857471206736232 \n",
      "\n",
      "#### Epoch: 1 of 1000 ####\n",
      " Training loss:\t0.09806891710624718\n",
      " Test loss:\t0.09182711747976449\n",
      " Validation loss:\t0.09444265182201679\n",
      "R2: [0.7427968612901131, 0.8273790439188885, 0.8931881470957561, 0.5928688758137132, 0.9711356375555588, 0.8221459942358064, 0.9448984874064241, 0.9143144305176102, 0.9271585130135881, 0.9924326892961547, 0.9881570614464915, 0.9883232918983516, 0.9885297136888572, 0.9880653275625946, 0.9762892347125925]\n",
      "avg R2 0.9038455539635 \n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 66\u001b[0m\n\u001b[0;32m     64\u001b[0m r2 \u001b[39m=\u001b[39m [\u001b[39m0\u001b[39m]\u001b[39m*\u001b[39md_target\n\u001b[0;32m     65\u001b[0m \u001b[39mfor\u001b[39;00m \u001b[39minput\u001b[39m, targets, descriptor, fingerprint \u001b[39min\u001b[39;00m testData:\n\u001b[1;32m---> 66\u001b[0m       tempTest, tempR2 \u001b[39m=\u001b[39m inferNew(\u001b[39minput\u001b[39;49m,targets,descriptor,fingerprint)\n\u001b[0;32m     67\u001b[0m       test \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tempTest\n\u001b[0;32m     68\u001b[0m       \u001b[39m# Add r2 for each variable\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[17], line 45\u001b[0m, in \u001b[0;36minferNew\u001b[1;34m(data_in, targets, descriptors, fingerprints)\u001b[0m\n\u001b[0;32m     43\u001b[0m     r2_scores \u001b[39m=\u001b[39m []\n\u001b[0;32m     44\u001b[0m     \u001b[39mfor\u001b[39;00m item \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(targets\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]):\n\u001b[1;32m---> 45\u001b[0m         r2 \u001b[39m=\u001b[39m r2_score(targets[:, item]\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy(), out[:, item]\u001b[39m.\u001b[39;49mcpu()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy())\n\u001b[0;32m     46\u001b[0m         r2_scores\u001b[39m.\u001b[39mappend(r2)\n\u001b[0;32m     48\u001b[0m \u001b[39mreturn\u001b[39;00m loss, r2_scores\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "hidden_channels = 1024\n",
    "embedding_dim = 128\n",
    "d_target = 15\n",
    "nHeads = 8\n",
    "learningRate = 0.0005\n",
    "nDescriptors = len(mol_descriptor[0])\n",
    "nFingerprints = len(mol_fingerprint[0])\n",
    "vocab_size = len(tokenDict)+1\n",
    "torch.manual_seed(12345)\n",
    "\n",
    "model = TransformerNetwork(hidden_channels,d_target,vocab_size, embedding_dim, nHeads,maxTokenLength,nDescriptors,nFingerprints).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learningRate, weight_decay=5e-4)\n",
    "# decay learning rate\n",
    "decayRate = 0.95\n",
    "lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=decayRate)\n",
    "criterion = torch.nn.MSELoss().to(device)\n",
    "\n",
    "def train(data_in, targets, descriptors,fingerprints):\n",
    "      model.train()\n",
    "      optimizer.zero_grad()  # Clear gradients.\n",
    "      out = model(data_in,descriptors,fingerprints).to(device)\n",
    "      loss = criterion(out, targets)\n",
    "      \n",
    "      loss.backward()  # Derive gradients.\n",
    "      optimizer.step()  # Update parameters based on gradients.\n",
    "      return loss\n",
    "\n",
    "def infer(data_in, targets, descriptors, fingerprints):\n",
    "      with torch.no_grad():\n",
    "            model.eval()\n",
    "            out = model(data_in, descriptors,fingerprints)\n",
    "            loss = criterion(out, targets)\n",
    "            r2 = r2_score(targets.cpu().numpy(), out.cpu().numpy())\n",
    "      return loss, r2\n",
    "\n",
    "# This calculates r2 for each target separately \n",
    "def inferNew(data_in, targets, descriptors,fingerprints):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        out = model(data_in, descriptors,fingerprints)\n",
    "        loss = criterion(out, targets)\n",
    "\n",
    "        r2_scores = []\n",
    "        for item in range(targets.shape[1]):\n",
    "            r2 = r2_score(targets[:, item].cpu().detach().numpy(), out[:, item].cpu().detach().numpy())\n",
    "            r2_scores.append(r2)\n",
    "\n",
    "    return loss, r2_scores\n",
    "\n",
    "\n",
    "earlyStop = False\n",
    "stopTolerance = 3\n",
    "minDiff = 0.01\n",
    "counter = 0\n",
    "epoch = 0\n",
    "lossList = []\n",
    "r2List = []\n",
    "while not earlyStop and epoch < 1000:\n",
    "      loss = 0\n",
    "      for input,targets,descriptor,fingerprint in trainingData:\n",
    "            loss += train(input,targets,descriptor,fingerprint).detach()\n",
    "      \n",
    "      test = 0\n",
    "      r2 = [0]*d_target\n",
    "      for input, targets, descriptor, fingerprint in testData:\n",
    "            tempTest, tempR2 = inferNew(input,targets,descriptor,fingerprint)\n",
    "            test += tempTest\n",
    "            # Add r2 for each variable\n",
    "            r2 = [x + y for x, y in zip(r2, tempR2)]\n",
    "      # Average over batch\n",
    "      r2 = [x / len(testData) for x in r2]\n",
    "      r2List.append(r2)\n",
    "\n",
    "      \n",
    "      val = 0\n",
    "      for input,targets,descriptor,fingerprint in validationData:\n",
    "            tempVal,tempR2 = infer(input,targets, descriptor, fingerprint)\n",
    "            val += tempVal\n",
    "      \n",
    "      lr_scheduler.step()\n",
    "\n",
    "      # Save loss AS PYTHON NUMBER (not tensor) in list\n",
    "      lossList.append([loss.item()/len(trainingData), test.item()/len(testData), val.item()/len(validationData)])\n",
    "\n",
    "      # Early stopping\n",
    "      if (lossList[epoch][2]-lossList[epoch][0]) > minDiff:\n",
    "            counter += 1\n",
    "      else:\n",
    "            counter = 0\n",
    "            if counter >= stopTolerance:\n",
    "                  earlyStop = True\n",
    "\n",
    "      print(f\"#### Epoch: {epoch} of 1000 ####\\n Training loss:\\t{lossList[epoch][0]}\\n Test loss:\\t{lossList[epoch][1]}\\n Validation loss:\\t{lossList[epoch][2]}\")\n",
    "      print('R2:', r2List[epoch])\n",
    "      print('avg R2',np.mean(r2List[epoch]),'\\n')\n",
    "      epoch+=1 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
