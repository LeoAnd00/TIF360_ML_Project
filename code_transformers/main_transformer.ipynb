{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TIF360 Project"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main source: https://www.kaggle.com/code/rmonge/predicting-molecule-properties-based-on-its-smiles/notebook"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UPDATED ON 15/05-2023  11:20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import atomInSmiles\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from code_graphs.utility_functions import get_num_parameters , get_data_split_indices, scale_targets\n",
    "\n",
    "\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input is list of strings on the form ['a','b','c']\n",
    "# where the string are tokens\n",
    "# Dictionary is the dictionary containign all possible tokens\n",
    "# and an index for them\n",
    "# MaxTokenLength is the max amount of tokens any input creates\n",
    "def create_Onehot_Matrix(input, dictionary,maxTokenLength):\n",
    "    # Create a matrix of zeros\n",
    "    onehot_Matrix = np.zeros((len(dictionary),maxTokenLength))\n",
    "    \n",
    "    # Change value in right place to one\n",
    "    keyCount = 0\n",
    "    for key in input:\n",
    "        onehot_Matrix[dictionary[key],keyCount] = 1\n",
    "        keyCount+=1\n",
    "\n",
    "    # Return it\n",
    "    return(onehot_Matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Input is list of strings on the form ['a','b','c']\n",
    "# where the string are tokens\n",
    "# Dictionary is the dictionary containign all possible tokens\n",
    "# and an index for them\n",
    "# MaxTokenLength is the max amount of tokens any input creates\n",
    "\n",
    "# THIS VERSION RETURNS TORCH TENSOR\n",
    "\n",
    "#Version taht returns tensor\n",
    "def create_encoded_tensor(input, dictionary,maxTokenLength):\n",
    "    # Create a matrix of zeros\n",
    "    #encoded_vector = np.zeros((1,maxTokenLength))\n",
    "    encoded_tensor = np.zeros(maxTokenLength,dtype=np.int32)\n",
    "    \n",
    "    # Change value in right place to one\n",
    "    keyCount = 0\n",
    "    for key in input:\n",
    "        encoded_tensor[keyCount] = dictionary[key]\n",
    "        keyCount+=1\n",
    "    \n",
    "    # encoded_tensor = np.expand_dims(encoded_tensor,axis=1)\n",
    "    return encoded_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(132820, 21)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "df = pd.read_csv(\"../data/smiles_and_targets.csv\")\n",
    "print(np.shape(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "properties_names = ['A', 'B', 'C', 'mu', 'alfa', 'homo', 'lumo', 'gap', 'RÂ²', 'zpve', 'U0', 'U', 'H', 'G', 'Cv']\n",
    "\n",
    "x_smiles = df.smiles.values\n",
    "targetTensor = df.loc[:, properties_names].values # shape = (n_samples, n_properties)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest word (max amount of tokens): 22\n"
     ]
    }
   ],
   "source": [
    "# tokenize all smiles\n",
    "#import atomInSmiles\n",
    "\n",
    "tokenList = []\n",
    "for smile in x_smiles:\n",
    "    tokenList.append(atomInSmiles.encode(smile).split())\n",
    "#tokenList is target\n",
    "\n",
    "# find longest word (max tokens from one smile+1)\n",
    "maxTokenLength = 0\n",
    "for token in tokenList:\n",
    "    if len(token)>maxTokenLength:\n",
    "        maxTokenLength = len(token)\n",
    "print('Longest word (max amount of tokens):', maxTokenLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Give each token a index in a dictionary\n",
    "tokenDict = {}\n",
    "count = 1\n",
    "\n",
    "dictList = []\n",
    "for itokens in tokenList:\n",
    "    for itoke in itokens:\n",
    "        #print(itoke)\n",
    "        if tokenDict.get(itoke) == None:\n",
    "            tokenDict[itoke] = count\n",
    "        \n",
    "            # current = [itoke, count]\n",
    "            # dictList.append(current)\n",
    "            count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodedTokens = []\n",
    "for token in tokenList:\n",
    "    encodedTokens.append(create_encoded_tensor(token,tokenDict,maxTokenLength))\n",
    "    \n",
    "encodedTokens = np.array(encodedTokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From attention is all you need\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "    def __init__(self, d_model, max_len,dropout):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) *\n",
    "                             -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x = x + torch.tensor(self.pe[:, :x.size(1)], \n",
    "        #                  requires_grad=False)\n",
    "        x = x + self.pe[:x.size(0), :].detach()\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TransformerLayer(torch.nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_channels, num_heads):\n",
    "        super().__init__()\n",
    "        self.Attention = torch.nn.MultiheadAttention(embedding_dim,num_heads=num_heads,dropout=0.15)\n",
    "        self.Norm1 = torch.nn.LayerNorm(embedding_dim)\n",
    "        self.Dense1 = torch.nn.Linear(embedding_dim,hidden_channels)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.Dense2 = torch.nn.Linear(hidden_channels,embedding_dim)\n",
    "        \n",
    "\n",
    "        self.Norm2 = torch.nn.LayerNorm(embedding_dim)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        addNormX = x\n",
    "        #print(x.shape)\n",
    "        x, _ = self.Attention(x,x,x)\n",
    "        #print('attention output',x.shape)\n",
    "        x = self.Norm1(x + addNormX)\n",
    "        #print('norm + input',x.shape)\n",
    "        addNormX = x\n",
    "        x = self.Dense1(x)\n",
    "        x = self.relu(x)\n",
    "        #print('first dense output',x.shape)\n",
    "        x = self.Dense2(x)\n",
    "        #print('second dense output',x.shape)\n",
    "        x = self.Norm2(x + addNormX)\n",
    "        #print(x.shape)\n",
    "\n",
    " \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerNetwork(torch.nn.Module):\n",
    "    def __init__(self,hidden_channels,output_dim, vocab_size, embedding_dim, num_heads,maxTokenLength):\n",
    "        super().__init__()\n",
    "        # Embedd and add pos encoding to input\n",
    "        self.EmbeddingLayer = torch.nn.Embedding(num_embeddings=vocab_size,embedding_dim = embedding_dim , max_norm=True)\n",
    "        self.PositionalEncoding = PositionalEncoding(embedding_dim, maxTokenLength, dropout = 0.15)\n",
    "\n",
    "        self.TransEnc1 = TransformerLayer(embedding_dim,hidden_channels,num_heads)\n",
    "        self.TransEnc2 = TransformerLayer(embedding_dim,hidden_channels,num_heads)\n",
    "        self.TransEnc3 = TransformerLayer(embedding_dim,hidden_channels,num_heads)\n",
    "        self.Pooling = torch.nn.AvgPool1d(kernel_size= 22)\n",
    "\n",
    "        self.DenseOut1 = torch.nn.Linear(embedding_dim,hidden_channels)\n",
    "        self.DenseOut2 = torch.nn.Linear(hidden_channels,output_dim)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.EmbeddingLayer(x)\n",
    "        x = self.PositionalEncoding(x)\n",
    "        x = self.TransEnc1(x)\n",
    "        x = self.TransEnc2(x)\n",
    "        x = self.TransEnc3(x)\n",
    "        x = self.Pooling(x.permute((0,2,1))).permute((0,2,1))\n",
    "        x = torch.squeeze(x,axis=1)\n",
    "        #x = x[:,-1,:]\n",
    "        x = self.DenseOut1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.DenseOut2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available: True\n",
      "device: cuda\n",
      "NVIDIA GeForce RTX 3080\n"
     ]
    }
   ],
   "source": [
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"device:\", \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "batch_size = 64\n",
    "## Create iterable dataset class:\n",
    "\n",
    "class datasetObject(Dataset):\n",
    "    def __init__(self,data,targets):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.data[index]\n",
    "        target = self.targets[index]\n",
    "        return (sample), (target)\n",
    "\n",
    "trainSplit, validationSplit, testSplit = get_data_split_indices(len(encodedTokens),0.1,0.1)\n",
    "\n",
    "# Training\n",
    "encodedTrainData = torch.tensor(encodedTokens[trainSplit], dtype=torch.long, device=device)\n",
    "trainTargets = targetTensor[trainSplit]\n",
    "\n",
    "# Validation\n",
    "encodedValidationData = torch.tensor(encodedTokens[validationSplit], dtype=torch.long, device=device)\n",
    "validationTargets = targetTensor[validationSplit]\n",
    "\n",
    "# Test\n",
    "encodedTestData = torch.tensor(encodedTokens[testSplit], dtype=torch.long, device=device)\n",
    "testTargets = targetTensor[testSplit]\n",
    "\n",
    "trainTargets, validationTargets, testTargets, scalerTargets = scale_targets(trainTargets,validationTargets,testTargets)\n",
    "\n",
    "trainTargets = torch.tensor(trainTargets,dtype=torch.float,device=device)\n",
    "validationTargets = torch.tensor(validationTargets,dtype=torch.float, device=device)\n",
    "testTargets = torch.tensor(testTargets,dtype=torch.float, device=device)\n",
    "\n",
    "trainingData = DataLoader(datasetObject(encodedTrainData,trainTargets),batch_size,shuffle=False)\n",
    "testData = DataLoader(datasetObject(encodedTestData,testTargets),batch_size,shuffle=False)\n",
    "validationData = DataLoader(datasetObject(encodedValidationData,validationTargets),batch_size,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'hidden_channels': 128, 'embedding_dim': 128, 'nHeads': 4},\n",
       " {'hidden_channels': 128, 'embedding_dim': 128, 'nHeads': 8},\n",
       " {'hidden_channels': 128, 'embedding_dim': 256, 'nHeads': 4},\n",
       " {'hidden_channels': 128, 'embedding_dim': 256, 'nHeads': 8},\n",
       " {'hidden_channels': 256, 'embedding_dim': 128, 'nHeads': 4},\n",
       " {'hidden_channels': 256, 'embedding_dim': 128, 'nHeads': 8},\n",
       " {'hidden_channels': 256, 'embedding_dim': 256, 'nHeads': 4},\n",
       " {'hidden_channels': 256, 'embedding_dim': 256, 'nHeads': 8},\n",
       " {'hidden_channels': 512, 'embedding_dim': 128, 'nHeads': 4},\n",
       " {'hidden_channels': 512, 'embedding_dim': 128, 'nHeads': 8},\n",
       " {'hidden_channels': 512, 'embedding_dim': 256, 'nHeads': 4},\n",
       " {'hidden_channels': 512, 'embedding_dim': 256, 'nHeads': 8}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creates list of dictionary for many runs of different parametervalues\n",
    "heads = [4, 8]\n",
    "embeddings = [128, 256]\n",
    "hiddenChannels = [128, 256, 512]\n",
    "parameterList = []\n",
    "\n",
    "for channel in hiddenChannels:\n",
    "    for embed in embeddings:\n",
    "        for ihead in heads:\n",
    "            tempDict = {}\n",
    "            tempDict['hidden_channels'] = channel\n",
    "            tempDict['embedding_dim'] = embed\n",
    "            tempDict['nHeads'] = ihead\n",
    "            parameterList.append(tempDict)\n",
    "\n",
    "parameterList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#### Epoch: 0 of 100 ####\n",
      " Training loss:\t0.372864210483613\n",
      " Test loss:\t0.1676819507892315\n",
      " Validation loss:\t0.16987477816068208\n",
      "R2:  0.8258428307584319\n",
      "#### Epoch: 1 of 100 ####\n",
      " Training loss:\t0.16223219860612018\n",
      " Test loss:\t0.14424280019906852\n",
      " Validation loss:\t0.14550171448634222\n",
      "R2:  0.8493322696446207\n",
      "#### Epoch: 2 of 100 ####\n",
      " Training loss:\t0.14286631492876561\n",
      " Test loss:\t0.1233750581741333\n",
      " Validation loss:\t0.12572072102473333\n",
      "R2:  0.8712227951597025\n",
      "#### Epoch: 3 of 100 ####\n",
      " Training loss:\t0.13359503427375732\n",
      " Test loss:\t0.11846773441021259\n",
      " Validation loss:\t0.12058992569263165\n",
      "R2:  0.8762032248524214\n",
      "#### Epoch: 4 of 100 ####\n",
      " Training loss:\t0.1277505046699795\n",
      " Test loss:\t0.12289679967440091\n",
      " Validation loss:\t0.1254733892587515\n",
      "R2:  0.8718648183095528\n",
      "#### Epoch: 5 of 100 ####\n",
      " Training loss:\t0.12388179992639035\n",
      " Test loss:\t0.12303654047159049\n",
      " Validation loss:\t0.12575082595531756\n",
      "R2:  0.8715546746555929\n",
      "#### Epoch: 6 of 100 ####\n",
      " Training loss:\t0.12062647829279707\n",
      " Test loss:\t0.11328678864699143\n",
      " Validation loss:\t0.11618475730602558\n",
      "R2:  0.881582437338776\n",
      "#### Epoch: 7 of 100 ####\n",
      " Training loss:\t0.11821277854960772\n",
      " Test loss:\t0.11101917120126578\n",
      " Validation loss:\t0.11367426468775822\n",
      "R2:  0.8839898491114668\n",
      "#### Epoch: 8 of 100 ####\n",
      " Training loss:\t0.11634186360579093\n",
      " Test loss:\t0.10856954868023212\n",
      " Validation loss:\t0.11121766383831318\n",
      "R2:  0.8866269258251531\n",
      "#### Epoch: 9 of 100 ####\n",
      " Training loss:\t0.11491717408034975\n",
      " Test loss:\t0.11328575244316688\n",
      " Validation loss:\t0.11592049782092755\n",
      "R2:  0.8819437498299348\n",
      "#### Epoch: 10 of 100 ####\n",
      " Training loss:\t0.11333812366832387\n",
      " Test loss:\t0.11438765892615685\n",
      " Validation loss:\t0.11700520148644081\n",
      "R2:  0.8807814162901898\n",
      "#### Epoch: 11 of 100 ####\n",
      " Training loss:\t0.11233835605021918\n",
      " Test loss:\t0.10921989037440373\n",
      " Validation loss:\t0.1117028731566209\n",
      "R2:  0.8859200771420989\n",
      "#### Epoch: 12 of 100 ####\n",
      " Training loss:\t0.11145626759399929\n",
      " Test loss:\t0.1079014906516442\n",
      " Validation loss:\t0.11056288389059213\n",
      "R2:  0.8873996666124003\n",
      "#### Epoch: 13 of 100 ####\n",
      " Training loss:\t0.1105834666969258\n",
      " Test loss:\t0.10521171643183781\n",
      " Validation loss:\t0.10782883717463566\n",
      "R2:  0.8900315171079721\n",
      "#### Epoch: 14 of 100 ####\n",
      " Training loss:\t0.10979263700683888\n",
      " Test loss:\t0.1095343828201294\n",
      " Validation loss:\t0.11218953132629395\n",
      "R2:  0.8858868560912057\n",
      "#### Epoch: 15 of 100 ####\n",
      " Training loss:\t0.10906034374294477\n",
      " Test loss:\t0.10788586506476769\n",
      " Validation loss:\t0.11051224745236911\n",
      "R2:  0.8873829316210404\n",
      "#### Epoch: 16 of 100 ####\n",
      " Training loss:\t0.10846220104899627\n",
      " Test loss:\t0.10453033447265625\n",
      " Validation loss:\t0.10713847783895639\n",
      "R2:  0.8909409652355441\n",
      "#### Epoch: 17 of 100 ####\n",
      " Training loss:\t0.10780319774244149\n",
      " Test loss:\t0.10895925301771897\n",
      " Validation loss:\t0.11151584295126107\n",
      "R2:  0.8864178101575472\n",
      "#### Epoch: 18 of 100 ####\n",
      " Training loss:\t0.10734104291995127\n",
      " Test loss:\t0.10553647921635555\n",
      " Validation loss:\t0.10814000093019925\n",
      "R2:  0.8901810123219523\n",
      "#### Epoch: 19 of 100 ####\n",
      " Training loss:\t0.1067847263949381\n",
      " Test loss:\t0.10467865833869347\n",
      " Validation loss:\t0.10731137715853177\n",
      "R2:  0.890779590879258\n",
      "#### Epoch: 20 of 100 ####\n",
      " Training loss:\t0.10628256433201008\n",
      " Test loss:\t0.1045144979770367\n",
      " Validation loss:\t0.10722331817333515\n",
      "R2:  0.8909078569604615\n",
      "#### Epoch: 21 of 100 ####\n",
      " Training loss:\t0.10591288089465119\n",
      " Test loss:\t0.10524922150831956\n",
      " Validation loss:\t0.10797413495870736\n",
      "R2:  0.8903415666697077\n",
      "#### Epoch: 22 of 100 ####\n",
      " Training loss:\t0.10559073763393192\n",
      " Test loss:\t0.1045831166780912\n",
      " Validation loss:\t0.10718589562636155\n",
      "R2:  0.890933059567052\n",
      "#### Epoch: 23 of 100 ####\n",
      " Training loss:\t0.10508752830914171\n",
      " Test loss:\t0.10254775560819186\n",
      " Validation loss:\t0.10524333440340482\n",
      "R2:  0.892835481425561\n",
      "#### Epoch: 24 of 100 ####\n",
      " Training loss:\t0.10465577164017817\n",
      " Test loss:\t0.10104462733635536\n",
      " Validation loss:\t0.10380197488344632\n",
      "R2:  0.8943222858330876\n",
      "#### Epoch: 25 of 100 ####\n",
      " Training loss:\t0.10415361876375771\n",
      " Test loss:\t0.10008617547842172\n",
      " Validation loss:\t0.10284173488616943\n",
      "R2:  0.8953215725692937\n",
      "#### Epoch: 26 of 100 ####\n",
      " Training loss:\t0.10388856046103627\n",
      " Test loss:\t0.10237314150883602\n",
      " Validation loss:\t0.10506503398601826\n",
      "R2:  0.8929175070975616\n",
      "#### Epoch: 27 of 100 ####\n",
      " Training loss:\t0.1035087075598049\n",
      " Test loss:\t0.10376027914193961\n",
      " Validation loss:\t0.10633261387164776\n",
      "R2:  0.8916998984665383\n",
      "#### Epoch: 28 of 100 ####\n",
      " Training loss:\t0.10342934532096629\n",
      " Test loss:\t0.10201434905712421\n",
      " Validation loss:\t0.10458572094257061\n",
      "R2:  0.8933762164183767\n",
      "#### Epoch: 29 of 100 ####\n",
      " Training loss:\t0.10299219617206314\n",
      " Test loss:\t0.10208505850571853\n",
      " Validation loss:\t0.10480711093315712\n",
      "R2:  0.8933195373150655\n",
      "#### Epoch: 30 of 100 ####\n",
      " Training loss:\t0.10266512894329\n",
      " Test loss:\t0.10406213540297288\n",
      " Validation loss:\t0.10667906357691838\n",
      "R2:  0.8912626620482652\n",
      "#### Epoch: 31 of 100 ####\n",
      " Training loss:\t0.10267490338733726\n",
      " Test loss:\t0.09959998497596154\n",
      " Validation loss:\t0.10252706821148212\n",
      "R2:  0.895731565976196\n",
      "#### Epoch: 32 of 100 ####\n",
      " Training loss:\t0.10233844754783451\n",
      " Test loss:\t0.10207245900080754\n",
      " Validation loss:\t0.10481242950146015\n",
      "R2:  0.8932278927344067\n",
      "#### Epoch: 33 of 100 ####\n",
      " Training loss:\t0.10195500272214592\n",
      " Test loss:\t0.09951687776125394\n",
      " Validation loss:\t0.10233008861541748\n",
      "R2:  0.8957921202729099\n",
      "#### Epoch: 34 of 100 ####\n",
      " Training loss:\t0.10163644219937631\n",
      " Test loss:\t0.09949003733121432\n",
      " Validation loss:\t0.10226893424987793\n",
      "R2:  0.8958666516346382\n",
      "#### Epoch: 35 of 100 ####\n",
      " Training loss:\t0.10135720911548483\n",
      " Test loss:\t0.10047904344705436\n",
      " Validation loss:\t0.10309434854067288\n",
      "R2:  0.8949680631059101\n",
      "#### Epoch: 36 of 100 ####\n",
      " Training loss:\t0.10124099979481016\n",
      " Test loss:\t0.09934804072746864\n",
      " Validation loss:\t0.102203369140625\n",
      "R2:  0.8960142754584998\n",
      "#### Epoch: 37 of 100 ####\n",
      " Training loss:\t0.1010788120037816\n",
      " Test loss:\t0.09980264993814322\n",
      " Validation loss:\t0.10272374519935021\n",
      "R2:  0.895464717034591\n",
      "#### Epoch: 38 of 100 ####\n",
      " Training loss:\t0.10096162891330523\n",
      " Test loss:\t0.09978981201465313\n",
      " Validation loss:\t0.1026990688764132\n",
      "R2:  0.8954153635246246\n",
      "#### Epoch: 39 of 100 ####\n",
      " Training loss:\t0.10058782073405045\n",
      " Test loss:\t0.09943055189572848\n",
      " Validation loss:\t0.10217508902916542\n",
      "R2:  0.895987670734145\n",
      "#### Epoch: 40 of 100 ####\n",
      " Training loss:\t0.10038528580180425\n",
      " Test loss:\t0.09962793496938852\n",
      " Validation loss:\t0.1025082881634052\n",
      "R2:  0.8956921399188654\n",
      "#### Epoch: 41 of 100 ####\n",
      " Training loss:\t0.10028977368266952\n",
      " Test loss:\t0.09809482097625732\n",
      " Validation loss:\t0.10093854940854587\n",
      "R2:  0.8972420312789503\n",
      "#### Epoch: 42 of 100 ####\n",
      " Training loss:\t0.10019500567064739\n",
      " Test loss:\t0.09852979733393742\n",
      " Validation loss:\t0.10134034890394944\n",
      "R2:  0.8968140328396583\n",
      "#### Epoch: 43 of 100 ####\n",
      " Training loss:\t0.10004899531943764\n",
      " Test loss:\t0.09835328505589412\n",
      " Validation loss:\t0.10128794266627385\n",
      "R2:  0.8970018336771364\n",
      "#### Epoch: 44 of 100 ####\n",
      " Training loss:\t0.09990665038474564\n",
      " Test loss:\t0.09776865519010104\n",
      " Validation loss:\t0.10064962277045617\n",
      "R2:  0.8975805517188616\n",
      "#### Epoch: 45 of 100 ####\n",
      " Training loss:\t0.099622961844686\n",
      " Test loss:\t0.096518507370582\n",
      " Validation loss:\t0.0993592830804678\n",
      "R2:  0.8988573623370459\n",
      "#### Epoch: 46 of 100 ####\n",
      " Training loss:\t0.099467976275587\n",
      " Test loss:\t0.09788572788238525\n",
      " Validation loss:\t0.10075855255126953\n",
      "R2:  0.8975089355060345\n",
      "#### Epoch: 47 of 100 ####\n",
      " Training loss:\t0.09937465973175987\n",
      " Test loss:\t0.09591476733867939\n",
      " Validation loss:\t0.09880651877476619\n",
      "R2:  0.8995725877212833\n",
      "#### Epoch: 48 of 100 ####\n",
      " Training loss:\t0.0992162200358171\n",
      " Test loss:\t0.09683299981630765\n",
      " Validation loss:\t0.09974409066713773\n",
      "R2:  0.8985232691616776\n",
      "#### Epoch: 49 of 100 ####\n",
      " Training loss:\t0.09888503725165564\n",
      " Test loss:\t0.09684185798351581\n",
      " Validation loss:\t0.09981783536764291\n",
      "R2:  0.8985748290020118\n",
      "#### Epoch: 50 of 100 ####\n",
      " Training loss:\t0.09891839346061766\n",
      " Test loss:\t0.09558979364541861\n",
      " Validation loss:\t0.09860825538635254\n",
      "R2:  0.8998192977026847\n",
      "#### Epoch: 51 of 100 ####\n",
      " Training loss:\t0.09877159306975748\n",
      " Test loss:\t0.09722774762373704\n",
      " Validation loss:\t0.10013867341555081\n",
      "R2:  0.8983327113031362\n",
      "#### Epoch: 52 of 100 ####\n",
      " Training loss:\t0.09870349348919137\n",
      " Test loss:\t0.09656127599569467\n",
      " Validation loss:\t0.09958484539618859\n",
      "R2:  0.8989713791658562\n",
      "#### Epoch: 53 of 100 ####\n",
      " Training loss:\t0.09847825869675647\n",
      " Test loss:\t0.09667403881366436\n",
      " Validation loss:\t0.09957772034865159\n",
      "R2:  0.8988463032194571\n",
      "#### Epoch: 54 of 100 ####\n",
      " Training loss:\t0.09854457609485533\n",
      " Test loss:\t0.0950308029468243\n",
      " Validation loss:\t0.09801892133859488\n",
      "R2:  0.9004930238905187\n",
      "#### Epoch: 55 of 100 ####\n",
      " Training loss:\t0.09830972302900173\n",
      " Test loss:\t0.09637499772585355\n",
      " Validation loss:\t0.09936524354494534\n",
      "R2:  0.899099601575612\n",
      "#### Epoch: 56 of 100 ####\n",
      " Training loss:\t0.09817145690366778\n",
      " Test loss:\t0.09568309783935547\n",
      " Validation loss:\t0.09863397708305946\n",
      "R2:  0.899825382799572\n",
      "#### Epoch: 57 of 100 ####\n",
      " Training loss:\t0.09804144944863316\n",
      " Test loss:\t0.09582241681905893\n",
      " Validation loss:\t0.09880631703596848\n",
      "R2:  0.8997307188759858\n",
      "#### Epoch: 58 of 100 ####\n",
      " Training loss:\t0.09800858012457198\n",
      " Test loss:\t0.0950637413905217\n",
      " Validation loss:\t0.09814894199371338\n",
      "R2:  0.9004245032864951\n",
      "#### Epoch: 59 of 100 ####\n",
      " Training loss:\t0.09795961603926005\n",
      " Test loss:\t0.09583231119009164\n",
      " Validation loss:\t0.09890861694629376\n",
      "R2:  0.8996972527500168\n",
      "#### Epoch: 60 of 100 ####\n",
      " Training loss:\t0.0977969505498382\n",
      " Test loss:\t0.09573526565845196\n",
      " Validation loss:\t0.09871576382563664\n",
      "R2:  0.8998328492264742\n",
      "#### Epoch: 61 of 100 ####\n",
      " Training loss:\t0.09774451396466736\n",
      " Test loss:\t0.09531763883737418\n",
      " Validation loss:\t0.09831020465263954\n",
      "R2:  0.9001518538393385\n",
      "#### Epoch: 62 of 100 ####\n",
      " Training loss:\t0.09752085006502577\n",
      " Test loss:\t0.09557102276728703\n",
      " Validation loss:\t0.09861866327432486\n",
      "R2:  0.899922597961094\n",
      "#### Epoch: 63 of 100 ####\n",
      " Training loss:\t0.09753330696928149\n",
      " Test loss:\t0.09505411294790414\n",
      " Validation loss:\t0.09805676570305458\n",
      "R2:  0.900430939396531\n",
      "#### Epoch: 64 of 100 ####\n",
      " Training loss:\t0.09755503305966849\n",
      " Test loss:\t0.09485142047588642\n",
      " Validation loss:\t0.09788414148183969\n",
      "R2:  0.9006394533665818\n",
      "#### Epoch: 65 of 100 ####\n",
      " Training loss:\t0.09734970542912308\n",
      " Test loss:\t0.09538032458378719\n",
      " Validation loss:\t0.09839796102963962\n",
      "R2:  0.9001110716578036\n",
      "#### Epoch: 66 of 100 ####\n",
      " Training loss:\t0.09728433424130324\n",
      " Test loss:\t0.0947572451371413\n",
      " Validation loss:\t0.09777356111086331\n",
      "R2:  0.9007242947267282\n",
      "#### Epoch: 67 of 100 ####\n",
      " Training loss:\t0.09727306239652318\n",
      " Test loss:\t0.09478508509122409\n",
      " Validation loss:\t0.09779000282287598\n",
      "R2:  0.9007635684773564\n",
      "#### Epoch: 68 of 100 ####\n",
      " Training loss:\t0.09730278993499775\n",
      " Test loss:\t0.09455463519463173\n",
      " Validation loss:\t0.09761465512789212\n",
      "R2:  0.9009221755788295\n",
      "#### Epoch: 69 of 100 ####\n",
      " Training loss:\t0.09707298944830392\n",
      " Test loss:\t0.09423973010136531\n",
      " Validation loss:\t0.09727530296032245\n",
      "R2:  0.9012781020556763\n",
      "#### Epoch: 70 of 100 ####\n",
      " Training loss:\t0.09714032654874229\n",
      " Test loss:\t0.09388517416440524\n",
      " Validation loss:\t0.09695262175339919\n",
      "R2:  0.9016361770228415\n",
      "#### Epoch: 71 of 100 ####\n",
      " Training loss:\t0.09703483788239389\n",
      " Test loss:\t0.09363908951099102\n",
      " Validation loss:\t0.09670780255244328\n",
      "R2:  0.9018411602134737\n",
      "#### Epoch: 72 of 100 ####\n",
      " Training loss:\t0.09680164757440925\n",
      " Test loss:\t0.09382826548356277\n",
      " Validation loss:\t0.09683325657477745\n",
      "R2:  0.9017185424401453\n",
      "#### Epoch: 73 of 100 ####\n",
      " Training loss:\t0.09689190501121783\n",
      " Test loss:\t0.0940939738200261\n",
      " Validation loss:\t0.09713775378007156\n",
      "R2:  0.9014206099862444\n",
      "#### Epoch: 74 of 100 ####\n",
      " Training loss:\t0.09679839555073186\n",
      " Test loss:\t0.09343570929307204\n",
      " Validation loss:\t0.09646554176624005\n",
      "R2:  0.9021021494891978\n",
      "#### Epoch: 75 of 100 ####\n",
      " Training loss:\t0.09682397078778786\n",
      " Test loss:\t0.09364455479841965\n",
      " Validation loss:\t0.09672123652238113\n",
      "R2:  0.9018240064455528\n",
      "#### Epoch: 76 of 100 ####\n",
      " Training loss:\t0.09682922547011401\n",
      " Test loss:\t0.09357809103452243\n",
      " Validation loss:\t0.09668204417595497\n",
      "R2:  0.9019153313858461\n",
      "#### Epoch: 77 of 100 ####\n",
      " Training loss:\t0.09666388670699128\n",
      " Test loss:\t0.09404966464409462\n",
      " Validation loss:\t0.0970907578101525\n",
      "R2:  0.9014630383610996\n",
      "#### Epoch: 78 of 100 ####\n",
      " Training loss:\t0.0966774919246924\n",
      " Test loss:\t0.09309831949380729\n",
      " Validation loss:\t0.09617781639099121\n",
      "R2:  0.9024068652062038\n",
      "#### Epoch: 79 of 100 ####\n",
      " Training loss:\t0.09668391329347908\n",
      " Test loss:\t0.09376247112567608\n",
      " Validation loss:\t0.09689251276162955\n",
      "R2:  0.9017169219251127\n",
      "#### Epoch: 80 of 100 ####\n",
      " Training loss:\t0.0965958789915008\n",
      " Test loss:\t0.09376085721529447\n",
      " Validation loss:\t0.09682990037477933\n",
      "R2:  0.9017309650508044\n",
      "#### Epoch: 81 of 100 ####\n",
      " Training loss:\t0.09660958526078511\n",
      " Test loss:\t0.0930559176665086\n",
      " Validation loss:\t0.09616509767679068\n",
      "R2:  0.902457337500992\n",
      "#### Epoch: 82 of 100 ####\n",
      " Training loss:\t0.0964883417339658\n",
      " Test loss:\t0.09344347623678353\n",
      " Validation loss:\t0.09653330766237699\n",
      "R2:  0.9020555763048357\n",
      "#### Epoch: 83 of 100 ####\n",
      " Training loss:\t0.09643136701405873\n",
      " Test loss:\t0.09365153312683105\n",
      " Validation loss:\t0.09673642195188083\n",
      "R2:  0.9018601342916613\n",
      "#### Epoch: 84 of 100 ####\n",
      " Training loss:\t0.09642059124068708\n",
      " Test loss:\t0.09349736800560585\n",
      " Validation loss:\t0.09656082666837253\n",
      "R2:  0.9019791672556922\n",
      "#### Epoch: 85 of 100 ####\n",
      " Training loss:\t0.09623713668464969\n",
      " Test loss:\t0.09341601225046012\n",
      " Validation loss:\t0.09647298776186429\n",
      "R2:  0.9020726382623079\n",
      "#### Epoch: 86 of 100 ####\n",
      " Training loss:\t0.09634307548986867\n",
      " Test loss:\t0.09329020060025729\n",
      " Validation loss:\t0.09636915647066556\n",
      "R2:  0.9022057666631915\n",
      "#### Epoch: 87 of 100 ####\n",
      " Training loss:\t0.09626596394538305\n",
      " Test loss:\t0.09386599063873291\n",
      " Validation loss:\t0.09696621161240798\n",
      "R2:  0.9016466339925746\n",
      "#### Epoch: 88 of 100 ####\n",
      " Training loss:\t0.09627752057189758\n",
      " Test loss:\t0.09344017505645752\n",
      " Validation loss:\t0.0965414047241211\n",
      "R2:  0.9020653400172989\n",
      "#### Epoch: 89 of 100 ####\n",
      " Training loss:\t0.09627051126662685\n",
      " Test loss:\t0.09361350536346436\n",
      " Validation loss:\t0.09671334119943473\n",
      "R2:  0.9018925808003968\n",
      "#### Epoch: 90 of 100 ####\n",
      " Training loss:\t0.09625991003689889\n",
      " Test loss:\t0.0934005333827092\n",
      " Validation loss:\t0.09650008495037372\n",
      "R2:  0.9020920744680466\n",
      "#### Epoch: 91 of 100 ####\n",
      " Training loss:\t0.0962029445034994\n",
      " Test loss:\t0.09332463374504676\n",
      " Validation loss:\t0.09642690878648025\n",
      "R2:  0.9021697239785488\n",
      "#### Epoch: 92 of 100 ####\n",
      " Training loss:\t0.09616794390968354\n",
      " Test loss:\t0.09342035880455604\n",
      " Validation loss:\t0.09651900254763089\n",
      "R2:  0.9020505187953686\n",
      "#### Epoch: 93 of 100 ####\n",
      " Training loss:\t0.09617193085396598\n",
      " Test loss:\t0.0932170427762545\n",
      " Validation loss:\t0.09633138546576867\n",
      "R2:  0.9022649293695123\n",
      "#### Epoch: 94 of 100 ####\n",
      " Training loss:\t0.09606059956880739\n",
      " Test loss:\t0.09328017784998967\n",
      " Validation loss:\t0.09641519876626822\n",
      "R2:  0.902207121474392\n",
      "#### Epoch: 95 of 100 ####\n",
      " Training loss:\t0.09613829904977705\n",
      " Test loss:\t0.09319252234238845\n",
      " Validation loss:\t0.09627309212317833\n",
      "R2:  0.9023068985838911\n",
      "#### Epoch: 96 of 100 ####\n",
      " Training loss:\t0.09620605872956332\n",
      " Test loss:\t0.09340144120729886\n",
      " Validation loss:\t0.09649335421048678\n",
      "R2:  0.9020860829359709\n",
      "#### Epoch: 97 of 100 ####\n",
      " Training loss:\t0.09601600826109553\n",
      " Test loss:\t0.09338242274064285\n",
      " Validation loss:\t0.09649222630720872\n",
      "R2:  0.9021018356719738\n",
      "#### Epoch: 98 of 100 ####\n",
      " Training loss:\t0.09608260125442128\n",
      " Test loss:\t0.0933614052259005\n",
      " Validation loss:\t0.0964438456755418\n",
      "R2:  0.9021086236038839\n",
      "#### Epoch: 99 of 100 ####\n",
      " Training loss:\t0.09610429059877804\n",
      " Test loss:\t0.09355560632852408\n",
      " Validation loss:\t0.09663134354811448\n",
      "R2:  0.9019181068996635\n",
      "\n",
      " NO EARLY STOPPING\n",
      "Training Loss:  0.09601600826109553 \tTesting loss:  0.09338242274064285 \tValidation Loss:  0.09649222630720872 \n",
      "\n",
      "\n",
      "R2:  0.9021018356719738\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "hidden_channels = 512\n",
    "embedding_dim = 128\n",
    "learningRate = 0.001\n",
    "d_target = 15\n",
    "vocab_size = len(tokenDict)+1\n",
    "nHeads = 4\n",
    "\n",
    "# paramList = [{hidden_channels : 128, embedding_dim : 32, nHeads : 16},\n",
    "#              {hidden_channels : 128, embedding_dim : 32, nHeads : 16},\n",
    "#              {hidden_channels : 128, embedding_dim : 32, nHeads : 16},\n",
    "#              {hidden_channels : 128, embedding_dim : 32, nHeads : 16}]\n",
    "\n",
    "\n",
    "torch.manual_seed(12345)\n",
    "\n",
    "model = TransformerNetwork(hidden_channels,d_target,vocab_size, embedding_dim, nHeads, maxTokenLength).to(device) \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learningRate, weight_decay=5e-4)\n",
    "# decay learning rate\n",
    "decayRate = 0.95\n",
    "lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=decayRate)\n",
    "criterion = torch.nn.MSELoss().to(device)\n",
    "\n",
    "def train(data_in, targets):\n",
    "      model.train()\n",
    "      optimizer.zero_grad()  # Clear gradients.\n",
    "      out = model(data_in)\n",
    "      loss = criterion(out, targets)\n",
    "      \n",
    "      loss.backward()  # Derive gradients.\n",
    "      optimizer.step()  # Update parameters based on gradients.\n",
    "      return loss\n",
    "\n",
    "def infer(data_in, targets):\n",
    "      with torch.no_grad():\n",
    "            model.eval()\n",
    "            out = model(data_in)\n",
    "            loss = criterion(out, targets)\n",
    "            r2 = r2_score(targets.cpu().detach().numpy(), out.cpu().detach().numpy())\n",
    "\n",
    "      return loss, r2\n",
    "\n",
    "parameterTest = False\n",
    "earlyStop = False\n",
    "stopTolerance = 3\n",
    "minDiff = 0.01\n",
    "epoch = 0\n",
    "finalLoss = []\n",
    "\n",
    "if parameterTest == False:\n",
    "      lossList = []\n",
    "      r2List = []\n",
    "      counter = 0\n",
    "      r2array = np.zeros(d_target)\n",
    "      while not earlyStop and epoch < 100:\n",
    "            loss = 0\n",
    "            for input,targets in trainingData:\n",
    "                  loss += train(input,targets).detach()\n",
    "            \n",
    "            test = 0\n",
    "            r2 = 0\n",
    "            for input, targets in testData:\n",
    "                  tempTest, tempR2 = infer(input,targets)\n",
    "                  test += tempTest\n",
    "                  r2 += tempR2\n",
    "\n",
    "            r2List.append(r2.item()/len(testData))\n",
    "\n",
    "            val = 0\n",
    "            for input,targets in validationData:\n",
    "                  tempVal,tempR2 = infer(input,targets)\n",
    "                  val += tempVal.detach()\n",
    "            \n",
    "            lr_scheduler.step()\n",
    "            lossList.append([loss.item()/len(trainingData), test.item()/len(testData), val.item()/len(validationData)])\n",
    "            # Early stopping\n",
    "            if (lossList[epoch][2]-lossList[epoch][0]) > minDiff:\n",
    "                  counter += 1\n",
    "                  if counter >= stopTolerance:\n",
    "                        earlyStop = True         \n",
    "            else:\n",
    "                  counter = 0\n",
    "            print(f\"#### Epoch: {epoch} of 100 ####\\n Training loss:\\t{lossList[epoch][0]}\\n Test loss:\\t{lossList[epoch][1]}\\n Validation loss:\\t{lossList[epoch][2]}\")\n",
    "            print('R2: ',r2List[epoch])\n",
    "            epoch+=1   \n",
    "      if earlyStop == False:\n",
    "                  epoch -= stopTolerance\n",
    "                  print('\\n NO EARLY STOPPING')\n",
    "                  print('Training Loss: ', lossList[epoch][0], '\\tTesting loss: ', lossList[epoch][1], '\\tValidation Loss: ', lossList[epoch][2],'\\n\\n')\n",
    "                  print('R2: ',r2List[epoch])\n",
    "                  finalLoss.append(lossList[epoch])       \n",
    "else: # THIS LOOP RUNS WHEN DOING PARAMETER TESTING CHANGE BOOL TO TURN OFF\n",
    "      c = 0\n",
    "      for currentParam in parameterList:\n",
    "            c+=1\n",
    "            print(\"network counter:\", c)\n",
    "            hidden_channels = currentParam['hidden_channels']\n",
    "            embedding_dim = currentParam['embedding_dim']\n",
    "            nHeads = currentParam['nHeads']\n",
    "            \n",
    "            model = TransformerNetwork(hidden_channels=hidden_channels, output_dim=d_target, embedding_dim=embedding_dim,\n",
    "                                       vocab_size=vocab_size, num_heads=nHeads, maxTokenLength=maxTokenLength).to(device) \n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=learningRate, weight_decay=5e-4)\n",
    "            # decay learning rate\n",
    "            decayRate = 0.94\n",
    "            lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=decayRate)\n",
    "            criterion = torch.nn.MSELoss().to(device)\n",
    "            \n",
    "            r2List = []\n",
    "            lossList = []\n",
    "            counter = 0\n",
    "            epoch = 0\n",
    "            earlyStop = False\n",
    "            while not earlyStop and epoch < 100:\n",
    "                  loss = 0\n",
    "                  for input,targets in trainingData:\n",
    "                        loss += train(input,targets).detach()\n",
    "                  \n",
    "                  test = 0\n",
    "                  r2 = 0\n",
    "                  for input, targets in testData:\n",
    "                        tempTest, tempR2 = infer(input,targets)\n",
    "                        test += tempTest\n",
    "                        r2 += tempR2\n",
    "                  \n",
    "                  r2List.append(r2.item()/len(testData))\n",
    "\n",
    "                  val = 0\n",
    "                  for input,targets in validationData:\n",
    "                        tempVal,tempR2 = infer(input,targets)\n",
    "                        val += tempVal.detach()\n",
    "                  \n",
    "                  lr_scheduler.step()\n",
    "                  lossList.append([loss.item()/len(trainingData), test.item()/len(testData), val.item()/len(validationData)])\n",
    "                  # Early stopping\n",
    "                  if (lossList[epoch][2]-lossList[epoch][0]) > minDiff:\n",
    "                        counter += 1\n",
    "                        if counter >= stopTolerance:\n",
    "                              print('\\n EARLY STOPPING')\n",
    "                              print('Hidden_channels = ',currentParam['hidden_channels'],'\\tembedding_dim = ',currentParam['embedding_dim'], '\\tnHeads = ',currentParam['nHeads'])\n",
    "                              print('Training Loss: ', lossList[epoch][0], '\\tTesting loss: ', lossList[epoch][1], '\\tValidation Loss: ', lossList[epoch][2],'\\n\\n')\n",
    "                              print('R2: ',r2List[epoch])\n",
    "                              finalLoss.append(lossList[epoch])\n",
    "                              \n",
    "\n",
    "                              earlyStop = True\n",
    "                  else:\n",
    "                        counter = 0       \n",
    "\n",
    "                  if epoch%10 == 0:\n",
    "                        print(f\"#### Epoch: {epoch} of 100 ####\\n Training loss:\\t{lossList[epoch][0]}\\n Test loss:\\t{lossList[epoch][1]}\\n Validation loss:\\t{lossList[epoch][2]}\")\n",
    "                  epoch += 1\n",
    "            if earlyStop == False:\n",
    "                  epoch -= 1\n",
    "                  print('\\n NO EARLY STOPPING')\n",
    "                  print('Hidden_channels = ',currentParam['hidden_channels'],'\\tembedding_dim = ',currentParam['embedding_dim'], '\\tnHeads = ',currentParam['nHeads'])\n",
    "                  print('Training Loss: ', lossList[epoch][0], '\\tTesting loss: ', lossList[epoch][1], '\\tValidation Loss: ', lossList[epoch][2],'\\n\\n')\n",
    "                  print('R2: ',r2List[epoch])\n",
    "                  finalLoss.append(lossList[epoch])\n",
    "                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.09601600826109553, 0.09338242274064285, 0.09649222630720872]]\n",
      "[0.8258428307584319, 0.8493322696446207, 0.8712227951597025, 0.8762032248524214, 0.8718648183095528, 0.8715546746555929, 0.881582437338776, 0.8839898491114668, 0.8866269258251531, 0.8819437498299348, 0.8807814162901898, 0.8859200771420989, 0.8873996666124003, 0.8900315171079721, 0.8858868560912057, 0.8873829316210404, 0.8909409652355441, 0.8864178101575472, 0.8901810123219523, 0.890779590879258, 0.8909078569604615, 0.8903415666697077, 0.890933059567052, 0.892835481425561, 0.8943222858330876, 0.8953215725692937, 0.8929175070975616, 0.8916998984665383, 0.8933762164183767, 0.8933195373150655, 0.8912626620482652, 0.895731565976196, 0.8932278927344067, 0.8957921202729099, 0.8958666516346382, 0.8949680631059101, 0.8960142754584998, 0.895464717034591, 0.8954153635246246, 0.895987670734145, 0.8956921399188654, 0.8972420312789503, 0.8968140328396583, 0.8970018336771364, 0.8975805517188616, 0.8988573623370459, 0.8975089355060345, 0.8995725877212833, 0.8985232691616776, 0.8985748290020118, 0.8998192977026847, 0.8983327113031362, 0.8989713791658562, 0.8988463032194571, 0.9004930238905187, 0.899099601575612, 0.899825382799572, 0.8997307188759858, 0.9004245032864951, 0.8996972527500168, 0.8998328492264742, 0.9001518538393385, 0.899922597961094, 0.900430939396531, 0.9006394533665818, 0.9001110716578036, 0.9007242947267282, 0.9007635684773564, 0.9009221755788295, 0.9012781020556763, 0.9016361770228415, 0.9018411602134737, 0.9017185424401453, 0.9014206099862444, 0.9021021494891978, 0.9018240064455528, 0.9019153313858461, 0.9014630383610996, 0.9024068652062038, 0.9017169219251127, 0.9017309650508044, 0.902457337500992, 0.9020555763048357, 0.9018601342916613, 0.9019791672556922, 0.9020726382623079, 0.9022057666631915, 0.9016466339925746, 0.9020653400172989, 0.9018925808003968, 0.9020920744680466, 0.9021697239785488, 0.9020505187953686, 0.9022649293695123, 0.902207121474392, 0.9023068985838911, 0.9020860829359709, 0.9021018356719738, 0.9021086236038839, 0.9019181068996635]\n"
     ]
    }
   ],
   "source": [
    "print(finalLoss)\n",
    "print(r2List)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
