{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TIF360 Project"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main source: https://www.kaggle.com/code/rmonge/predicting-molecule-properties-based-on-its-smiles/notebook"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UPDATED ON 15/05-2023  11:20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input is list of strings on the form ['a','b','c']\n",
    "# where the string are tokens\n",
    "# Dictionary is the dictionary containign all possible tokens\n",
    "# and an index for them\n",
    "# MaxTokenLength is the max amount of tokens any input creates\n",
    "def create_Onehot_Matrix(input, dictionary,maxTokenLength):\n",
    "    # Create a matrix of zeros\n",
    "    onehot_Matrix = np.zeros((len(dictionary),maxTokenLength))\n",
    "    \n",
    "    # Change value in right place to one\n",
    "    keyCount = 0\n",
    "    for key in input:\n",
    "        onehot_Matrix[dictionary[key],keyCount] = 1\n",
    "        keyCount+=1\n",
    "\n",
    "    # Return it\n",
    "    return(onehot_Matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Input is list of strings on the form ['a','b','c']\n",
    "# where the string are tokens\n",
    "# Dictionary is the dictionary containign all possible tokens\n",
    "# and an index for them\n",
    "# MaxTokenLength is the max amount of tokens any input creates\n",
    "def create_encoded_vector(input, dictionary,maxTokenLength):\n",
    "    # Create a matrix of zeros\n",
    "    encoded_vector = np.zeros((1,maxTokenLength))\n",
    "    print(encoded_vector)\n",
    "    encoded_vector[0] = 10\n",
    "    print(encoded_vector.shape)\n",
    "    \n",
    "    \n",
    "    # Change value in right place to one\n",
    "    keyCount = 0\n",
    "    for key in input:\n",
    "        encoded_vector[keyCount] = dictionary[0,key]\n",
    "        keyCount+=1\n",
    "    \n",
    "    return encoded_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDict = {}\n",
    "testDict['a'] = 1\n",
    "testDict['b'] = 2\n",
    "testDict['c'] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0.]]\n",
      "(1, 5)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "(0, 'b')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m create_encoded_vector([\u001b[39m'\u001b[39;49m\u001b[39mb\u001b[39;49m\u001b[39m'\u001b[39;49m,\u001b[39m'\u001b[39;49m\u001b[39mc\u001b[39;49m\u001b[39m'\u001b[39;49m],testDict,\u001b[39m5\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[26], line 15\u001b[0m, in \u001b[0;36mcreate_encoded_vector\u001b[0;34m(input, dictionary, maxTokenLength)\u001b[0m\n\u001b[1;32m     13\u001b[0m keyCount \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     14\u001b[0m \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m \u001b[39minput\u001b[39m:\n\u001b[0;32m---> 15\u001b[0m     encoded_vector[keyCount] \u001b[39m=\u001b[39m dictionary[\u001b[39m0\u001b[39;49m,key]\n\u001b[1;32m     16\u001b[0m     keyCount\u001b[39m+\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m     18\u001b[0m \u001b[39mreturn\u001b[39;00m encoded_vector\n",
      "\u001b[0;31mKeyError\u001b[0m: (0, 'b')"
     ]
    }
   ],
   "source": [
    "create_encoded_vector(['b','c'],testDict,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(132820, 21)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "df = pd.read_csv(\"../data/smiles_and_targets.csv\")\n",
    "print(np.shape(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "properties_names = ['A', 'B', 'C', 'mu', 'alfa', 'homo', 'lumo', 'gap', 'RÂ²', 'zpve', 'U0', 'U', 'H', 'G', 'Cv']\n",
    "\n",
    "x_smiles = df.smiles.values\n",
    "y = df.loc[:, properties_names].values  # shape = (n_samples, n_properties)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model for all targets at once"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CC(C)C#N\n",
      "['[CH3;!R;C]', '[CH;!R;CCC]', '(', '[CH3;!R;C]', ')', '[C;!R;CN]', '#', '[N;!R;C]']\n",
      "(132820,)\n"
     ]
    }
   ],
   "source": [
    "import atomInSmiles\n",
    "\n",
    "\n",
    "testSmile = x_smiles[8]\n",
    "print(testSmile)\n",
    "tokens = atomInSmiles.encode(testSmile)\n",
    "tokens = tokens.split()\n",
    "print(tokens)\n",
    "print(x_smiles.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest word (max amount of tokens): 22\n"
     ]
    }
   ],
   "source": [
    "# tokenize all smiles\n",
    "import atomInSmiles\n",
    "\n",
    "tokenList = []\n",
    "for smile in x_smiles:\n",
    "    tokenList.append(atomInSmiles.encode(smile).split())\n",
    "#tokenList is target\n",
    "\n",
    "# find longest word (max tokens from one smile+1)\n",
    "maxTokenLength = 0\n",
    "for token in tokenList:\n",
    "    if len(token)>maxTokenLength:\n",
    "        maxTokenLength = len(token)\n",
    "print('Longest word (max amount of tokens):', maxTokenLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Give each token a index in a dictionary\n",
    "tokenDict = {}\n",
    "count = 1\n",
    "\n",
    "dictList = []\n",
    "for itokens in tokenList:\n",
    "    for itoke in itokens:\n",
    "        #print(itoke)\n",
    "        if tokenDict.get(itoke) == None:\n",
    "            tokenDict[itoke] = count\n",
    "        \n",
    "            # current = [itoke, count]\n",
    "            # dictList.append(current)\n",
    "            count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 149 is out of bounds for axis 0 with size 149",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m inputData \u001b[39m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m tokenList:\n\u001b[0;32m----> 4\u001b[0m     inputData\u001b[39m.\u001b[39mappend(create_Onehot_Matrix(token,tokenDict,maxTokenLength))\n\u001b[1;32m      5\u001b[0m inputData \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(inputData)\n",
      "Cell \u001b[0;32mIn[2], line 13\u001b[0m, in \u001b[0;36mcreate_Onehot_Matrix\u001b[0;34m(input, dictionary, maxTokenLength)\u001b[0m\n\u001b[1;32m     11\u001b[0m keyCount \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     12\u001b[0m \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m \u001b[39minput\u001b[39m:\n\u001b[0;32m---> 13\u001b[0m     onehot_Matrix[dictionary[key],keyCount] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     14\u001b[0m     keyCount\u001b[39m+\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[39m# Return it\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 149 is out of bounds for axis 0 with size 149"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create one hot encoded matrix containing all the data made into encoded matrices\n",
    "inputData = []\n",
    "for token in tokenList:\n",
    "    inputData.append(create_Onehot_Matrix(token,tokenDict,maxTokenLength))\n",
    "inputData = np.asarray(inputData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "(132820, 149, 22)\n"
     ]
    }
   ],
   "source": [
    "print(inputData[0])\n",
    "print(inputData.shape)\n",
    "print(inputData.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class PositionalEncoding(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-np.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class TransformerLayer(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.Attention = torch.nn.MultiheadAttention(data_features,num_heads=3,dropout=0.15)\n",
    "        self.Norm1 = torch.nn.LayerNorm(data_features)\n",
    "        self.Dense1 = torch.nn.Linear(hidden_channels)\n",
    "        self.Norm2 = torch.nn.LayerNorm(hidden_channels)\n",
    "        self.Dense2 = torch.nn.Linear(hidden_channels)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        addNormX = x\n",
    "        x = self.Attention(x)\n",
    "        x = self.Norm1(x + addNormX)\n",
    "        addNormX = x\n",
    "        x = self.Dense1(x)\n",
    "        x = self.Dense2(x)\n",
    "        x = self.Norm2(x + addNormX)\n",
    "\n",
    "       \n",
    " \n",
    "        return x\n",
    "    \n",
    "class TransformerNetwork(torch.nn.Module):\n",
    "    def __init__(self,hidden_channels,output_dim):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.EmbeddingLayer = torch.nn.Embedding(num_embeddings=148,embedding_dim = 600 , max_norm=True)\n",
    "        self.PositionalEncoding = PositionalEncoding(d_model=1024)\n",
    "        self.TransEnc1 = TransformerLayer(hidden_channels)\n",
    "        \n",
    "        self.Pooling = torch.nn.AvgPool1d()\n",
    "\n",
    "        self.DenseOut1 = torch.nn.Linear(hidden_channels)\n",
    "        self.DenseOut2 = torch.nn.Linear(output_dim)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.EmbeddingLayer(x)\n",
    "        x = self.PositionalEncoding(x)\n",
    "        x = self.TransEnc1(x)\n",
    "        x = self.Pooling(x)\n",
    "        x = self.DenseOut1(x)\n",
    "        x = self.DenseOut2(x)\n",
    "\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
