{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TIF360 Project"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main source: https://www.kaggle.com/code/rmonge/predicting-molecule-properties-based-on-its-smiles/notebook"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UPDATED ON 15/05-2023  11:20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import atomInSmiles\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from code_graphs.utility_functions import get_num_parameters , get_data_split_indices, scale_targets\n",
    "\n",
    "\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input is list of strings on the form ['a','b','c']\n",
    "# where the string are tokens\n",
    "# Dictionary is the dictionary containign all possible tokens\n",
    "# and an index for them\n",
    "# MaxTokenLength is the max amount of tokens any input creates\n",
    "def create_Onehot_Matrix(input, dictionary,maxTokenLength):\n",
    "    # Create a matrix of zeros\n",
    "    onehot_Matrix = np.zeros((len(dictionary),maxTokenLength))\n",
    "    \n",
    "    # Change value in right place to one\n",
    "    keyCount = 0\n",
    "    for key in input:\n",
    "        onehot_Matrix[dictionary[key],keyCount] = 1\n",
    "        keyCount+=1\n",
    "\n",
    "    # Return it\n",
    "    return(onehot_Matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Input is list of strings on the form ['a','b','c']\n",
    "# where the string are tokens\n",
    "# Dictionary is the dictionary containign all possible tokens\n",
    "# and an index for them\n",
    "# MaxTokenLength is the max amount of tokens any input creates\n",
    "\n",
    "# THIS VERSION RETURNS TORCH TENSOR\n",
    "\n",
    "#Version taht returns tensor\n",
    "def create_encoded_tensor(input, dictionary,maxTokenLength):\n",
    "    # Create a matrix of zeros\n",
    "    #encoded_vector = np.zeros((1,maxTokenLength))\n",
    "    encoded_tensor = np.zeros(maxTokenLength,dtype=np.int32)\n",
    "    \n",
    "    # Change value in right place to one\n",
    "    keyCount = 0\n",
    "    for key in input:\n",
    "        encoded_tensor[keyCount] = dictionary[key]\n",
    "        keyCount+=1\n",
    "    \n",
    "    # encoded_tensor = np.expand_dims(encoded_tensor,axis=1)\n",
    "    return encoded_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(132820, 21)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "df = pd.read_csv(\"../data/smiles_and_targets.csv\")\n",
    "print(np.shape(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "properties_names = ['A', 'B', 'C', 'mu', 'alfa', 'homo', 'lumo', 'gap', 'RÂ²', 'zpve', 'U0', 'U', 'H', 'G', 'Cv']\n",
    "\n",
    "x_smiles = df.smiles.values\n",
    "targetTensor = df.loc[:, properties_names].values # shape = (n_samples, n_properties)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longest word (max amount of tokens): 22\n"
     ]
    }
   ],
   "source": [
    "# tokenize all smiles\n",
    "#import atomInSmiles\n",
    "\n",
    "tokenList = []\n",
    "for smile in x_smiles:\n",
    "    tokenList.append(atomInSmiles.encode(smile).split())\n",
    "#tokenList is target\n",
    "\n",
    "# find longest word (max tokens from one smile+1)\n",
    "maxTokenLength = 0\n",
    "for token in tokenList:\n",
    "    if len(token)>maxTokenLength:\n",
    "        maxTokenLength = len(token)\n",
    "print('Longest word (max amount of tokens):', maxTokenLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Give each token a index in a dictionary\n",
    "tokenDict = {}\n",
    "count = 1\n",
    "\n",
    "dictList = []\n",
    "for itokens in tokenList:\n",
    "    for itoke in itokens:\n",
    "        #print(itoke)\n",
    "        if tokenDict.get(itoke) == None:\n",
    "            tokenDict[itoke] = count\n",
    "        \n",
    "            # current = [itoke, count]\n",
    "            # dictList.append(current)\n",
    "            count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "encodedTokens = []\n",
    "for token in tokenList:\n",
    "    encodedTokens.append(create_encoded_tensor(token,tokenDict,maxTokenLength))\n",
    "    \n",
    "encodedTokens = np.array(encodedTokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From attention is all you need\n",
    "class PositionalEncoding(nn.Module):\n",
    "    \"Implement the PE function.\"\n",
    "    def __init__(self, d_model, max_len,dropout):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) *\n",
    "                             -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x = x + torch.tensor(self.pe[:, :x.size(1)], \n",
    "        #                  requires_grad=False)\n",
    "        x = x + self.pe[:x.size(0), :].detach()\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TransformerLayer(torch.nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_channels, num_heads):\n",
    "        super().__init__()\n",
    "        self.Attention = torch.nn.MultiheadAttention(embedding_dim,num_heads=num_heads,dropout=0.15)\n",
    "        self.Norm1 = torch.nn.LayerNorm(embedding_dim)\n",
    "        self.Dense1 = torch.nn.Linear(embedding_dim,hidden_channels)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.Dense2 = torch.nn.Linear(hidden_channels,embedding_dim)\n",
    "        \n",
    "\n",
    "        self.Norm2 = torch.nn.LayerNorm(embedding_dim)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        addNormX = x\n",
    "        #print(x.shape)\n",
    "        x, _ = self.Attention(x,x,x)\n",
    "        #print('attention output',x.shape)\n",
    "        x = self.Norm1(x + addNormX)\n",
    "        #print('norm + input',x.shape)\n",
    "        addNormX = x\n",
    "        x = self.Dense1(x)\n",
    "        x = self.relu(x)\n",
    "        #print('first dense output',x.shape)\n",
    "        x = self.Dense2(x)\n",
    "        #print('second dense output',x.shape)\n",
    "        x = self.Norm2(x + addNormX)\n",
    "        #print(x.shape)\n",
    "\n",
    " \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerNetwork(torch.nn.Module):\n",
    "    def __init__(self,hidden_channels,output_dim, vocab_size, embedding_dim, num_heads,maxTokenLength):\n",
    "        super().__init__()\n",
    "        # Embedd and add pos encoding to input\n",
    "        self.EmbeddingLayer = torch.nn.Embedding(num_embeddings=vocab_size,embedding_dim = embedding_dim , max_norm=True)\n",
    "        self.PositionalEncoding = PositionalEncoding(embedding_dim, maxTokenLength, dropout = 0.15)\n",
    "\n",
    "        self.TransEnc1 = TransformerLayer(embedding_dim,hidden_channels,num_heads)\n",
    "        self.TransEnc2 = TransformerLayer(embedding_dim,hidden_channels,num_heads)\n",
    "        self.TransEnc3 = TransformerLayer(embedding_dim,hidden_channels,num_heads)\n",
    "        self.Pooling = torch.nn.AvgPool1d(kernel_size= 22)\n",
    "\n",
    "        self.DenseOut1 = torch.nn.Linear(embedding_dim,hidden_channels)\n",
    "        self.DenseOut2 = torch.nn.Linear(hidden_channels,output_dim)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.EmbeddingLayer(x)\n",
    "        x = self.PositionalEncoding(x)\n",
    "        x = self.TransEnc1(x)\n",
    "        x = self.TransEnc2(x)\n",
    "        x = self.TransEnc3(x)\n",
    "        x = self.Pooling(x.permute((0,2,1))).permute((0,2,1))\n",
    "        x = torch.squeeze(x,axis=1)\n",
    "        #x = x[:,-1,:]\n",
    "        x = self.DenseOut1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.DenseOut2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available: True\n",
      "device: cuda\n",
      "NVIDIA GeForce RTX 3080\n"
     ]
    }
   ],
   "source": [
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"device:\", \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "batch_size = 64\n",
    "## Create iterable dataset class:\n",
    "\n",
    "class datasetObject(Dataset):\n",
    "    def __init__(self,data,targets):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, index):\n",
    "        sample = self.data[index]\n",
    "        target = self.targets[index]\n",
    "        return (sample), (target)\n",
    "\n",
    "trainSplit, validationSplit, testSplit = get_data_split_indices(len(encodedTokens),0.1,0.1)\n",
    "\n",
    "# Training\n",
    "encodedTrainData = torch.tensor(encodedTokens[trainSplit], dtype=torch.long, device=device)\n",
    "trainTargets = targetTensor[trainSplit]\n",
    "\n",
    "# Validation\n",
    "encodedValidationData = torch.tensor(encodedTokens[validationSplit], dtype=torch.long, device=device)\n",
    "validationTargets = targetTensor[validationSplit]\n",
    "\n",
    "# Test\n",
    "encodedTestData = torch.tensor(encodedTokens[testSplit], dtype=torch.long, device=device)\n",
    "testTargets = targetTensor[testSplit]\n",
    "\n",
    "trainTargets, validationTargets, testTargets, scalerTargets = scale_targets(trainTargets,validationTargets,testTargets)\n",
    "\n",
    "trainTargets = torch.tensor(trainTargets,dtype=torch.float,device=device)\n",
    "validationTargets = torch.tensor(validationTargets,dtype=torch.float, device=device)\n",
    "testTargets = torch.tensor(testTargets,dtype=torch.float, device=device)\n",
    "\n",
    "trainingData = DataLoader(datasetObject(encodedTrainData,trainTargets),batch_size,shuffle=True)\n",
    "testData = DataLoader(datasetObject(encodedTestData,testTargets),batch_size,shuffle=False)\n",
    "validationData = DataLoader(datasetObject(encodedValidationData,validationTargets),batch_size,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'hidden_channels': 64, 'embedding_dim': 32, 'nHeads': 8},\n",
       " {'hidden_channels': 64, 'embedding_dim': 32, 'nHeads': 16},\n",
       " {'hidden_channels': 64, 'embedding_dim': 64, 'nHeads': 8},\n",
       " {'hidden_channels': 64, 'embedding_dim': 64, 'nHeads': 16},\n",
       " {'hidden_channels': 64, 'embedding_dim': 128, 'nHeads': 8},\n",
       " {'hidden_channels': 64, 'embedding_dim': 128, 'nHeads': 16},\n",
       " {'hidden_channels': 128, 'embedding_dim': 32, 'nHeads': 8},\n",
       " {'hidden_channels': 128, 'embedding_dim': 32, 'nHeads': 16},\n",
       " {'hidden_channels': 128, 'embedding_dim': 64, 'nHeads': 8},\n",
       " {'hidden_channels': 128, 'embedding_dim': 64, 'nHeads': 16},\n",
       " {'hidden_channels': 128, 'embedding_dim': 128, 'nHeads': 8},\n",
       " {'hidden_channels': 128, 'embedding_dim': 128, 'nHeads': 16},\n",
       " {'hidden_channels': 256, 'embedding_dim': 32, 'nHeads': 8},\n",
       " {'hidden_channels': 256, 'embedding_dim': 32, 'nHeads': 16},\n",
       " {'hidden_channels': 256, 'embedding_dim': 64, 'nHeads': 8},\n",
       " {'hidden_channels': 256, 'embedding_dim': 64, 'nHeads': 16},\n",
       " {'hidden_channels': 256, 'embedding_dim': 128, 'nHeads': 8},\n",
       " {'hidden_channels': 256, 'embedding_dim': 128, 'nHeads': 16}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creates list of dictionary for many runs of different parametervalues\n",
    "heads = [8,16]\n",
    "embeddings = [32,64,128]\n",
    "hiddenChannels = [64,128,256]\n",
    "parameterList = []\n",
    "\n",
    "for channel in hiddenChannels:\n",
    "    for embed in embeddings:\n",
    "        for ihead in heads:\n",
    "            tempDict = {}\n",
    "            tempDict['hidden_channels'] = channel\n",
    "            tempDict['embedding_dim'] = embed\n",
    "            tempDict['nHeads'] = ihead\n",
    "            parameterList.append(tempDict)\n",
    "\n",
    "parameterList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "network counter: 1\n",
      "#### Epoch: 0 of 100 ####\n",
      " Training loss:\t0.4432685755592452\n",
      " Test loss:\t0.21982479095458984\n",
      " Validation loss:\t0.2233289755307711\n",
      "\n",
      " EARLY STOPPING\n",
      "Hidden_channels =  64 \tembedding_dim =  32 \tnHeads =  8\n",
      "Training Loss:  0.15404361217298973 \tTesting loss:  0.25609986598675066 \tValidation Loss:  0.2586972713470459 \n",
      "\n",
      "\n",
      "#### Epoch: 5 of 100 ####\n",
      " Training loss:\t0.15404361217298973\n",
      " Test loss:\t0.25609986598675066\n",
      " Validation loss:\t0.2586972713470459\n",
      "network counter: 2\n",
      "#### Epoch: 0 of 100 ####\n",
      " Training loss:\t0.39946352265672036\n",
      " Test loss:\t0.19928411337045523\n",
      " Validation loss:\t0.20267822192265436\n",
      "#### Epoch: 5 of 100 ####\n",
      " Training loss:\t0.15266308218940491\n",
      " Test loss:\t0.1872864319727971\n",
      " Validation loss:\t0.1902000537285438\n",
      "\n",
      " EARLY STOPPING\n",
      "Hidden_channels =  64 \tembedding_dim =  32 \tnHeads =  16\n",
      "Training Loss:  0.14868952264848923 \tTesting loss:  0.19524429394648626 \tValidation Loss:  0.19762178567739633 \n",
      "\n",
      "\n",
      "network counter: 3\n",
      "#### Epoch: 0 of 100 ####\n",
      " Training loss:\t0.39666814189729455\n",
      " Test loss:\t0.19327596517709586\n",
      " Validation loss:\t0.1959844552553617\n",
      "#### Epoch: 5 of 100 ####\n",
      " Training loss:\t0.13914455756015767\n",
      " Test loss:\t0.12457421192756066\n",
      " Validation loss:\t0.12779626479515663\n",
      "#### Epoch: 10 of 100 ####\n",
      " Training loss:\t0.13048236991611323\n",
      " Test loss:\t0.11803882855635422\n",
      " Validation loss:\t0.12091444088862492\n",
      "#### Epoch: 15 of 100 ####\n",
      " Training loss:\t0.1262921741538418\n",
      " Test loss:\t0.11363048736865704\n",
      " Validation loss:\t0.11677105610187237\n",
      "#### Epoch: 20 of 100 ####\n",
      " Training loss:\t0.12417160668048709\n",
      " Test loss:\t0.113289475440979\n",
      " Validation loss:\t0.11629143128028283\n",
      "#### Epoch: 25 of 100 ####\n",
      " Training loss:\t0.12283284774679222\n",
      " Test loss:\t0.11269824321453388\n",
      " Validation loss:\t0.11560932489541861\n",
      "#### Epoch: 30 of 100 ####\n",
      " Training loss:\t0.12127654312749285\n",
      " Test loss:\t0.11009672971872184\n",
      " Validation loss:\t0.11309635639190674\n",
      "#### Epoch: 35 of 100 ####\n",
      " Training loss:\t0.12052808161028748\n",
      " Test loss:\t0.1100322099832388\n",
      " Validation loss:\t0.11323844469510592\n",
      "#### Epoch: 40 of 100 ####\n",
      " Training loss:\t0.1196555287489182\n",
      " Test loss:\t0.1117564531473013\n",
      " Validation loss:\t0.11468894665057842\n",
      "#### Epoch: 45 of 100 ####\n",
      " Training loss:\t0.11925076203917538\n",
      " Test loss:\t0.10894598410679744\n",
      " Validation loss:\t0.11201911706190842\n",
      "#### Epoch: 50 of 100 ####\n",
      " Training loss:\t0.1188296065999398\n",
      " Test loss:\t0.10971691975226769\n",
      " Validation loss:\t0.1127851651265071\n",
      "#### Epoch: 55 of 100 ####\n",
      " Training loss:\t0.11832647995371764\n",
      " Test loss:\t0.10833898874429557\n",
      " Validation loss:\t0.11144649982452393\n",
      "#### Epoch: 60 of 100 ####\n",
      " Training loss:\t0.11798365786447358\n",
      " Test loss:\t0.10854503741631141\n",
      " Validation loss:\t0.1115352465556218\n",
      "#### Epoch: 65 of 100 ####\n",
      " Training loss:\t0.11775167098897972\n",
      " Test loss:\t0.1081452277990488\n",
      " Validation loss:\t0.11126478818746713\n",
      "#### Epoch: 70 of 100 ####\n",
      " Training loss:\t0.11761865954597193\n",
      " Test loss:\t0.10887329395000751\n",
      " Validation loss:\t0.1119072620685284\n",
      "#### Epoch: 75 of 100 ####\n",
      " Training loss:\t0.11742495284749398\n",
      " Test loss:\t0.10800197491279015\n",
      " Validation loss:\t0.11098068494063157\n",
      "#### Epoch: 80 of 100 ####\n",
      " Training loss:\t0.1174769668533169\n",
      " Test loss:\t0.10862547617692214\n",
      " Validation loss:\t0.11160602019383357\n",
      "#### Epoch: 85 of 100 ####\n",
      " Training loss:\t0.1174297665774571\n",
      " Test loss:\t0.10825181007385254\n",
      " Validation loss:\t0.11126277079949012\n",
      "#### Epoch: 90 of 100 ####\n",
      " Training loss:\t0.11743503044629082\n",
      " Test loss:\t0.107948715870197\n",
      " Validation loss:\t0.11096677413353553\n",
      "#### Epoch: 95 of 100 ####\n",
      " Training loss:\t0.11725819936220651\n",
      " Test loss:\t0.1081688770881066\n",
      " Validation loss:\t0.1111901173224816\n",
      "\n",
      " NO EARLY STOPPING\n",
      "Hidden_channels =  64 \tembedding_dim =  64 \tnHeads =  8\n",
      "Training Loss:  0.11713990470431498 \tTesting loss:  0.1081843559558575 \tValidation Loss:  0.11121173088367169 \n",
      "\n",
      "\n",
      "network counter: 4\n",
      "#### Epoch: 0 of 100 ####\n",
      " Training loss:\t0.41142108510733366\n",
      " Test loss:\t0.2057934174170861\n",
      " Validation loss:\t0.20948173449589655\n",
      "#### Epoch: 5 of 100 ####\n",
      " Training loss:\t0.1400907402796461\n",
      " Test loss:\t0.13223203328939584\n",
      " Validation loss:\t0.13502536370204046\n",
      "#### Epoch: 10 of 100 ####\n",
      " Training loss:\t0.13089812368890164\n",
      " Test loss:\t0.11998109634105976\n",
      " Validation loss:\t0.12298738956451416\n",
      "#### Epoch: 15 of 100 ####\n",
      " Training loss:\t0.12701942402508373\n",
      " Test loss:\t0.12106781739455003\n",
      " Validation loss:\t0.1236882209777832\n",
      "#### Epoch: 20 of 100 ####\n",
      " Training loss:\t0.12498336323480302\n",
      " Test loss:\t0.1157682492182805\n",
      " Validation loss:\t0.11873298424940842\n",
      "#### Epoch: 25 of 100 ####\n",
      " Training loss:\t0.12340463435054184\n",
      " Test loss:\t0.11248075045072116\n",
      " Validation loss:\t0.11505441482250507\n",
      "#### Epoch: 30 of 100 ####\n",
      " Training loss:\t0.12197506678958459\n",
      " Test loss:\t0.11271838958446796\n",
      " Validation loss:\t0.11544639330643874\n",
      "#### Epoch: 35 of 100 ####\n",
      " Training loss:\t0.12111363878887436\n",
      " Test loss:\t0.11287210537837102\n",
      " Validation loss:\t0.11578674499805157\n",
      "#### Epoch: 40 of 100 ####\n",
      " Training loss:\t0.12034761266920813\n",
      " Test loss:\t0.11119488569406363\n",
      " Validation loss:\t0.11385236336634709\n",
      "#### Epoch: 45 of 100 ####\n",
      " Training loss:\t0.11959653299734722\n",
      " Test loss:\t0.11064476233262283\n",
      " Validation loss:\t0.11350746338184063\n",
      "#### Epoch: 50 of 100 ####\n",
      " Training loss:\t0.119164748768861\n",
      " Test loss:\t0.1118094462614793\n",
      " Validation loss:\t0.11466680123255803\n",
      "#### Epoch: 55 of 100 ####\n",
      " Training loss:\t0.11868049120917369\n",
      " Test loss:\t0.11003582294170673\n",
      " Validation loss:\t0.1127643218407264\n",
      "#### Epoch: 60 of 100 ####\n",
      " Training loss:\t0.11831493251371068\n",
      " Test loss:\t0.11049002867478591\n",
      " Validation loss:\t0.11329697645627536\n",
      "#### Epoch: 65 of 100 ####\n",
      " Training loss:\t0.11801403764498514\n",
      " Test loss:\t0.11007402493403508\n",
      " Validation loss:\t0.1128859336559589\n",
      "#### Epoch: 70 of 100 ####\n",
      " Training loss:\t0.11777765961887605\n",
      " Test loss:\t0.10989290934342605\n",
      " Validation loss:\t0.11271091607900766\n",
      "#### Epoch: 75 of 100 ####\n",
      " Training loss:\t0.11757851450792069\n",
      " Test loss:\t0.10968038668999305\n",
      " Validation loss:\t0.11243420380812424\n",
      "#### Epoch: 80 of 100 ####\n",
      " Training loss:\t0.11764025702525305\n",
      " Test loss:\t0.1101981309744028\n",
      " Validation loss:\t0.11298812352693997\n",
      "#### Epoch: 85 of 100 ####\n",
      " Training loss:\t0.11734301838653767\n",
      " Test loss:\t0.1095036635032067\n",
      " Validation loss:\t0.11232984983004056\n",
      "#### Epoch: 90 of 100 ####\n",
      " Training loss:\t0.11728274571041541\n",
      " Test loss:\t0.10997076217944805\n",
      " Validation loss:\t0.11276279962979831\n",
      "#### Epoch: 95 of 100 ####\n",
      " Training loss:\t0.11727396340918497\n",
      " Test loss:\t0.1098783016204834\n",
      " Validation loss:\t0.11268181067246658\n",
      "\n",
      " NO EARLY STOPPING\n",
      "Hidden_channels =  64 \tembedding_dim =  64 \tnHeads =  16\n",
      "Training Loss:  0.11733708390265182 \tTesting loss:  0.1096648986522968 \tValidation Loss:  0.11247202983269325 \n",
      "\n",
      "\n",
      "network counter: 5\n",
      "#### Epoch: 0 of 100 ####\n",
      " Training loss:\t0.4938424677105283\n",
      " Test loss:\t0.21996943767254168\n",
      " Validation loss:\t0.22390514153700608\n",
      "#### Epoch: 5 of 100 ####\n",
      " Training loss:\t0.13808058544069368\n",
      " Test loss:\t0.12486624717712402\n",
      " Validation loss:\t0.12807029944199783\n",
      "#### Epoch: 10 of 100 ####\n",
      " Training loss:\t0.12854989618511534\n",
      " Test loss:\t0.1244574235035823\n",
      " Validation loss:\t0.1274952613390409\n",
      "#### Epoch: 15 of 100 ####\n",
      " Training loss:\t0.12327090635994412\n",
      " Test loss:\t0.11796775230994591\n",
      " Validation loss:\t0.12107573105738713\n",
      "#### Epoch: 20 of 100 ####\n",
      " Training loss:\t0.12028710114388923\n",
      " Test loss:\t0.11351418495178223\n",
      " Validation loss:\t0.11609510275033805\n",
      "#### Epoch: 25 of 100 ####\n",
      " Training loss:\t0.11779498537214028\n",
      " Test loss:\t0.11397658861600436\n",
      " Validation loss:\t0.11687991252312294\n",
      "#### Epoch: 30 of 100 ####\n",
      " Training loss:\t0.11618138450516914\n",
      " Test loss:\t0.11128741961259109\n",
      " Validation loss:\t0.11416945090660682\n",
      "#### Epoch: 35 of 100 ####\n",
      " Training loss:\t0.11482155172312425\n",
      " Test loss:\t0.1101986628312331\n",
      " Validation loss:\t0.11318193949185885\n",
      "#### Epoch: 40 of 100 ####\n",
      " Training loss:\t0.11370629133193483\n",
      " Test loss:\t0.10815831331106332\n",
      " Validation loss:\t0.11093335885267991\n",
      "#### Epoch: 45 of 100 ####\n",
      " Training loss:\t0.11298545364297204\n",
      " Test loss:\t0.10804975032806396\n",
      " Validation loss:\t0.11100912094116211\n",
      "#### Epoch: 50 of 100 ####\n",
      " Training loss:\t0.1125483044940689\n",
      " Test loss:\t0.10927809201754056\n",
      " Validation loss:\t0.1122058446590717\n",
      "#### Epoch: 55 of 100 ####\n",
      " Training loss:\t0.11182367277748251\n",
      " Test loss:\t0.10844358114095834\n",
      " Validation loss:\t0.11138385075789231\n",
      "#### Epoch: 60 of 100 ####\n",
      " Training loss:\t0.11158767139818351\n",
      " Test loss:\t0.10680427918067345\n",
      " Validation loss:\t0.10976665753584641\n",
      "#### Epoch: 65 of 100 ####\n",
      " Training loss:\t0.11108414054562857\n",
      " Test loss:\t0.10841539272895226\n",
      " Validation loss:\t0.11150101514963004\n",
      "#### Epoch: 70 of 100 ####\n",
      " Training loss:\t0.11082335396892402\n",
      " Test loss:\t0.10716361265916091\n",
      " Validation loss:\t0.11010258931380051\n",
      "#### Epoch: 75 of 100 ####\n",
      " Training loss:\t0.11076695799899346\n",
      " Test loss:\t0.10660909689389743\n",
      " Validation loss:\t0.10956811904907227\n",
      "#### Epoch: 80 of 100 ####\n",
      " Training loss:\t0.11074051004371896\n",
      " Test loss:\t0.10803486750676082\n",
      " Validation loss:\t0.11105873951545128\n",
      "#### Epoch: 85 of 100 ####\n",
      " Training loss:\t0.11051080142210076\n",
      " Test loss:\t0.10702043313246506\n",
      " Validation loss:\t0.11000096797943115\n",
      "#### Epoch: 90 of 100 ####\n",
      " Training loss:\t0.11034397444475279\n",
      " Test loss:\t0.1074610031568087\n",
      " Validation loss:\t0.11047204641195443\n",
      "#### Epoch: 95 of 100 ####\n",
      " Training loss:\t0.11035587097204715\n",
      " Test loss:\t0.10750931042891282\n",
      " Validation loss:\t0.11054359949552096\n",
      "\n",
      " NO EARLY STOPPING\n",
      "Hidden_channels =  64 \tembedding_dim =  128 \tnHeads =  8\n",
      "Training Loss:  0.11030124799807627 \tTesting loss:  0.10707175731658936 \tValidation Loss:  0.110060086617103 \n",
      "\n",
      "\n",
      "network counter: 6\n",
      "#### Epoch: 0 of 100 ####\n",
      " Training loss:\t0.4993273271701911\n",
      " Test loss:\t0.2021937003502479\n",
      " Validation loss:\t0.2050965015704815\n",
      "#### Epoch: 5 of 100 ####\n",
      " Training loss:\t0.13721148670616817\n",
      " Test loss:\t0.13162711033454308\n",
      " Validation loss:\t0.13445274646465594\n",
      "#### Epoch: 10 of 100 ####\n",
      " Training loss:\t0.1274026943071286\n",
      " Test loss:\t0.1193996392763578\n",
      " Validation loss:\t0.12238862881293663\n",
      "#### Epoch: 15 of 100 ####\n",
      " Training loss:\t0.12287446262605359\n",
      " Test loss:\t0.1163699351824247\n",
      " Validation loss:\t0.11950949522165152\n",
      "#### Epoch: 20 of 100 ####\n",
      " Training loss:\t0.1198785679660455\n",
      " Test loss:\t0.11136071498577411\n",
      " Validation loss:\t0.11430095709287204\n",
      "#### Epoch: 25 of 100 ####\n",
      " Training loss:\t0.11785801400053149\n",
      " Test loss:\t0.11233203227703388\n",
      " Validation loss:\t0.115524750489455\n",
      "#### Epoch: 30 of 100 ####\n",
      " Training loss:\t0.1161632135932091\n",
      " Test loss:\t0.11156746057363656\n",
      " Validation loss:\t0.11445384759169358\n",
      "#### Epoch: 35 of 100 ####\n",
      " Training loss:\t0.11481972360811916\n",
      " Test loss:\t0.1112828254699707\n",
      " Validation loss:\t0.11388017581059383\n",
      "#### Epoch: 40 of 100 ####\n",
      " Training loss:\t0.11384889348883862\n",
      " Test loss:\t0.1090855598449707\n",
      " Validation loss:\t0.11203217506408691\n",
      "#### Epoch: 45 of 100 ####\n",
      " Training loss:\t0.1129565437036706\n",
      " Test loss:\t0.10860357834742619\n",
      " Validation loss:\t0.11142809574420635\n",
      "#### Epoch: 50 of 100 ####\n",
      " Training loss:\t0.11237009543385008\n",
      " Test loss:\t0.10810975845043476\n",
      " Validation loss:\t0.11103432912092942\n",
      "#### Epoch: 55 of 100 ####\n",
      " Training loss:\t0.1119257899955552\n",
      " Test loss:\t0.10814956518319938\n",
      " Validation loss:\t0.11116476242358868\n",
      "#### Epoch: 60 of 100 ####\n",
      " Training loss:\t0.1116780298865753\n",
      " Test loss:\t0.10783492601834811\n",
      " Validation loss:\t0.11085346111884484\n",
      "#### Epoch: 65 of 100 ####\n",
      " Training loss:\t0.1112554046061296\n",
      " Test loss:\t0.10875261746920072\n",
      " Validation loss:\t0.11163303485283485\n",
      "#### Epoch: 70 of 100 ####\n",
      " Training loss:\t0.11108153157748062\n",
      " Test loss:\t0.10726611430828388\n",
      " Validation loss:\t0.11025480123666617\n",
      "#### Epoch: 75 of 100 ####\n",
      " Training loss:\t0.11075287508289905\n",
      " Test loss:\t0.10859878246600811\n",
      " Validation loss:\t0.11158720346597525\n",
      "#### Epoch: 80 of 100 ####\n",
      " Training loss:\t0.11085338466214818\n",
      " Test loss:\t0.1085062760573167\n",
      " Validation loss:\t0.11152263788076547\n",
      "#### Epoch: 85 of 100 ####\n",
      " Training loss:\t0.11061055770772972\n",
      " Test loss:\t0.10757135427915133\n",
      " Validation loss:\t0.11054469071901761\n",
      "#### Epoch: 90 of 100 ####\n",
      " Training loss:\t0.11048580493502126\n",
      " Test loss:\t0.10759078539334811\n",
      " Validation loss:\t0.11056886269496037\n",
      "#### Epoch: 95 of 100 ####\n",
      " Training loss:\t0.11051318072755964\n",
      " Test loss:\t0.10809840605809139\n",
      " Validation loss:\t0.11109404380504902\n",
      "\n",
      " NO EARLY STOPPING\n",
      "Hidden_channels =  64 \tembedding_dim =  128 \tnHeads =  16\n",
      "Training Loss:  0.110383678530441 \tTesting loss:  0.10787840989919809 \tValidation Loss:  0.11087634013249324 \n",
      "\n",
      "\n",
      "network counter: 7\n",
      "#### Epoch: 0 of 100 ####\n",
      " Training loss:\t0.374633348110137\n",
      " Test loss:\t0.19489352519695574\n",
      " Validation loss:\t0.1982279007251446\n",
      "#### Epoch: 5 of 100 ####\n",
      " Training loss:\t0.1430298439498984\n",
      " Test loss:\t0.2119306784409743\n",
      " Validation loss:\t0.21587126071636492\n",
      "\n",
      " EARLY STOPPING\n",
      "Hidden_channels =  128 \tembedding_dim =  32 \tnHeads =  8\n",
      "Training Loss:  0.13906409294041147 \tTesting loss:  0.1651133390573355 \tValidation Loss:  0.16895312529343826 \n",
      "\n",
      "\n",
      "network counter: 8\n",
      "#### Epoch: 0 of 100 ####\n",
      " Training loss:\t0.3826426598481713\n",
      " Test loss:\t0.18854885834913987\n",
      " Validation loss:\t0.19107600358816293\n",
      "#### Epoch: 5 of 100 ####\n",
      " Training loss:\t0.14285589742918892\n",
      " Test loss:\t0.14102021547464225\n",
      " Validation loss:\t0.14357434786283052\n",
      "\n",
      " EARLY STOPPING\n",
      "Hidden_channels =  128 \tembedding_dim =  32 \tnHeads =  16\n",
      "Training Loss:  0.13560345286278033 \tTesting loss:  0.16221952438354492 \tValidation Loss:  0.16520014176001915 \n",
      "\n",
      "\n",
      "network counter: 9\n",
      "#### Epoch: 0 of 100 ####\n",
      " Training loss:\t0.36014354322273856\n",
      " Test loss:\t0.17524047998281625\n",
      " Validation loss:\t0.17867014958308294\n",
      "#### Epoch: 5 of 100 ####\n",
      " Training loss:\t0.13304368764933586\n",
      " Test loss:\t0.12214871553274301\n",
      " Validation loss:\t0.12493670903719388\n",
      "#### Epoch: 10 of 100 ####\n",
      " Training loss:\t0.12476818766814983\n",
      " Test loss:\t0.11233990009014423\n",
      " Validation loss:\t0.11540398230919471\n",
      "#### Epoch: 15 of 100 ####\n",
      " Training loss:\t0.12094090488716323\n",
      " Test loss:\t0.11215128348423885\n",
      " Validation loss:\t0.1149809268804697\n",
      "#### Epoch: 20 of 100 ####\n",
      " Training loss:\t0.11755346290179579\n",
      " Test loss:\t0.11039213033822867\n",
      " Validation loss:\t0.11364926741673397\n",
      "#### Epoch: 25 of 100 ####\n",
      " Training loss:\t0.11542716304772163\n",
      " Test loss:\t0.1049570211997399\n",
      " Validation loss:\t0.10800081949967605\n",
      "#### Epoch: 30 of 100 ####\n",
      " Training loss:\t0.11402564189435486\n",
      " Test loss:\t0.10541699482844426\n",
      " Validation loss:\t0.10847978408520038\n",
      "#### Epoch: 35 of 100 ####\n",
      " Training loss:\t0.11260197207125319\n",
      " Test loss:\t0.10463344133817233\n",
      " Validation loss:\t0.10772670232332669\n",
      "#### Epoch: 40 of 100 ####\n",
      " Training loss:\t0.11197748247361342\n",
      " Test loss:\t0.10283246407142052\n",
      " Validation loss:\t0.10587201668665959\n",
      "#### Epoch: 45 of 100 ####\n",
      " Training loss:\t0.11113154292465477\n",
      " Test loss:\t0.10273098028623141\n",
      " Validation loss:\t0.1056973750774677\n",
      "#### Epoch: 50 of 100 ####\n",
      " Training loss:\t0.11080736944576404\n",
      " Test loss:\t0.10448115605574387\n",
      " Validation loss:\t0.10770642757415771\n",
      "#### Epoch: 55 of 100 ####\n",
      " Training loss:\t0.11023593192930181\n",
      " Test loss:\t0.10137995389791635\n",
      " Validation loss:\t0.10447880854973426\n",
      "#### Epoch: 60 of 100 ####\n",
      " Training loss:\t0.10988826855057195\n",
      " Test loss:\t0.10123129074390118\n",
      " Validation loss:\t0.10415080877450797\n",
      "#### Epoch: 65 of 100 ####\n",
      " Training loss:\t0.10970546623657153\n",
      " Test loss:\t0.10252600449782151\n",
      " Validation loss:\t0.10560786724090576\n",
      "#### Epoch: 70 of 100 ####\n",
      " Training loss:\t0.10959430030854907\n",
      " Test loss:\t0.10137499295748197\n",
      " Validation loss:\t0.10448558513934796\n",
      "#### Epoch: 75 of 100 ####\n",
      " Training loss:\t0.1094353094221525\n",
      " Test loss:\t0.10196081491617057\n",
      " Validation loss:\t0.10505010531498836\n",
      "#### Epoch: 80 of 100 ####\n",
      " Training loss:\t0.10927723718651226\n",
      " Test loss:\t0.1012797263952402\n",
      " Validation loss:\t0.10437583923339844\n",
      "#### Epoch: 85 of 100 ####\n",
      " Training loss:\t0.10936088033787533\n",
      " Test loss:\t0.10181195919330303\n",
      " Validation loss:\t0.1049423309472891\n",
      "#### Epoch: 90 of 100 ####\n",
      " Training loss:\t0.10911516882008203\n",
      " Test loss:\t0.10159504413604736\n",
      " Validation loss:\t0.10473303611461933\n",
      "#### Epoch: 95 of 100 ####\n",
      " Training loss:\t0.1091237673911612\n",
      " Test loss:\t0.10153502684373122\n",
      " Validation loss:\t0.10459075524256779\n",
      "\n",
      " NO EARLY STOPPING\n",
      "Hidden_channels =  128 \tembedding_dim =  64 \tnHeads =  8\n",
      "Training Loss:  0.10900657511417726 \tTesting loss:  0.10120835671058068 \tValidation Loss:  0.10428742262033316 \n",
      "\n",
      "\n",
      "network counter: 10\n",
      "#### Epoch: 0 of 100 ####\n",
      " Training loss:\t0.3663769560646824\n",
      " Test loss:\t0.19731563788193923\n",
      " Validation loss:\t0.20090803733238807\n",
      "#### Epoch: 5 of 100 ####\n",
      " Training loss:\t0.13358438711149156\n",
      " Test loss:\t0.12453961372375488\n",
      " Validation loss:\t0.12726870866922232\n",
      "#### Epoch: 10 of 100 ####\n",
      " Training loss:\t0.12564402929922674\n",
      " Test loss:\t0.11494181706355168\n",
      " Validation loss:\t0.11804203803722675\n",
      "#### Epoch: 15 of 100 ####\n",
      " Training loss:\t0.12134532250961864\n",
      " Test loss:\t0.11476522225600022\n",
      " Validation loss:\t0.11721688050490159\n",
      "#### Epoch: 20 of 100 ####\n",
      " Training loss:\t0.1186988182917623\n",
      " Test loss:\t0.11231675514808068\n",
      " Validation loss:\t0.11524704786447379\n",
      "#### Epoch: 25 of 100 ####\n",
      " Training loss:\t0.11661196530116459\n",
      " Test loss:\t0.10654453130868766\n",
      " Validation loss:\t0.1097422929910513\n",
      "#### Epoch: 30 of 100 ####\n",
      " Training loss:\t0.11508007841896824\n",
      " Test loss:\t0.10640086577488826\n",
      " Validation loss:\t0.10944117032564603\n",
      "#### Epoch: 35 of 100 ####\n",
      " Training loss:\t0.11396967769027977\n",
      " Test loss:\t0.10542618311368503\n",
      " Validation loss:\t0.1083598045202402\n",
      "#### Epoch: 40 of 100 ####\n",
      " Training loss:\t0.11308398812309509\n",
      " Test loss:\t0.10405089304997371\n",
      " Validation loss:\t0.10712526394770695\n",
      "#### Epoch: 45 of 100 ####\n",
      " Training loss:\t0.11198722017163042\n",
      " Test loss:\t0.10486114942110501\n",
      " Validation loss:\t0.10768661132225624\n",
      "#### Epoch: 50 of 100 ####\n",
      " Training loss:\t0.11150869499266255\n",
      " Test loss:\t0.10600435733795166\n",
      " Validation loss:\t0.10886998359973614\n",
      "#### Epoch: 55 of 100 ####\n",
      " Training loss:\t0.11109893082280535\n",
      " Test loss:\t0.10511937508216271\n",
      " Validation loss:\t0.10797555630023663\n",
      "#### Epoch: 60 of 100 ####\n",
      " Training loss:\t0.11073046919095236\n",
      " Test loss:\t0.10538068184485802\n",
      " Validation loss:\t0.10821387400993934\n",
      "#### Epoch: 65 of 100 ####\n",
      " Training loss:\t0.11050083406139467\n",
      " Test loss:\t0.10549599390763503\n",
      " Validation loss:\t0.10848337870377761\n",
      "#### Epoch: 70 of 100 ####\n",
      " Training loss:\t0.11031622200540431\n",
      " Test loss:\t0.10442755772517277\n",
      " Validation loss:\t0.10734054675469032\n",
      "#### Epoch: 75 of 100 ####\n",
      " Training loss:\t0.110190835363674\n",
      " Test loss:\t0.10459173642672025\n",
      " Validation loss:\t0.1075423130622277\n",
      "#### Epoch: 80 of 100 ####\n",
      " Training loss:\t0.10994441648479832\n",
      " Test loss:\t0.10476409471951999\n",
      " Validation loss:\t0.10771727561950684\n",
      "#### Epoch: 85 of 100 ####\n",
      " Training loss:\t0.10990937914495222\n",
      " Test loss:\t0.10507234243246225\n",
      " Validation loss:\t0.10802674293518066\n",
      "#### Epoch: 90 of 100 ####\n",
      " Training loss:\t0.10987195331313968\n",
      " Test loss:\t0.104705443749061\n",
      " Validation loss:\t0.10764584174522987\n",
      "#### Epoch: 95 of 100 ####\n",
      " Training loss:\t0.10989982517708646\n",
      " Test loss:\t0.10504374137291542\n",
      " Validation loss:\t0.1079882475046011\n",
      "\n",
      " NO EARLY STOPPING\n",
      "Hidden_channels =  128 \tembedding_dim =  64 \tnHeads =  16\n",
      "Training Loss:  0.10983815615181461 \tTesting loss:  0.1052007400072538 \tValidation Loss:  0.10814521862910344 \n",
      "\n",
      "\n",
      "network counter: 11\n",
      "#### Epoch: 0 of 100 ####\n",
      " Training loss:\t0.3956067328134407\n",
      " Test loss:\t0.1845234724191519\n",
      " Validation loss:\t0.18743060185359076\n",
      "#### Epoch: 5 of 100 ####\n",
      " Training loss:\t0.12699249837141593\n",
      " Test loss:\t0.11941525569328895\n",
      " Validation loss:\t0.1217435414974506\n",
      "#### Epoch: 10 of 100 ####\n",
      " Training loss:\t0.11688254469493152\n",
      " Test loss:\t0.11360439887413612\n",
      " Validation loss:\t0.11590202038104717\n",
      "#### Epoch: 15 of 100 ####\n",
      " Training loss:\t0.1119808814814118\n",
      " Test loss:\t0.11002856034498948\n",
      " Validation loss:\t0.11261933583479661\n",
      "#### Epoch: 20 of 100 ####\n",
      " Training loss:\t0.10858578713524421\n",
      " Test loss:\t0.10397691910083477\n",
      " Validation loss:\t0.1066903334397536\n",
      "#### Epoch: 25 of 100 ####\n",
      " Training loss:\t0.10597138976132131\n",
      " Test loss:\t0.10112530451554519\n",
      " Validation loss:\t0.10388741126427284\n",
      "#### Epoch: 30 of 100 ####\n",
      " Training loss:\t0.10459249497608275\n",
      " Test loss:\t0.10105767616858849\n",
      " Validation loss:\t0.10385539898505577\n",
      "#### Epoch: 35 of 100 ####\n",
      " Training loss:\t0.10322786683708986\n",
      " Test loss:\t0.10012523027566764\n",
      " Validation loss:\t0.10296065073746902\n",
      "#### Epoch: 40 of 100 ####\n",
      " Training loss:\t0.10225120328550666\n",
      " Test loss:\t0.09973219724801871\n",
      " Validation loss:\t0.10224909965808575\n",
      "#### Epoch: 45 of 100 ####\n",
      " Training loss:\t0.10140369284375471\n",
      " Test loss:\t0.09920153251061073\n",
      " Validation loss:\t0.10196053064786471\n",
      "#### Epoch: 50 of 100 ####\n",
      " Training loss:\t0.10095596083813874\n",
      " Test loss:\t0.0988244368479802\n",
      " Validation loss:\t0.1015848288169274\n",
      "#### Epoch: 55 of 100 ####\n",
      " Training loss:\t0.10043306482764054\n",
      " Test loss:\t0.09686858837421124\n",
      " Validation loss:\t0.09974697002997765\n",
      "#### Epoch: 60 of 100 ####\n",
      " Training loss:\t0.10008888313527713\n",
      " Test loss:\t0.09755816826453576\n",
      " Validation loss:\t0.10032663895533635\n",
      "#### Epoch: 65 of 100 ####\n",
      " Training loss:\t0.09977407989410088\n",
      " Test loss:\t0.09626016250023475\n",
      " Validation loss:\t0.0991142621407142\n",
      "#### Epoch: 70 of 100 ####\n",
      " Training loss:\t0.09965551882749003\n",
      " Test loss:\t0.09741283380068265\n",
      " Validation loss:\t0.10032193477337177\n",
      "#### Epoch: 75 of 100 ####\n",
      " Training loss:\t0.09939285820324259\n",
      " Test loss:\t0.09673069073603703\n",
      " Validation loss:\t0.09955240212954007\n",
      "#### Epoch: 80 of 100 ####\n",
      " Training loss:\t0.09931885169831332\n",
      " Test loss:\t0.09662188933445857\n",
      " Validation loss:\t0.09954105890714206\n",
      "#### Epoch: 85 of 100 ####\n",
      " Training loss:\t0.09912586154742531\n",
      " Test loss:\t0.09698854959928073\n",
      " Validation loss:\t0.09979121501629169\n",
      "#### Epoch: 90 of 100 ####\n",
      " Training loss:\t0.09920452561168912\n",
      " Test loss:\t0.09642216792473426\n",
      " Validation loss:\t0.09923417751605694\n",
      "#### Epoch: 95 of 100 ####\n",
      " Training loss:\t0.09910193069522313\n",
      " Test loss:\t0.09694052659548245\n",
      " Validation loss:\t0.09976355846111591\n",
      "\n",
      " NO EARLY STOPPING\n",
      "Hidden_channels =  128 \tembedding_dim =  128 \tnHeads =  8\n",
      "Training Loss:  0.09891728189320251 \tTesting loss:  0.09646898966569167 \tValidation Loss:  0.09928101759690505 \n",
      "\n",
      "\n",
      "network counter: 12\n",
      "#### Epoch: 0 of 100 ####\n",
      " Training loss:\t0.4220861794358632\n",
      " Test loss:\t0.20967815472529486\n",
      " Validation loss:\t0.21357162182147688\n",
      "#### Epoch: 5 of 100 ####\n",
      " Training loss:\t0.1279961886282501\n",
      " Test loss:\t0.11549284824958214\n",
      " Validation loss:\t0.11801675649789664\n",
      "#### Epoch: 10 of 100 ####\n",
      " Training loss:\t0.11734963267198319\n",
      " Test loss:\t0.1084680832349337\n",
      " Validation loss:\t0.11126550344320443\n",
      "#### Epoch: 15 of 100 ####\n",
      " Training loss:\t0.11170866688929862\n",
      " Test loss:\t0.10444110173445481\n",
      " Validation loss:\t0.10708878590510441\n",
      "#### Epoch: 20 of 100 ####\n",
      " Training loss:\t0.10799052900467057\n",
      " Test loss:\t0.10402629925654484\n",
      " Validation loss:\t0.10729565987220177\n",
      "#### Epoch: 25 of 100 ####\n",
      " Training loss:\t0.10606175743622065\n",
      " Test loss:\t0.10315823554992676\n",
      " Validation loss:\t0.10609613015101506\n",
      "#### Epoch: 30 of 100 ####\n",
      " Training loss:\t0.10445861081485645\n",
      " Test loss:\t0.10387519689706656\n",
      " Validation loss:\t0.10672117196596585\n",
      "#### Epoch: 35 of 100 ####\n",
      " Training loss:\t0.1031476410665403\n",
      " Test loss:\t0.09900793662438026\n",
      " Validation loss:\t0.1018221928523137\n",
      "#### Epoch: 40 of 100 ####\n",
      " Training loss:\t0.10231026334263058\n",
      " Test loss:\t0.09652989644270676\n",
      " Validation loss:\t0.09948700207930344\n",
      "#### Epoch: 45 of 100 ####\n",
      " Training loss:\t0.10144100843747648\n",
      " Test loss:\t0.09825515747070312\n",
      " Validation loss:\t0.10120994311112624\n",
      "#### Epoch: 50 of 100 ####\n",
      " Training loss:\t0.10079471925738918\n",
      " Test loss:\t0.09526542516855094\n",
      " Validation loss:\t0.09827630336468036\n",
      "#### Epoch: 55 of 100 ####\n",
      " Training loss:\t0.10041076917378462\n",
      " Test loss:\t0.09695761020366962\n",
      " Validation loss:\t0.0998088213113638\n",
      "#### Epoch: 60 of 100 ####\n",
      " Training loss:\t0.09995814075963745\n",
      " Test loss:\t0.0966714803989117\n",
      " Validation loss:\t0.0995102020410391\n",
      "#### Epoch: 65 of 100 ####\n",
      " Training loss:\t0.0997436817405742\n",
      " Test loss:\t0.09592620226053092\n",
      " Validation loss:\t0.09875639585348275\n",
      "#### Epoch: 70 of 100 ####\n",
      " Training loss:\t0.09939061669539716\n",
      " Test loss:\t0.09603660840254563\n",
      " Validation loss:\t0.09894532423753005\n",
      "#### Epoch: 75 of 100 ####\n",
      " Training loss:\t0.09918136642612226\n",
      " Test loss:\t0.0974412331214318\n",
      " Validation loss:\t0.10026719936957726\n",
      "#### Epoch: 80 of 100 ####\n",
      " Training loss:\t0.0991609999388546\n",
      " Test loss:\t0.09683919869936429\n",
      " Validation loss:\t0.09955540070166954\n",
      "#### Epoch: 85 of 100 ####\n",
      " Training loss:\t0.0989865849062594\n",
      " Test loss:\t0.09727933773627648\n",
      " Validation loss:\t0.10001954665550819\n",
      "#### Epoch: 90 of 100 ####\n",
      " Training loss:\t0.09892297752789171\n",
      " Test loss:\t0.09611392021179199\n",
      " Validation loss:\t0.09893547571622409\n",
      "#### Epoch: 95 of 100 ####\n",
      " Training loss:\t0.09879574439814118\n",
      " Test loss:\t0.09611016970414382\n",
      " Validation loss:\t0.0989539439861591\n",
      "\n",
      " NO EARLY STOPPING\n",
      "Hidden_channels =  128 \tembedding_dim =  128 \tnHeads =  16\n",
      "Training Loss:  0.09885943445507694 \tTesting loss:  0.09652199194981502 \tValidation Loss:  0.09928240225865291 \n",
      "\n",
      "\n",
      "network counter: 13\n",
      "#### Epoch: 0 of 100 ####\n",
      " Training loss:\t0.3702881667787666\n",
      " Test loss:\t0.1733860602745643\n",
      " Validation loss:\t0.17719545731177697\n",
      "\n",
      " EARLY STOPPING\n",
      "Hidden_channels =  256 \tembedding_dim =  32 \tnHeads =  8\n",
      "Training Loss:  0.1401371964483933 \tTesting loss:  0.1789579024681678 \tValidation Loss:  0.18131118554335374 \n",
      "\n",
      "\n",
      "network counter: 14\n",
      "#### Epoch: 0 of 100 ####\n",
      " Training loss:\t0.365359495233011\n",
      " Test loss:\t0.17163606790395883\n",
      " Validation loss:\t0.17412752371567947\n",
      "#### Epoch: 5 of 100 ####\n",
      " Training loss:\t0.13824678773552734\n",
      " Test loss:\t0.15862011909484863\n",
      " Validation loss:\t0.16147354932931754\n",
      "\n",
      " EARLY STOPPING\n",
      "Hidden_channels =  256 \tembedding_dim =  32 \tnHeads =  16\n",
      "Training Loss:  0.1277252785202109 \tTesting loss:  0.15491881737342247 \tValidation Loss:  0.15802572323725775 \n",
      "\n",
      "\n",
      "#### Epoch: 10 of 100 ####\n",
      " Training loss:\t0.1277252785202109\n",
      " Test loss:\t0.15491881737342247\n",
      " Validation loss:\t0.15802572323725775\n",
      "network counter: 15\n",
      "#### Epoch: 0 of 100 ####\n",
      " Training loss:\t0.3198638309706126\n",
      " Test loss:\t0.1690517205458421\n",
      " Validation loss:\t0.17181783456068772\n",
      "#### Epoch: 5 of 100 ####\n",
      " Training loss:\t0.12956930455639015\n",
      " Test loss:\t0.11843968354738675\n",
      " Validation loss:\t0.1214355413730328\n",
      "#### Epoch: 10 of 100 ####\n",
      " Training loss:\t0.1220908167848811\n",
      " Test loss:\t0.11239969730377197\n",
      " Validation loss:\t0.11498354948483981\n",
      "#### Epoch: 15 of 100 ####\n",
      " Training loss:\t0.11806668551982992\n",
      " Test loss:\t0.10995407287891094\n",
      " Validation loss:\t0.11313551205855149\n",
      "#### Epoch: 20 of 100 ####\n",
      " Training loss:\t0.11506197181256114\n",
      " Test loss:\t0.10497361880082351\n",
      " Validation loss:\t0.10795386020953839\n",
      "#### Epoch: 25 of 100 ####\n",
      " Training loss:\t0.11272634819715156\n",
      " Test loss:\t0.10535432742192195\n",
      " Validation loss:\t0.1082452810727633\n",
      "#### Epoch: 30 of 100 ####\n",
      " Training loss:\t0.11146563783171376\n",
      " Test loss:\t0.10339418741372916\n",
      " Validation loss:\t0.10638595544374906\n",
      "#### Epoch: 35 of 100 ####\n",
      " Training loss:\t0.11011836300550779\n",
      " Test loss:\t0.10064436839177059\n",
      " Validation loss:\t0.10332301029792199\n",
      "#### Epoch: 40 of 100 ####\n",
      " Training loss:\t0.10889169783709926\n",
      " Test loss:\t0.10143158986018254\n",
      " Validation loss:\t0.10413913543407734\n",
      "#### Epoch: 45 of 100 ####\n",
      " Training loss:\t0.10810429471433342\n",
      " Test loss:\t0.09986951717963585\n",
      " Validation loss:\t0.1025994924398569\n",
      "#### Epoch: 50 of 100 ####\n",
      " Training loss:\t0.10765554300637793\n",
      " Test loss:\t0.10046477501208965\n",
      " Validation loss:\t0.10320353507995605\n",
      "#### Epoch: 55 of 100 ####\n",
      " Training loss:\t0.107037281860866\n",
      " Test loss:\t0.0989544941828801\n",
      " Validation loss:\t0.10166733081524189\n",
      "#### Epoch: 60 of 100 ####\n",
      " Training loss:\t0.10673282262720593\n",
      " Test loss:\t0.0986631283393273\n",
      " Validation loss:\t0.1014867379115178\n",
      "#### Epoch: 65 of 100 ####\n",
      " Training loss:\t0.10636888994358161\n",
      " Test loss:\t0.09871880824749286\n",
      " Validation loss:\t0.10153307364537166\n",
      "#### Epoch: 70 of 100 ####\n",
      " Training loss:\t0.10623046764761909\n",
      " Test loss:\t0.09847453007331261\n",
      " Validation loss:\t0.10133052789247952\n",
      "#### Epoch: 75 of 100 ####\n",
      " Training loss:\t0.10618585796689212\n",
      " Test loss:\t0.09889589823209323\n",
      " Validation loss:\t0.10173656390263484\n",
      "#### Epoch: 80 of 100 ####\n",
      " Training loss:\t0.10604896981769266\n",
      " Test loss:\t0.09902266355661246\n",
      " Validation loss:\t0.10188873914571908\n",
      "#### Epoch: 85 of 100 ####\n",
      " Training loss:\t0.10594640246074936\n",
      " Test loss:\t0.09875747790703407\n",
      " Validation loss:\t0.10158393016228309\n",
      "#### Epoch: 90 of 100 ####\n",
      " Training loss:\t0.10591581139056386\n",
      " Test loss:\t0.09811606773963341\n",
      " Validation loss:\t0.10096230873694786\n",
      "#### Epoch: 95 of 100 ####\n",
      " Training loss:\t0.10586981454719484\n",
      " Test loss:\t0.09828870113079365\n",
      " Validation loss:\t0.10110268226036659\n",
      "\n",
      " NO EARLY STOPPING\n",
      "Hidden_channels =  256 \tembedding_dim =  64 \tnHeads =  8\n",
      "Training Loss:  0.1056977328875207 \tTesting loss:  0.09869729555570163 \tValidation Loss:  0.10152411460876465 \n",
      "\n",
      "\n",
      "network counter: 16\n",
      "#### Epoch: 0 of 100 ####\n",
      " Training loss:\t0.33710528884143587\n",
      " Test loss:\t0.15698526455805853\n",
      " Validation loss:\t0.16047052236703727\n",
      "#### Epoch: 5 of 100 ####\n",
      " Training loss:\t0.1286102699128208\n",
      " Test loss:\t0.11453776176159199\n",
      " Validation loss:\t0.11762023889101468\n",
      "#### Epoch: 10 of 100 ####\n",
      " Training loss:\t0.12122341755425478\n",
      " Test loss:\t0.10868449394519512\n",
      " Validation loss:\t0.11134674915900597\n",
      "#### Epoch: 15 of 100 ####\n",
      " Training loss:\t0.11716739992145168\n",
      " Test loss:\t0.10691447441394512\n",
      " Validation loss:\t0.11025943205906795\n",
      "#### Epoch: 20 of 100 ####\n",
      " Training loss:\t0.1148052548587071\n",
      " Test loss:\t0.10609786327068622\n",
      " Validation loss:\t0.10972784115717961\n",
      "#### Epoch: 25 of 100 ####\n",
      " Training loss:\t0.11326576154824843\n",
      " Test loss:\t0.10201268929701585\n",
      " Validation loss:\t0.10503069254068229\n",
      "#### Epoch: 30 of 100 ####\n",
      " Training loss:\t0.11162601588075237\n",
      " Test loss:\t0.10178503623375526\n",
      " Validation loss:\t0.10478119666759785\n",
      "#### Epoch: 35 of 100 ####\n",
      " Training loss:\t0.11060503661668422\n",
      " Test loss:\t0.10064889834477352\n",
      " Validation loss:\t0.10372492900261512\n",
      "#### Epoch: 40 of 100 ####\n",
      " Training loss:\t0.11000630598625169\n",
      " Test loss:\t0.10300886631011963\n",
      " Validation loss:\t0.10603912977071908\n",
      "#### Epoch: 45 of 100 ####\n",
      " Training loss:\t0.10926597452823976\n",
      " Test loss:\t0.10073279417478122\n",
      " Validation loss:\t0.10365196374746469\n",
      "#### Epoch: 50 of 100 ####\n",
      " Training loss:\t0.10879936424958139\n",
      " Test loss:\t0.09963080516228309\n",
      " Validation loss:\t0.10268372755784255\n",
      "#### Epoch: 55 of 100 ####\n",
      " Training loss:\t0.10838659609175101\n",
      " Test loss:\t0.10023858914008507\n",
      " Validation loss:\t0.10323176017174354\n",
      "#### Epoch: 60 of 100 ####\n",
      " Training loss:\t0.10827982131434377\n",
      " Test loss:\t0.10000603932600755\n",
      " Validation loss:\t0.10301336875328651\n",
      "#### Epoch: 65 of 100 ####\n",
      " Training loss:\t0.10779881577833383\n",
      " Test loss:\t0.09946593871483436\n",
      " Validation loss:\t0.10248585847707894\n",
      "#### Epoch: 70 of 100 ####\n",
      " Training loss:\t0.10765098649862657\n",
      " Test loss:\t0.0992908110985389\n",
      " Validation loss:\t0.10222717431875375\n",
      "#### Epoch: 75 of 100 ####\n",
      " Training loss:\t0.10749169245748043\n",
      " Test loss:\t0.09969430703383225\n",
      " Validation loss:\t0.10270922917586106\n",
      "#### Epoch: 80 of 100 ####\n",
      " Training loss:\t0.1075107085281363\n",
      " Test loss:\t0.09879920115837684\n",
      " Validation loss:\t0.10178698943211482\n",
      "#### Epoch: 85 of 100 ####\n",
      " Training loss:\t0.10725993524467564\n",
      " Test loss:\t0.09975815736330472\n",
      " Validation loss:\t0.10277812297527607\n",
      "#### Epoch: 90 of 100 ####\n",
      " Training loss:\t0.10730024563986304\n",
      " Test loss:\t0.09896857004899245\n",
      " Validation loss:\t0.10197711907900296\n",
      "#### Epoch: 95 of 100 ####\n",
      " Training loss:\t0.10724370268581145\n",
      " Test loss:\t0.09910409267132099\n",
      " Validation loss:\t0.10209769469041091\n",
      "\n",
      " NO EARLY STOPPING\n",
      "Hidden_channels =  256 \tembedding_dim =  64 \tnHeads =  16\n",
      "Training Loss:  0.1073607663516895 \tTesting loss:  0.09885958524850699 \tValidation Loss:  0.10182532897362342 \n",
      "\n",
      "\n",
      "network counter: 17\n",
      "#### Epoch: 0 of 100 ####\n",
      " Training loss:\t0.3592960327809866\n",
      " Test loss:\t0.1708202728858361\n",
      " Validation loss:\t0.17446118134718674\n",
      "#### Epoch: 5 of 100 ####\n",
      " Training loss:\t0.12255733519846385\n",
      " Test loss:\t0.11062932014465332\n",
      " Validation loss:\t0.11329370278578538\n",
      "#### Epoch: 10 of 100 ####\n",
      " Training loss:\t0.11320658206652619\n",
      " Test loss:\t0.10441063000605656\n",
      " Validation loss:\t0.10772584034846379\n",
      "#### Epoch: 15 of 100 ####\n",
      " Training loss:\t0.1091217187999746\n",
      " Test loss:\t0.10393597529484676\n",
      " Validation loss:\t0.10758668642777663\n",
      "#### Epoch: 20 of 100 ####\n",
      " Training loss:\t0.10622581008828454\n",
      " Test loss:\t0.10067156644967887\n",
      " Validation loss:\t0.10351310326502873\n",
      "#### Epoch: 25 of 100 ####\n",
      " Training loss:\t0.10419480187716361\n",
      " Test loss:\t0.0959923450763409\n",
      " Validation loss:\t0.09890131766979511\n",
      "#### Epoch: 30 of 100 ####\n",
      " Training loss:\t0.10271309169927755\n",
      " Test loss:\t0.0976869142972506\n",
      " Validation loss:\t0.10064215843494122\n",
      "#### Epoch: 35 of 100 ####\n",
      " Training loss:\t0.10116832533347757\n",
      " Test loss:\t0.0938789019217858\n",
      " Validation loss:\t0.09692097627199613\n",
      "#### Epoch: 40 of 100 ####\n",
      " Training loss:\t0.10024835172006698\n",
      " Test loss:\t0.09425559410682091\n",
      " Validation loss:\t0.09744837650885949\n",
      "#### Epoch: 45 of 100 ####\n",
      " Training loss:\t0.0995487899826206\n",
      " Test loss:\t0.09319772170140193\n",
      " Validation loss:\t0.0961914612696721\n",
      "#### Epoch: 50 of 100 ####\n",
      " Training loss:\t0.09883325290852178\n",
      " Test loss:\t0.09292115614964412\n",
      " Validation loss:\t0.0959233962572538\n",
      "#### Epoch: 55 of 100 ####\n",
      " Training loss:\t0.09844666629724658\n",
      " Test loss:\t0.09224237845494197\n",
      " Validation loss:\t0.09523511849916898\n",
      "#### Epoch: 60 of 100 ####\n",
      " Training loss:\t0.09819162128776904\n",
      " Test loss:\t0.09325060477623573\n",
      " Validation loss:\t0.0960914813555204\n",
      "#### Epoch: 65 of 100 ####\n",
      " Training loss:\t0.09783597483396674\n",
      " Test loss:\t0.09160430614764874\n",
      " Validation loss:\t0.09467663214756893\n",
      "#### Epoch: 70 of 100 ####\n",
      " Training loss:\t0.09764209359184509\n",
      " Test loss:\t0.09315891449268048\n",
      " Validation loss:\t0.09619938410245456\n",
      "#### Epoch: 75 of 100 ####\n",
      " Training loss:\t0.09753493298112019\n",
      " Test loss:\t0.09247116859142597\n",
      " Validation loss:\t0.09545522469740647\n",
      "#### Epoch: 80 of 100 ####\n",
      " Training loss:\t0.09743163070357183\n",
      " Test loss:\t0.0917809192950909\n",
      " Validation loss:\t0.09480521312126747\n",
      "#### Epoch: 85 of 100 ####\n",
      " Training loss:\t0.09705193397296903\n",
      " Test loss:\t0.09209094597743107\n",
      " Validation loss:\t0.09513217669266921\n",
      "#### Epoch: 90 of 100 ####\n",
      " Training loss:\t0.09717429825371106\n",
      " Test loss:\t0.09199895308567928\n",
      " Validation loss:\t0.09498996001023513\n",
      "#### Epoch: 95 of 100 ####\n",
      " Training loss:\t0.0969392890172289\n",
      " Test loss:\t0.09199635799114521\n",
      " Validation loss:\t0.09502163300147423\n",
      "\n",
      " NO EARLY STOPPING\n",
      "Hidden_channels =  256 \tembedding_dim =  128 \tnHeads =  8\n",
      "Training Loss:  0.09695525516737376 \tTesting loss:  0.09198159437913161 \tValidation Loss:  0.095032655275785 \n",
      "\n",
      "\n",
      "network counter: 18\n",
      "#### Epoch: 0 of 100 ####\n",
      " Training loss:\t0.45531396102216287\n",
      " Test loss:\t0.1673153547140268\n",
      " Validation loss:\t0.17113843330970177\n",
      "#### Epoch: 5 of 100 ####\n",
      " Training loss:\t0.1258492742512041\n",
      " Test loss:\t0.11361814462221585\n",
      " Validation loss:\t0.11673489900735709\n",
      "#### Epoch: 10 of 100 ####\n",
      " Training loss:\t0.11676512275525851\n",
      " Test loss:\t0.11739594202775222\n",
      " Validation loss:\t0.12044327992659348\n",
      "#### Epoch: 15 of 100 ####\n",
      " Training loss:\t0.11261282133668536\n",
      " Test loss:\t0.10672180469219501\n",
      " Validation loss:\t0.10947721738081712\n",
      "#### Epoch: 20 of 100 ####\n",
      " Training loss:\t0.10954014584646392\n",
      " Test loss:\t0.1088033731167133\n",
      " Validation loss:\t0.1112917294869056\n",
      "#### Epoch: 25 of 100 ####\n",
      " Training loss:\t0.10708532729542163\n",
      " Test loss:\t0.10154118904700646\n",
      " Validation loss:\t0.10460241941305307\n",
      "#### Epoch: 30 of 100 ####\n",
      " Training loss:\t0.10506580221875471\n",
      " Test loss:\t0.10504755607018104\n",
      " Validation loss:\t0.10800198408273551\n",
      "#### Epoch: 35 of 100 ####\n",
      " Training loss:\t0.10395729391062425\n",
      " Test loss:\t0.10087012327634372\n",
      " Validation loss:\t0.10383661893697885\n",
      "#### Epoch: 40 of 100 ####\n",
      " Training loss:\t0.10280746469147445\n",
      " Test loss:\t0.10265168776878944\n",
      " Validation loss:\t0.10572380285996658\n",
      "#### Epoch: 45 of 100 ####\n",
      " Training loss:\t0.10212722217943351\n",
      " Test loss:\t0.10580072953150822\n",
      " Validation loss:\t0.10876617064842811\n",
      "#### Epoch: 50 of 100 ####\n",
      " Training loss:\t0.10137928429315925\n",
      " Test loss:\t0.1064841105387761\n",
      " Validation loss:\t0.10965772775503305\n",
      "#### Epoch: 55 of 100 ####\n",
      " Training loss:\t0.100989859051047\n",
      " Test loss:\t0.10561833931849553\n",
      " Validation loss:\t0.1086364801113422\n",
      "#### Epoch: 60 of 100 ####\n",
      " Training loss:\t0.10052293643113147\n",
      " Test loss:\t0.10339606725252591\n",
      " Validation loss:\t0.1062381909443782\n",
      "#### Epoch: 65 of 100 ####\n",
      " Training loss:\t0.10032529790741647\n",
      " Test loss:\t0.10506961895869328\n",
      " Validation loss:\t0.10797600562755878\n",
      "#### Epoch: 70 of 100 ####\n",
      " Training loss:\t0.1000483063313704\n",
      " Test loss:\t0.10563286451193002\n",
      " Validation loss:\t0.10848636810596173\n",
      "#### Epoch: 75 of 100 ####\n",
      " Training loss:\t0.10006134198560261\n",
      " Test loss:\t0.10260084042182335\n",
      " Validation loss:\t0.10549581967867337\n",
      "#### Epoch: 80 of 100 ####\n",
      " Training loss:\t0.09980185070646448\n",
      " Test loss:\t0.10768719819875863\n",
      " Validation loss:\t0.11053698796492356\n",
      "#### Epoch: 85 of 100 ####\n",
      " Training loss:\t0.09985075967273103\n",
      " Test loss:\t0.10262783674093393\n",
      " Validation loss:\t0.10548150539398193\n",
      "#### Epoch: 90 of 100 ####\n",
      " Training loss:\t0.09972535465798558\n",
      " Test loss:\t0.10589103515331562\n",
      " Validation loss:\t0.10873557971074031\n",
      "#### Epoch: 95 of 100 ####\n",
      " Training loss:\t0.0995841764097541\n",
      " Test loss:\t0.10376880719111516\n",
      " Validation loss:\t0.10663825732011062\n",
      "\n",
      " NO EARLY STOPPING\n",
      "Hidden_channels =  256 \tembedding_dim =  128 \tnHeads =  16\n",
      "Training Loss:  0.09955940958536368 \tTesting loss:  0.10429469438699576 \tValidation Loss:  0.1071015871488131 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hidden_channels = 128\n",
    "embedding_dim = 32\n",
    "learningRate = 0.001\n",
    "d_target = 15\n",
    "vocab_size = len(tokenDict)+1\n",
    "nHeads = 16\n",
    "\n",
    "# paramList = [{hidden_channels : 128, embedding_dim : 32, nHeads : 16},\n",
    "#              {hidden_channels : 128, embedding_dim : 32, nHeads : 16},\n",
    "#              {hidden_channels : 128, embedding_dim : 32, nHeads : 16},\n",
    "#              {hidden_channels : 128, embedding_dim : 32, nHeads : 16}]\n",
    "\n",
    "\n",
    "torch.manual_seed(12345)\n",
    "\n",
    "model = TransformerNetwork(hidden_channels,d_target,vocab_size, embedding_dim, nHeads, maxTokenLength).to(device) \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learningRate, weight_decay=5e-4)\n",
    "# decay learning rate\n",
    "decayRate = 0.95\n",
    "lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=decayRate)\n",
    "criterion = torch.nn.MSELoss().to(device)\n",
    "\n",
    "def train(data_in, targets):\n",
    "      model.train()\n",
    "      optimizer.zero_grad()  # Clear gradients.\n",
    "      out = model(data_in)\n",
    "      loss = criterion(out, targets)\n",
    "      \n",
    "      loss.backward()  # Derive gradients.\n",
    "      optimizer.step()  # Update parameters based on gradients.\n",
    "      return loss\n",
    "\n",
    "def infer(data_in, targets):\n",
    "      with torch.no_grad():\n",
    "            model.eval()\n",
    "            out = model(data_in)\n",
    "            loss = criterion(out, targets)\n",
    "      return loss\n",
    "\n",
    "parameterTest = True\n",
    "earlyStop = False\n",
    "stopTolerance = 3\n",
    "minDiff = 0.01\n",
    "epoch = 0\n",
    "finalLoss = []\n",
    "\n",
    "if parameterTest == False:\n",
    "      lossList = []\n",
    "      counter = 0\n",
    "      while not earlyStop and epoch < 1000:\n",
    "            loss = 0\n",
    "            for input,targets in trainingData:\n",
    "                  loss += train(input,targets).detach()\n",
    "            \n",
    "            test = 0\n",
    "            for input, targets in testData:\n",
    "                  test += infer(input,targets)\n",
    "\n",
    "            val = 0\n",
    "            for input,targets in validationData:\n",
    "                  val += infer(input,targets)\n",
    "            \n",
    "            lr_scheduler.step()\n",
    "            lossList.append([loss.item()/len(trainingData), test.item()/len(testData), val.item()/len(validationData)])\n",
    "            # Early stopping\n",
    "            if (lossList[epoch][2]-lossList[epoch][0]) > minDiff:\n",
    "                  counter += 1\n",
    "            else:\n",
    "                  counter = 0\n",
    "                  if counter >= stopTolerance:\n",
    "                        earlyStop = True         \n",
    "            print(f\"#### Epoch: {epoch} of 1000 ####\\n Training loss:\\t{lossList[epoch][0]}\\n Test loss:\\t{lossList[epoch][1]}\\n Validation loss:\\t{lossList[epoch][2]}\")\n",
    "            epoch+=1          \n",
    "else: # THIS LOOP RUNS WHEN DOING PARAMETER TESTING CHANGE BOOL TO TURN OFF\n",
    "      c = 0\n",
    "      for currentParam in parameterList:\n",
    "            c+=1\n",
    "            print(\"network counter:\", c)\n",
    "            hidden_channels = currentParam['hidden_channels']\n",
    "            embedding_dim = currentParam['embedding_dim']\n",
    "            nHeads = currentParam['nHeads']\n",
    "            \n",
    "            model = TransformerNetwork(hidden_channels=hidden_channels, output_dim=d_target, embedding_dim=embedding_dim,\n",
    "                                       vocab_size=vocab_size, num_heads=nHeads, maxTokenLength=maxTokenLength).to(device) \n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=learningRate, weight_decay=5e-4)\n",
    "            # decay learning rate\n",
    "            decayRate = 0.94\n",
    "            lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=decayRate)\n",
    "            criterion = torch.nn.MSELoss().to(device)\n",
    "            \n",
    "            lossList = []\n",
    "            counter = 0\n",
    "            epoch = 0\n",
    "            earlyStop = False\n",
    "            while not earlyStop and epoch < 100:\n",
    "                  loss = 0\n",
    "                  for input,targets in trainingData:\n",
    "                        loss += train(input,targets).detach()\n",
    "                  \n",
    "                  test = 0\n",
    "                  for input, targets in testData:\n",
    "                        test += infer(input,targets)\n",
    "\n",
    "                  val = 0\n",
    "                  for input,targets in validationData:\n",
    "                        val += infer(input,targets)\n",
    "                  \n",
    "                  lr_scheduler.step()\n",
    "                  lossList.append([loss.item()/len(trainingData), test.item()/len(testData), val.item()/len(validationData)])\n",
    "                  # Early stopping\n",
    "                  if (lossList[epoch][2]-lossList[epoch][0]) > minDiff:\n",
    "                        counter += 1\n",
    "                        if counter >= stopTolerance:\n",
    "                              print('\\n EARLY STOPPING')\n",
    "                              print('Hidden_channels = ',currentParam['hidden_channels'],'\\tembedding_dim = ',currentParam['embedding_dim'], '\\tnHeads = ',currentParam['nHeads'])\n",
    "                              print('Training Loss: ', lossList[epoch][0], '\\tTesting loss: ', lossList[epoch][1], '\\tValidation Loss: ', lossList[epoch][2],'\\n\\n')\n",
    "                              finalLoss.append(lossList[epoch])\n",
    "                              earlyStop = True\n",
    "                  else:\n",
    "                        counter = 0       \n",
    "\n",
    "                  if epoch%5 == 0:\n",
    "                        print(f\"#### Epoch: {epoch} of 100 ####\\n Training loss:\\t{lossList[epoch][0]}\\n Test loss:\\t{lossList[epoch][1]}\\n Validation loss:\\t{lossList[epoch][2]}\")\n",
    "                  epoch += 1\n",
    "            if earlyStop == False:\n",
    "                  epoch -= 1\n",
    "                  print('\\n NO EARLY STOPPING')\n",
    "                  print('Hidden_channels = ',currentParam['hidden_channels'],'\\tembedding_dim = ',currentParam['embedding_dim'], '\\tnHeads = ',currentParam['nHeads'])\n",
    "                  print('Training Loss: ', lossList[epoch][0], '\\tTesting loss: ', lossList[epoch][1], '\\tValidation Loss: ', lossList[epoch][2],'\\n\\n')\n",
    "                  finalLoss.append(lossList[epoch])\n",
    "                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.22219835771701912, 0.18370749400212213, 0.1861894314105694]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(finalLoss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
