{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TIF360 Project"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main source: https://www.kaggle.com/code/rmonge/predicting-molecule-properties-based-on-its-smiles/notebook"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time as time\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "import networkx as nx\n",
    "from torch.nn import Linear, LeakyReLU\n",
    "from torch_geometric.nn import global_mean_pool, GATConv, BatchNorm, GraphNorm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available: False\n",
      "device: cpu\n"
     ]
    }
   ],
   "source": [
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"device:\", \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load smiles data and targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(132820, 21)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/smiles_and_targets.csv\")\n",
    "print(np.shape(df))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose mode: Which features to include"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# singles\n",
    "# mode = \"rdkit_descriptors\"\n",
    "# mode = \"morgan_fingerprints\"\n",
    "# mode = \"mordred_descriptors\"\n",
    "\n",
    "# combinations\n",
    "# mode = \"rdkit+morgan\"\n",
    "# mode = \"mordred+morgan\"\n",
    "# mode = \"rdkit+mordred\"\n",
    "mode = \"rdkit+mordred+morgan\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features: (132820, 2227)\n"
     ]
    }
   ],
   "source": [
    "from utility_functions import load_molecular_features\n",
    "\n",
    "features = load_molecular_features(mode)\n",
    "print(\"features:\", np.shape(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from utility_functions import get_data_split_indices, scale_targets\n",
    "\n",
    "def create_scale_and_split_dataset(features:np.ndarray, targets:np.ndarray, val_share, test_share, batch_size):\n",
    "    \n",
    "    num_samples = features.shape[0]\n",
    "    print(\"num_samples:\", num_samples)\n",
    "    train_indices, val_indices, test_indices = get_data_split_indices(num_samples, val_share, test_share)\n",
    "    print(\"train_indices:\", train_indices)\n",
    "    print(\"val_indices:\", val_indices)\n",
    "    print(\"test_indices:\", test_indices)\n",
    "    \n",
    "    X_train, y_train = features[train_indices], targets[train_indices]\n",
    "    X_val, y_val = features[val_indices], targets[val_indices]\n",
    "    X_test, y_test = features[test_indices], targets[test_indices]\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_val = scaler.transform(X_val)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    y_train, y_val, y_test, scaler_targets = scale_targets(y_train, y_val, y_test)\n",
    "    \n",
    "    train_data = TensorDataset(torch.FloatTensor(X_train, device=device),\n",
    "                               torch.FloatTensor(y_train, device=device))\n",
    "    val_data = TensorDataset(torch.FloatTensor(X_val, device=device),   \n",
    "                             torch.FloatTensor(y_val, device=device))\n",
    "    test_data = TensorDataset(torch.FloatTensor(X_test, device=device),\n",
    "                              torch.FloatTensor(y_test, device=device))\n",
    "\n",
    "    return train_data, val_data, test_data, scaler_targets\n",
    "\n",
    "def create_data_loaders(train_data, val_data, test_data, batch_size):\n",
    "    \n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Loading data...\n",
      "num_samples: 132820\n",
      "train_indices: [127438   5606   9214 ...  64809  71506  76175]\n",
      "val_indices: [ 84700 105937  14560 ... 129099  46956  43265]\n",
      "test_indices: [ 26727  53117 114907 ...  85412 130333  77285]\n",
      "...Data loading done...\n"
     ]
    }
   ],
   "source": [
    "print(\"...Loading data...\")\n",
    "properties_names = ['A', 'B', 'C', 'mu', 'alfa', 'homo', 'lumo', 'gap', 'RÂ²', 'zpve', 'U0', 'U', 'H', 'G', 'Cv']\n",
    "x_smiles = df.smiles.values\n",
    "y = df.loc[:, properties_names].values\n",
    "\n",
    "# features = np.concatenate((mol_descriptor), axis=1)\n",
    "features = mol_descriptor\n",
    "\n",
    "train_data, val_data, test_data, scaler_targets = create_scale_and_split_dataset(features, y, 0.15, 0.2, 64)\n",
    "train_loader, val_loader, test_loader = create_data_loaders(train_data, val_data, test_data, 64)\n",
    "print(\"...Data loading done...\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense network "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model for all targets at once"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, feature_dim, target_dim):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        \n",
    "        self.input_norm = BatchNorm(feature_dim)\n",
    "        self.lin1 = Linear(feature_dim, hidden_channels)\n",
    "        self.lin2 = Linear(hidden_channels, 512)\n",
    "        self.lin3 = Linear(512, target_dim)\n",
    "\n",
    "    def forward(self, x): \n",
    "                \n",
    "        x = self.input_norm(x)\n",
    "        x = self.lin1(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        \n",
    "        x = self.lin2(x)\n",
    "        x = F.leaky_relu(x)\n",
    "\n",
    "        x = self.lin3(x)\n",
    " \n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 717173\n",
      "Trainable parameters: 717173\n",
      "\n",
      "...Starting training...\n",
      "Device used: cpu\n",
      "Epoch: 01 | Train Loss: 0.15350 | Validation Loss: 0.11355\n",
      "Validation R2: [0.59545072 0.78840536 0.87502583 0.5469624  0.95366012 0.8124005\n",
      " 0.93087593 0.89631883 0.90801047 0.98002215 0.98621267 0.98523387\n",
      " 0.9851991  0.98607401 0.97075313]\n",
      "Validation loss: 0.11355060066741246\n",
      "Epoch: 02 | Train Loss: 0.12704 | Validation Loss: 0.11098\n",
      "Validation R2: [0.65804845 0.8042125  0.87799829 0.58014746 0.95171283 0.80101679\n",
      " 0.91997258 0.90320567 0.91980022 0.95029874 0.98381823 0.98310717\n",
      " 0.98257769 0.98254086 0.95773256]\n",
      "Validation loss: 0.11097705612579982\n",
      "Epoch: 03 | Train Loss: 0.12132 | Validation Loss: 0.10546\n",
      "...Training done...\n",
      "...Calculating final results...\n",
      "====================================================\n",
      "Final training R2: [0.65523054 0.8111751  0.89372835 0.58501221 0.95490911 0.81181616\n",
      " 0.92402429 0.91409194 0.90623216 0.96590456 0.99248892 0.99258645\n",
      " 0.99252159 0.99257691 0.95650353]\n",
      "Average final training R2:  0.889920121434095\n",
      "Final training loss: 0.10498162168454382\n",
      "Final validation R2: [0.64422475 0.81421322 0.89363718 0.57688676 0.95514544 0.81378838\n",
      " 0.92287797 0.91133016 0.90654802 0.96564163 0.99216757 0.99225061\n",
      " 0.9922298  0.99222566 0.95747199]\n",
      "Average validation R2:  0.888709276041606\n",
      "Final validation loss: 0.10545679812247936\n",
      "Final test R2: [0.6510727  0.8106871  0.89235487 0.55673554 0.95499193 0.80872592\n",
      " 0.92298513 0.91285229 0.90466843 0.96608659 0.99169896 0.99177378\n",
      " 0.99173217 0.99174958 0.95609257]\n",
      "Average final test R2:  0.8869471706294123\n",
      "Final test loss: 0.10651124782788639\n"
     ]
    }
   ],
   "source": [
    "from utility_functions import get_num_parameters\n",
    "\n",
    "feature_dim = train_data[:][0].shape[1]\n",
    "target_dim = train_data[:][1].shape[1]\n",
    "\n",
    "model = DNN(hidden_channels=1024, feature_dim=feature_dim, target_dim=target_dim).to(device) \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "\n",
    "# Decay for learning rate\n",
    "decayRate = 0.9\n",
    "lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=decayRate)\n",
    "\n",
    "criterion = torch.nn.MSELoss().to(device)\n",
    "\n",
    "train_params, tot_params = get_num_parameters(model)\n",
    "print(f\"Total number of parameters: {tot_params}\")\n",
    "print(f\"Trainable parameters: {train_params}\")\n",
    "\n",
    "def train(batch):\n",
    "      features = batch[:][0]\n",
    "      targets = batch[:][1]\n",
    "      \n",
    "      model.train()\n",
    "      optimizer.zero_grad()  # Clear gradients.\n",
    "      out = model(features).to(device)  # Perform a single forward pass.\n",
    "\n",
    "      loss = criterion(out, targets) \n",
    "\n",
    "      loss.backward()  # Derive gradients.\n",
    "      optimizer.step()  # Update parameters based on gradients.\n",
    "      \n",
    "      return loss\n",
    "\n",
    "def test(data):\n",
    "      for i, batch in enumerate(data):\n",
    "            features = batch[:][0]\n",
    "            targets = batch[:][1].cpu()\n",
    "            \n",
    "            model.eval()\n",
    "            out = model(features).cpu()\n",
    "            \n",
    "            # Caculate R2 for each target\n",
    "            for target_idx in range(target_dim):\n",
    "                  if target_idx != 0:\n",
    "                        r2_score_var = np.vstack((r2_score_var, r2_score(targets[:,target_idx].detach().numpy(), \n",
    "                                                          out[:,target_idx].detach().numpy())))\n",
    "                  else:\n",
    "                        r2_score_var = np.array([r2_score(targets[:,target_idx].detach().numpy(),\n",
    "                                                          out[:,target_idx].detach().numpy())])        \n",
    "            all_r2 = np.hstack((all_r2, r2_score_var)) if i != 0 else r2_score_var\n",
    "                  \n",
    "            loss = float(criterion(out, targets).detach().numpy())\n",
    "            all_loss = np.hstack((all_loss, loss)) if i != 0 else np.array(loss)\n",
    "\n",
    "      average_test_r2 = np.mean(all_r2, axis=1)\n",
    "      average_test_loss = np.mean(all_loss)\n",
    "      \n",
    "      return average_test_r2, average_test_loss\n",
    "\n",
    "# Vectors to append accuracy to:\n",
    "train_r2 = []\n",
    "train_loss = []\n",
    "test_r2 = []\n",
    "test_loss = []\n",
    "val_r2 = []\n",
    "val_loss = []\n",
    "\n",
    "n_epochs = 30\n",
    "print_every_N_epochs = True\n",
    "N = 10 # print R2 every N epochs\n",
    "\n",
    "epoch_times = []\n",
    "train_times = []\n",
    "test_times = []\n",
    "print()\n",
    "print(\"...Starting training...\")\n",
    "print(\"Device used:\", device)\n",
    "\n",
    "for epoch in np.arange(1, n_epochs+1):\n",
    "      epoch_start = time.time()\n",
    "      losses = []\n",
    "      train_start = time.time()\n",
    "      for batch in train_loader:\n",
    "            loss = train(batch)\n",
    "            losses.append(loss.cpu().detach().numpy())  \n",
    "      # Compute validation loss\n",
    "      model.eval()\n",
    "      val_losses = []\n",
    "      for batch in val_loader:\n",
    "            features = batch[:][0]\n",
    "            targets = batch[:][1]\n",
    "            out = model(features)\n",
    "            val_losses.append(criterion(out, targets).cpu().detach().numpy()) \n",
    "\n",
    "      print(f\"Epoch: {epoch:02d} | Train Loss: {np.mean(losses):.5f} | Validation Loss: {np.mean(val_losses):.5f}\")\n",
    "      train_end = time.time()\n",
    "      train_times.append(train_end - train_start)\n",
    "      \n",
    "      lr_scheduler.step() # Decay to learning rate\n",
    "      \n",
    "      if print_every_N_epochs and (epoch % N == 0 or epoch == 1) and epoch != n_epochs:\n",
    "            test_start = time.time()\n",
    "                \n",
    "            r2_temp_val, loss_temp_val = test(val_loader)\n",
    "            val_r2.append(r2_temp_val)\n",
    "            val_loss.append(loss_temp_val)\n",
    "            \n",
    "            print(f'Validation R2: {r2_temp_val}')\n",
    "            print(f\"Validation loss: {loss_temp_val}\")\n",
    "            test_end = time.time()\n",
    "            test_times.append(test_end - test_start)\n",
    "            \n",
    "      if epoch == n_epochs:         # calculate results of training: test on all data\n",
    "            print(\"...Training done...\")\n",
    "            print(\"...Calculating final results...\")\n",
    "            test_start = time.time()\n",
    "            r2_temp_train, loss_temp_train = test(train_loader) \n",
    "            train_r2.append(r2_temp_train)\n",
    "            train_loss.append(loss_temp_train)\n",
    "            \n",
    "            r2_temp_val, loss_temp_val = test(val_loader)\n",
    "            val_r2.append(r2_temp_val)\n",
    "            val_loss.append(loss_temp_val)\n",
    "            \n",
    "            r2_temp_test, loss_temp_test = test(test_loader)\n",
    "            test_r2.append(r2_temp_test)\n",
    "            test_loss.append(loss_temp_test)\n",
    "\n",
    "            print(\"====================================================\")\n",
    "            print(\"Final training R2:\", train_r2[-1])\n",
    "            print(\"Average final training R2: \", np.mean(train_r2[-1]))\n",
    "            print(\"Final training loss:\", train_loss[-1])\n",
    "            \n",
    "            print(\"Final validation R2:\", val_r2[-1])\n",
    "            print(\"Average validation R2: \", np.mean(val_r2[-1]))\n",
    "            print(\"Final validation loss:\", val_loss[-1])\n",
    "\n",
    "            print(\"Final test R2:\", test_r2[-1])\n",
    "            print(\"Average final test R2: \", np.mean(test_r2[-1]))\n",
    "            print(\"Final test loss:\", test_loss[-1])\n",
    "            \n",
    "            test_end = time.time()\n",
    "            test_times.append(test_end - test_start)\n",
    "            \n",
    "      epoch_end = time.time()\n",
    "      epoch_times.append(epoch_end - epoch_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used: cpu\n",
      "\n",
      "Total number of epochs: 3\n",
      "Total training time: 1.30 minutes\n",
      "Total time in training: 0.93 minutes\n",
      "Total time in testing: 0.37 minutes\n",
      "\n",
      "Average epoch time: 25.9 seconds\n",
      "Average time in training: 18.6 seconds\n",
      "Average time in testing: 7.4 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"Device used:\", device)\n",
    "print()\n",
    "print(f\"Total number of epochs: {len(epoch_times)}\")\n",
    "print(f\"Total training time: {np.sum(epoch_times)/60:.2f} minutes\")\n",
    "print(f\"Total time in training: {np.sum(train_times)/60:.2f} minutes\")\n",
    "print(f\"Total time in testing: {np.sum(test_times)/60:.2f} minutes\")\n",
    "print()\n",
    "print(f\"Average epoch time: {np.mean(epoch_times):.1f} seconds\")\n",
    "print(f\"Average time in training: {np.mean(train_times):.1f} seconds\")\n",
    "print(f\"Average time in testing: {np.mean(test_times):.1f} seconds\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model for just one target"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target index: 0\n",
      "Device used: cpu\n",
      "Epoch: 01 | Train Loss: 0.43022 | Validation Loss: 0.33497\n",
      "Epoch: 02 | Train Loss: 0.34407 | Validation Loss: 0.29044\n",
      "Epoch: 03 | Train Loss: 0.30709 | Validation Loss: 0.26470\n",
      "Epoch: 04 | Train Loss: 0.28532 | Validation Loss: 0.25244\n",
      "Epoch: 05 | Train Loss: 0.27400 | Validation Loss: 0.24770\n",
      "Epoch: 06 | Train Loss: 0.25970 | Validation Loss: 0.23591\n",
      "Epoch: 07 | Train Loss: 0.24940 | Validation Loss: 0.23370\n",
      "Epoch: 08 | Train Loss: 0.24348 | Validation Loss: 0.21924\n",
      "Epoch: 09 | Train Loss: 0.23632 | Validation Loss: 0.22306\n",
      "Epoch: 10 | Train Loss: 0.23171 | Validation Loss: 0.21338\n",
      "====================================================\n",
      "Final training R2: 0.7936178688244215\n",
      "Average final training R2:  0.7936178688244215\n",
      "Final training loss: 0.1921136474569609\n",
      "\n",
      "Final validation R2: 0.7652245657012323\n",
      "Average validation test R2:  0.7652245657012323\n",
      "Final validation loss: 0.2133761847105164\n",
      "\n",
      "Final test R2: 0.7735055394010021\n",
      "Average final test R2:  0.7735055394010021\n",
      "Final test loss: 0.21428167128648895\n",
      "====================================================\n",
      "Target index: 1\n",
      "Device used: cpu\n",
      "Epoch: 01 | Train Loss: 0.27642 | Validation Loss: 0.20748\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 67\u001b[0m\n\u001b[0;32m     65\u001b[0m losses \u001b[39m=\u001b[39m []\n\u001b[0;32m     66\u001b[0m \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m train_loader:\n\u001b[1;32m---> 67\u001b[0m       loss \u001b[39m=\u001b[39m train(data, target_index)\n\u001b[0;32m     68\u001b[0m       losses\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy())  \n\u001b[0;32m     69\u001b[0m \u001b[39m# Compute validation loss\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[10], line 10\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(batch, target_idx)\u001b[0m\n\u001b[0;32m      8\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m      9\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()  \u001b[39m# Clear gradients.\u001b[39;00m\n\u001b[1;32m---> 10\u001b[0m out \u001b[39m=\u001b[39m model(features)\u001b[39m.\u001b[39mto(device)  \u001b[39m# Perform a single forward pass.\u001b[39;00m\n\u001b[0;32m     12\u001b[0m loss \u001b[39m=\u001b[39m criterion(out, targets) \n\u001b[0;32m     14\u001b[0m loss\u001b[39m.\u001b[39mbackward()  \u001b[39m# Derive gradients.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Jesper\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[6], line 18\u001b[0m, in \u001b[0;36mDNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     15\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mleaky_relu(x)\n\u001b[0;32m     16\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mdropout(x, p\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m, training\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining)\n\u001b[1;32m---> 18\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlin2(x)\n\u001b[0;32m     19\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mleaky_relu(x)\n\u001b[0;32m     21\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlin3(x)\n",
      "File \u001b[1;32mc:\\Users\\Jesper\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Jesper\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "feature_dim = train_data[:][0].shape[1]\n",
    "target_dim = 1\n",
    "\n",
    "def train(batch, target_idx):\n",
    "      features = batch[:][0]\n",
    "      targets = batch[:][1][:,target_idx].unsqueeze(-1)\n",
    "      \n",
    "      model.train()\n",
    "      optimizer.zero_grad()  # Clear gradients.\n",
    "      out = model(features).to(device)  # Perform a single forward pass.\n",
    "\n",
    "      loss = criterion(out, targets) \n",
    "\n",
    "      loss.backward()  # Derive gradients.\n",
    "      optimizer.step()  # Update parameters based on gradients.\n",
    "      \n",
    "      return loss\n",
    "\n",
    "def test(data, target_idx):\n",
    "      for i, batch in enumerate(data):\n",
    "            features = batch[:][0]\n",
    "            targets = batch[:][1][:,target_idx].cpu().unsqueeze(-1)\n",
    "            \n",
    "            model.eval()\n",
    "            out = model(features).cpu()\n",
    "            \n",
    "            # Caculate R2    \n",
    "            r2_score_var = np.array([r2_score(targets.detach().numpy(), out.detach().numpy())])        \n",
    "            all_r2 = np.hstack((all_r2, r2_score_var)) if i != 0 else r2_score_var\n",
    "                  \n",
    "            loss = float(criterion(out, targets).detach().numpy())\n",
    "            all_loss = np.hstack((all_loss, loss)) if i != 0 else np.array(loss)\n",
    "\n",
    "      average_test_r2 = np.mean(all_r2)\n",
    "      average_test_loss = np.mean(all_loss)\n",
    "      \n",
    "      return average_test_r2, average_test_loss\n",
    "\n",
    "num_targets = train_data[:][1].shape[1]\n",
    "start_time = time.time()\n",
    "for target_index in range(num_targets):\n",
    "      print(\"Target index:\", target_index)\n",
    "\n",
    "      model = DNN(hidden_channels=1024, feature_dim=feature_dim, target_dim=target_dim).to(device) \n",
    "      optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=5e-4)\n",
    "      criterion = torch.nn.MSELoss().to(device)\n",
    "      \n",
    "      # Decay for learning rate\n",
    "      decayRate = 0.9\n",
    "      lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=decayRate)\n",
    "\n",
    "      # Vectors to append accuracy to:\n",
    "      train_r2 = []\n",
    "      train_loss = []\n",
    "      test_r2 = []\n",
    "      test_loss = []\n",
    "      val_r2 = []\n",
    "      val_loss = []\n",
    "\n",
    "      n_epochs = 10\n",
    "\n",
    "      print(\"Device used:\", device)\n",
    "\n",
    "      for epoch in np.arange(1, n_epochs+1):\n",
    "            losses = []\n",
    "            for data in train_loader:\n",
    "                  loss = train(data, target_index)\n",
    "                  losses.append(loss.cpu().detach().numpy())  \n",
    "            # Compute validation loss\n",
    "            model.eval()\n",
    "            val_losses = []\n",
    "            for batch in val_loader:\n",
    "                  features = batch[:][0]\n",
    "                  targets = batch[:][1][:, target_index].unsqueeze(-1)\n",
    "                  out = model(features)\n",
    "                  val_losses.append(criterion(out, targets).cpu().detach().numpy())\n",
    "            print(f\"Epoch: {epoch:02d} | Train Loss: {np.mean(losses):.5f} | Validation Loss: {np.mean(val_losses):.5f}\")\n",
    "            \n",
    "            lr_scheduler.step() # Decay to learning rate\n",
    "                  \n",
    "            if epoch == n_epochs:         # calculate results of training: test on all data\n",
    "                  test_start = time.time()\n",
    "                  r2_temp_train, loss_temp_train = test(train_loader, target_index) \n",
    "                  train_r2.append(r2_temp_train)\n",
    "                  train_loss.append(loss_temp_train)\n",
    "                  \n",
    "                  r2_temp_val, loss_temp_val = test(val_loader, target_index)\n",
    "                  val_r2.append(r2_temp_val)\n",
    "                  val_loss.append(loss_temp_val)\n",
    "                  \n",
    "                  r2_temp_test, loss_temp_test = test(test_loader, target_index)\n",
    "                  test_r2.append(r2_temp_test)\n",
    "                  test_loss.append(loss_temp_test)\n",
    "\n",
    "                  print(\"====================================================\")\n",
    "                  print(\"Final training R2:\", train_r2[-1])\n",
    "                  print(\"Average final training R2: \", np.mean(train_r2[-1]))\n",
    "                  print(\"Final training loss:\", train_loss[-1])\n",
    "                  print()\n",
    "                  print(\"Final validation R2:\", val_r2[-1])\n",
    "                  print(\"Average validation test R2: \", np.mean(val_r2[-1]))\n",
    "                  print(\"Final validation loss:\", val_loss[-1])\n",
    "                  print()\n",
    "                  print(\"Final test R2:\", test_r2[-1])\n",
    "                  print(\"Average final test R2: \", np.mean(test_r2[-1]))\n",
    "                  print(\"Final test loss:\", test_loss[-1])\n",
    "                  print(\"====================================================\")\n",
    "                                    \n",
    "print(\"...Done...\")\n",
    "end_time = time.time()\n",
    "print(f\"Time taken: {(end_time - start_time)/60} minutes\")\n",
    "print(f\"Average time per target: {(end_time - start_time)/(num_targets*60)} minutes\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
