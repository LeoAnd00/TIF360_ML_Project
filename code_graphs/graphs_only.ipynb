{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TIF360 Project"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main source: https://www.kaggle.com/code/rmonge/predicting-molecule-properties-based-on-its-smiles/notebook"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time as time\n",
    "import torch\n",
    "import torch_geometric\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.nn import Linear, LeakyReLU\n",
    "from torch_geometric.nn import global_mean_pool, GATConv, BatchNorm, GraphNorm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "import utility_functions as uf  # defined in utility_functions.py\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"once\") # only displays the warning once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available: True\n",
      "device: cuda\n",
      "cuda version: 11.8\n",
      "gpu: NVIDIA GeForce RTX 3080\n"
     ]
    }
   ],
   "source": [
    "# check if cuda is available\n",
    "print('cuda available:', torch.cuda.is_available())\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('device:', \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    print('cuda version:', torch.version.cuda)\n",
    "    print('gpu:', torch.cuda.get_device_name(0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(132820, 21)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/smiles_and_targets.csv\")\n",
    "print(np.shape(df))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert data to graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph_dataset_functions import create_graph_dataset_from_smiles\n",
    "\n",
    "properties_names = ['A', 'B', 'C', 'mu', 'alfa', 'homo', 'lumo', 'gap', 'RÂ²', 'zpve', 'U0', 'U', 'H', 'G', 'Cv']\n",
    "\n",
    "x_smiles = df.smiles.values\n",
    "y = df.loc[:, properties_names].values  # shape = (n_samples, n_properties)\n",
    "\n",
    "dataset = create_graph_dataset_from_smiles(x_smiles, y[0:len(x_smiles), :])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information of the graph dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of graphs (molecules): 132820\n",
      "=================================================================================\n",
      "Properties of graph 50, molecule smiles: CC1=CNC=C1\n",
      "Number of nodes: 6\n",
      "Number of edges: 12\n",
      "Number of node features: 78\n",
      "Number of edge features: 10\n",
      "Number of target properties: 15\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of graphs (molecules): {len(dataset)}')\n",
    "graph = dataset[50]\n",
    "print('=================================================================================')\n",
    "print(f'Properties of graph {50}, molecule smiles: {df.smiles.values[50]}')\n",
    "print(f'Number of nodes: {dataset[50].x.shape[0]}')\n",
    "print(f'Number of edges: {dataset[50].edge_index.shape[1]}')\n",
    "print(f'Number of node features: {dataset[50].x.shape[1]}')\n",
    "print(f'Number of edge features: {dataset[50].edge_attr.shape[1]}')\n",
    "print(f'Number of target properties: {dataset[50].y.shape[1]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create functions to load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_and_split_data(dataset, val_share, test_share):\n",
    "    # split the dataset into test and validation:\n",
    "    num_samples = len(dataset)\n",
    "\n",
    "    num_samples = len(dataset)\n",
    "\n",
    "    train_indices, val_indices, test_indices = uf.get_data_split_indices(num_samples, val_share=val_share, test_share=test_share)\n",
    "\n",
    "    train_data = [dataset[i] for i in train_indices]\n",
    "    val_data = [dataset[i] for i in val_indices]\n",
    "    test_data = [dataset[i] for i in test_indices]\n",
    "\n",
    "    # scale the targets\n",
    "    train_targets = np.concatenate([data.y for data in train_data], axis=0)\n",
    "    val_targets = np.concatenate([data.y for data in val_data], axis=0)\n",
    "    test_targets = np.concatenate([data.y for data in val_data], axis=0)    \n",
    "    \n",
    "    train_targets, val_targets, test_targets, scaler_targets = uf.scale_targets(train_targets, val_targets, test_targets)\n",
    "\n",
    "    train_targets = torch.tensor(train_targets, dtype=torch.float, device=device)\n",
    "    val_targets = torch.tensor(val_targets, dtype=torch.float, device=device)\n",
    "    test_targets = torch.tensor(test_targets, dtype=torch.float, device=device)\n",
    "\n",
    "\n",
    "    train_data = [Data(x=data.x.to(device), edge_index=data.edge_index.to(device), edge_attr=data.edge_attr.to(device), \n",
    "                       y=train_targets[index].reshape(1,-1)) for index, data in enumerate(train_data)]\n",
    "    \n",
    "    val_data = [Data(x=data.x.to(device), edge_index=data.edge_index.to(device), edge_attr=data.edge_attr.to(device),\n",
    "                        y=val_targets[index].reshape(1,-1)) for index, data in enumerate(val_data)]\n",
    "    \n",
    "    test_data = [Data(x=data.x.to(device), edge_index=data.edge_index.to(device), edge_attr=data.edge_attr.to(device), \n",
    "                      y=test_targets[index].reshape(1,-1)) for index, data in enumerate(test_data)]\n",
    "\n",
    "    return train_data, val_data, test_data, scaler_targets\n",
    "\n",
    "def create_data_loaders(train_data, val_data, test_data, batch_size): \n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Loading data...\n",
      "...Data loading done...\n"
     ]
    }
   ],
   "source": [
    "print(\"...Loading data...\")\n",
    "train_data, val_data, test_data, scaler_targets = scale_and_split_data(dataset, 0.15, 0.1)\n",
    "train_loader, val_loader, test_loader = create_data_loaders(train_data, val_data, test_data, batch_size=64)\n",
    "print(\"...Data loading done...\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test GNN Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================================\n",
      "Network 1 of 12\n",
      "Number of GAT layers: 1\n",
      "Number of hidden channels: 256\n",
      "Number of trainable parameters: 105 359\n",
      "=================================================================================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 103\u001b[0m\n\u001b[0;32m    101\u001b[0m losses \u001b[39m=\u001b[39m []\n\u001b[0;32m    102\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m train_loader:\n\u001b[1;32m--> 103\u001b[0m     loss \u001b[39m=\u001b[39m train(model, batch)\n\u001b[0;32m    104\u001b[0m     losses\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy())  \n\u001b[0;32m    105\u001b[0m \u001b[39m# Compute validation loss\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[18], line 11\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, batch)\u001b[0m\n\u001b[0;32m      8\u001b[0m targets \u001b[39m=\u001b[39m batch\u001b[39m.\u001b[39my\n\u001b[0;32m      9\u001b[0m loss \u001b[39m=\u001b[39m criterion(out, targets) \n\u001b[1;32m---> 11\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()  \u001b[39m# Derive gradients.\u001b[39;00m\n\u001b[0;32m     12\u001b[0m optimizer\u001b[39m.\u001b[39mstep()  \u001b[39m# Update parameters based on gradients.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\jespe\\anaconda3\\envs\\proj_env\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\jespe\\anaconda3\\envs\\proj_env\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import GNN_structures as GNNs\n",
    "\n",
    "def train(model, batch):\n",
    "      model.train()\n",
    "      optimizer.zero_grad()  # Clear gradients.\n",
    "      out = model(batch.x, batch.edge_index, batch.edge_attr, batch.batch).to(device)  # Perform a single forward pass.\n",
    "\n",
    "      targets = batch.y\n",
    "      loss = criterion(out, targets) \n",
    "\n",
    "      loss.backward()  # Derive gradients.\n",
    "      optimizer.step()  # Update parameters based on gradients.\n",
    "      \n",
    "      return loss\n",
    "\n",
    "def test(model, data):\n",
    "      all_r2 = []\n",
    "      all_loss = []\n",
    "      counter = -1    \n",
    "      for batch in data:\n",
    "            counter += 1\n",
    "            model.eval()\n",
    "            out = model(batch.x, batch.edge_index, batch.edge_attr, batch.batch).cpu()\n",
    "            targets = batch.y.cpu()\n",
    "            \n",
    "            # Caculate R2\n",
    "            r2_score_var = []\n",
    "            for item in range(target_dim):\n",
    "                  if item == 0:\n",
    "                        r2_score_var = r2_score(targets[:,item].detach().numpy(), out[:,item].detach().numpy())\n",
    "                  else:\n",
    "                        new_score = r2_score(targets[:,item].detach().numpy(), out[:,item].detach().numpy())\n",
    "                        r2_score_var = np.vstack((r2_score_var, new_score))\n",
    "\n",
    "            if counter == 0:\n",
    "                  all_r2 = r2_score_var\n",
    "            else:\n",
    "                  all_r2 = np.hstack((all_r2, r2_score_var))\n",
    "                  \n",
    "            loss = float(criterion(out, targets).detach().numpy())\n",
    "            all_loss = np.hstack((all_loss, loss))\n",
    "\n",
    "      average_test_r2 = np.mean(all_r2, axis=1)\n",
    "      average_test_loss = np.mean(all_loss)\n",
    "      \n",
    "      return average_test_r2, average_test_loss  \n",
    "  \n",
    "def early_stopping(val_losses, patience): # returns True if there is no improvement in val_loss\n",
    "      if len(val_losses) < patience:\n",
    "            return False\n",
    "      else:\n",
    "            best_loss = np.min(val_losses)\n",
    "            current_loss = val_losses[-1]\n",
    "            \n",
    "            if current_loss > best_loss:\n",
    "                  return True\n",
    "            else:\n",
    "                  return False\n",
    "\n",
    "layer_counts = [1, 3, 5, 7]\n",
    "channel_counts = [256, 512, 1024]\n",
    "n_epochs = 50\n",
    "\n",
    "# Run experiment\n",
    "feature_dim = train_data[0].x.shape[1]\n",
    "target_dim = train_data[0].y.shape[1]\n",
    "\n",
    "loss_train_res = np.zeros((len(layer_counts), len(channel_counts)))\n",
    "loss_val_res = np.zeros((len(layer_counts), len(channel_counts)))\n",
    "avg_r2_train = np.zeros((len(layer_counts), len(channel_counts)))\n",
    "avg_r2_val = np.zeros((len(layer_counts), len(channel_counts)))\n",
    "\n",
    "network_counter = 0\n",
    "for i, num_layers in enumerate(layer_counts):\n",
    "    for j, hidden_channels in enumerate(channel_counts):\n",
    "        network_counter += 1    \n",
    "        train_start = time.time()\n",
    "        \n",
    "        model_class = GNNs.define_GNN_structure(num_layers, hidden_channels, feature_dim, target_dim, device)\n",
    "        model = model_class().to(device)\n",
    "        \n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        criterion = torch.nn.MSELoss().to(device)\n",
    "        \n",
    "        decay_rate = 0.94\n",
    "        lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=decay_rate)\n",
    "        \n",
    "        print('=================================================================================')\n",
    "        print(f\"Network {network_counter} of {len(layer_counts)*len(channel_counts)}\")\n",
    "        print(f'Number of GAT layers: {num_layers}')\n",
    "        print(f'Number of hidden channels: {hidden_channels}')\n",
    "        trainable_params, _ = uf.get_num_parameters(model)\n",
    "        print(f\"Number of trainable parameters: {trainable_params:,}\".replace(',',' '))\n",
    "        print('=================================================================================')\n",
    "\n",
    "        val_losses_epoch = []\n",
    "        epochs_without_improvement = 0\n",
    "        best_val_loss = np.inf\n",
    "        patience = 3\n",
    "        for epoch in np.arange(1, n_epochs+1):\n",
    "            losses = []\n",
    "            for batch in train_loader:\n",
    "                loss = train(model, batch)\n",
    "                losses.append(loss.cpu().detach().numpy())  \n",
    "            # Compute validation loss\n",
    "            model.eval()\n",
    "            val_losses = []\n",
    "            for batch in val_loader:          \n",
    "                targets = batch.y\n",
    "                out = model(batch.x, batch.edge_index, batch.edge_attr, batch.batch)\n",
    "                val_losses.append(criterion(out, targets).cpu().detach().numpy()) \n",
    "            \n",
    "            val_loss_epoch = np.mean(val_losses)\n",
    "            val_losses_epoch.append(val_loss_epoch)\n",
    "            print(f\"Epoch: {epoch:02d} | Train Loss: {np.mean(losses):.5f} | Validation Loss: {val_loss_epoch:.5f}\")\n",
    "            train_end = time.time()\n",
    "        \n",
    "            lr_scheduler.step() # Decay to learning rate\n",
    "            \n",
    "            # check for early stopping\n",
    "            if early_stopping(val_losses_epoch, patience) and epoch > patience:\n",
    "                    epochs_without_improvement += 1\n",
    "                    if epochs_without_improvement >= patience:\n",
    "                        print(f\"Early stopping at epoch {epoch} after {patience} epochs without improvement.\")\n",
    "                        break\n",
    "            else:\n",
    "                    epochs_without_improvement = 0\n",
    "                    best_val_loss = val_loss_epoch\n",
    "                    # save the model\n",
    "                    torch.save(model.state_dict(), 'best_temp_model.pt')\n",
    "                    \n",
    "        print(\"...Training done...\")\n",
    "        print(\"Time elapsed for training:\", (train_end - train_start)/60, \" minutes\")\n",
    "        model.load_state_dict(torch.load('best_temp_model.pt'))\n",
    "        r2_train, loss_train = test(model, train_loader) \n",
    "        loss_train_res[i, j] = loss_train\n",
    "        avg_r2_train[i, j] = np.mean(r2_train)\n",
    "        \n",
    "        r2_val, loss_val = test(model, val_loader)\n",
    "        loss_val_res[i, j] = loss_val\n",
    "        avg_r2_val[i, j] = np.mean(r2_val)\n",
    "\n",
    "        print('=================================================================================')\n",
    "        print(\"Final training R2:\", r2_train)\n",
    "        print(\"Average final training R2: \", np.mean(r2_train))\n",
    "        print(\"Final training loss:\", loss_train)\n",
    "\n",
    "        print(\"Final validation R2:\", r2_val)\n",
    "        print(\"Average final validation R2: \", np.mean(r2_val))\n",
    "        print(\"Final validation loss:\", loss_val)\n",
    "print()\n",
    "print(\"Experiment done!\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Saving results...\n"
     ]
    }
   ],
   "source": [
    "print(\"...Saving results...\")\n",
    "df_loss_train = pd.DataFrame(loss_train_res, index=layer_counts, columns=channel_counts)\n",
    "df_loss_test = pd.DataFrame(loss_val_res, index=layer_counts, columns=channel_counts)\n",
    "df_avg_r2_train = pd.DataFrame(avg_r2_train, index=layer_counts, columns=channel_counts)\n",
    "df_avg_r2_test = pd.DataFrame(avg_r2_val, index=layer_counts, columns=channel_counts)\n",
    "\n",
    "df_loss_train.to_csv('experiment_results/loss_train.csv')\n",
    "df_loss_test.to_csv('experiment_results/loss_test.csv')\n",
    "df_avg_r2_train.to_csv('experiment_results/avg_r2_train.csv')\n",
    "df_avg_r2_test.to_csv('experiment_results/avg_r2_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:\n",
      "        256       384       512\n",
      "5  0.058069  0.049923  0.045519\n",
      "8  0.057213  0.051345  0.047007\n"
     ]
    }
   ],
   "source": [
    "print(\"Training loss:\")\n",
    "print(df_loss_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training R2:\n",
      "        256       384       512\n",
      "5  0.939137  0.947647  0.952251\n",
      "8  0.940072  0.946091  0.950522\n"
     ]
    }
   ],
   "source": [
    "print(\"Average training R2:\")\n",
    "print(df_avg_r2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss:\n",
      "        256       384       512\n",
      "5  0.063638  0.057531  0.053585\n",
      "8  0.063738  0.059040  0.056631\n"
     ]
    }
   ],
   "source": [
    "print(\"Test loss:\")\n",
    "print(df_loss_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average test R2:\n",
      "        256       384       512\n",
      "5  0.932895  0.939302  0.943422\n",
      "8  0.932761  0.937858  0.940112\n"
     ]
    }
   ],
   "source": [
    "print(\"Average test R2:\")\n",
    "print(df_avg_r2_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Neural Network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model for all targets at once"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters: 105487\n",
      "Trainable parameters: 105487\n",
      "\n",
      "...Starting training...\n",
      "Device used: cuda\n",
      "Epoch: 001, Loss: 0.38233\n",
      "Average Train R2: [0.50745208 0.61765652 0.65457011 0.50063527 0.72508102 0.72168224\n",
      " 0.89643196 0.85983842 0.75251658 0.89449857 0.7428561  0.74343352\n",
      " 0.74282347 0.74189098 0.7846295 ]\n",
      "Average Train Loss: 0.2668107972075408\n",
      "Average Test R2: [0.52782081 0.61150084 0.647742   0.4960412  0.71998918 0.71710412\n",
      " 0.89558413 0.85954995 0.74671881 0.8900153  0.73543768 0.73627739\n",
      " 0.73577947 0.73460852 0.7755669 ]\n",
      "Average Test Loss: 0.26992459702663696\n",
      "Epoch: 002, Loss: 0.29553\n",
      "Epoch: 003, Loss: 0.27875\n",
      "Epoch: 004, Loss: 0.26669\n",
      "Epoch: 005, Loss: 0.25606\n",
      "Epoch: 006, Loss: 0.24674\n",
      "Epoch: 007, Loss: 0.23806\n",
      "Epoch: 008, Loss: 0.23058\n",
      "Epoch: 009, Loss: 0.22232\n",
      "Epoch: 010, Loss: 0.21676\n",
      "Average Train R2: [0.69701478 0.76179004 0.76745    0.59795789 0.84665814 0.80386487\n",
      " 0.93829509 0.91494272 0.87232269 0.93906449 0.86451119 0.8645147\n",
      " 0.86451282 0.86451725 0.87810301]\n",
      "Average Train Loss: 0.16305421817241453\n",
      "Average Test R2: [0.69891713 0.75385142 0.75763479 0.58466605 0.8394916  0.80730151\n",
      " 0.93674168 0.91348265 0.86645074 0.93539946 0.85615215 0.85615624\n",
      " 0.85615428 0.85615845 0.86848475]\n",
      "Average Test Loss: 0.16787952277809381\n",
      "Epoch: 011, Loss: 0.21214\n",
      "Epoch: 012, Loss: 0.20750\n",
      "Epoch: 013, Loss: 0.20403\n",
      "Epoch: 014, Loss: 0.20128\n",
      "Epoch: 015, Loss: 0.19780\n",
      "Epoch: 016, Loss: 0.19492\n",
      "Epoch: 017, Loss: 0.19265\n",
      "Epoch: 018, Loss: 0.18977\n",
      "Epoch: 019, Loss: 0.18740\n",
      "Epoch: 020, Loss: 0.18524\n",
      "Average Train R2: [0.73828915 0.80469969 0.81647845 0.6324788  0.88218268 0.83925893\n",
      " 0.95018799 0.92364383 0.90185485 0.95293397 0.90665787 0.90665957\n",
      " 0.90665985 0.90665516 0.90462887]\n",
      "Average Train Loss: 0.1306982018680395\n",
      "Average Test R2: [0.74447939 0.80182567 0.81072967 0.61678183 0.87763738 0.83933509\n",
      " 0.94860487 0.92255297 0.89893309 0.95033895 0.90054581 0.90054775\n",
      " 0.90054797 0.90054315 0.89730401]\n",
      "Average Test Loss: 0.13418613818402475\n",
      "Epoch: 021, Loss: 0.18224\n",
      "Epoch: 022, Loss: 0.18076\n",
      "Epoch: 023, Loss: 0.17913\n",
      "Epoch: 024, Loss: 0.17668\n",
      "Epoch: 025, Loss: 0.17435\n",
      "Epoch: 026, Loss: 0.17265\n",
      "Epoch: 027, Loss: 0.17134\n",
      "Epoch: 028, Loss: 0.16905\n",
      "Epoch: 029, Loss: 0.16758\n",
      "Epoch: 030, Loss: 0.16519\n",
      "...Training done...\n",
      "...Calculating final results...\n",
      "====================================================\n",
      "Final training R2: [0.78279761 0.83841949 0.84953286 0.65406065 0.90704565 0.84133341\n",
      " 0.954357   0.92901516 0.92071918 0.95734684 0.93037575 0.93037665\n",
      " 0.93037674 0.93037492 0.92777498]\n",
      "Average final training R2:  0.8855937937347023\n",
      "Final training loss: 0.11101972438543364\n",
      "Final test R2: [0.78403117 0.83569358 0.84551405 0.63628811 0.90172127 0.84225897\n",
      " 0.95287651 0.92763797 0.91649449 0.95436222 0.92523803 0.92523919\n",
      " 0.92523927 0.92523688 0.9213844 ]\n",
      "Average final test R2:  0.8812810742969709\n",
      "Final test loss: 0.11462852022109124\n"
     ]
    }
   ],
   "source": [
    "from utility_functions import get_num_parameters\n",
    "\n",
    "feature_dim = train_data[0].x.shape[1]\n",
    "target_dim = train_data[0].y.shape[1]\n",
    "\n",
    "model = GNNs.get_GNN_structure(5, 128, feature_dim, target_dim).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005, weight_decay=5e-4)\n",
    "criterion = torch.nn.MSELoss().to(device)\n",
    "\n",
    "decay_rate = 0.94\n",
    "# decay_rate = 0.96\n",
    "lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=decay_rate)\n",
    "\n",
    "train_params, tot_params = get_num_parameters(model)\n",
    "print(f\"Total number of parameters: {tot_params}\")\n",
    "print(f\"Trainable parameters: {train_params}\")\n",
    "\n",
    "def train(data_in):\n",
    "      model.train()\n",
    "      optimizer.zero_grad()  # Clear gradients.\n",
    "      out = model(data_in.x, data_in.edge_index, data_in.edge_attr, data_in.batch).to(device)  # Perform a single forward pass.\n",
    "\n",
    "      targets = data_in.y\n",
    "      loss = criterion(out, targets) \n",
    "\n",
    "      loss.backward()  # Derive gradients.\n",
    "      optimizer.step()  # Update parameters based on gradients.\n",
    "      return loss\n",
    "\n",
    "def test(data):\n",
    "      all_r2 = []\n",
    "      all_loss = []\n",
    "      counter = -1    \n",
    "      for data_in in data:\n",
    "            counter += 1\n",
    "            model.eval()\n",
    "            out = model(data_in.x, data_in.edge_index, data_in.edge_attr, data_in.batch).cpu()\n",
    "            targets = data_in.y.cpu()\n",
    "            \n",
    "            # Caculate R2\n",
    "            r2_score_var = []\n",
    "            for item in range(target_dim):\n",
    "                  if item == 0:\n",
    "                        r2_score_var = r2_score(targets[:,item].detach().numpy(), out[:,item].detach().numpy())\n",
    "                  else:\n",
    "                        new_score = r2_score(targets[:,item].detach().numpy(), out[:,item].detach().numpy())\n",
    "                        r2_score_var = np.vstack((r2_score_var, new_score))\n",
    "\n",
    "            if counter == 0:\n",
    "                  all_r2 = r2_score_var\n",
    "            else:\n",
    "                  all_r2 = np.hstack((all_r2, r2_score_var))\n",
    "                  \n",
    "            loss = float(criterion(out, targets).detach().numpy())\n",
    "            all_loss = np.hstack((all_loss, loss))\n",
    "\n",
    "      average_test_r2 = np.mean(all_r2, axis=1)\n",
    "      average_test_loss = np.mean(all_loss)\n",
    "      \n",
    "      return average_test_r2, average_test_loss\n",
    "\n",
    "# Vectors to append accuracy to:\n",
    "train_r2 = []\n",
    "train_loss = []\n",
    "test_r2 = []\n",
    "test_loss = []\n",
    "\n",
    "n_epochs = 30\n",
    "print_every_N_epochs = True\n",
    "N = 10 # print R2 every N epochs\n",
    "\n",
    "epoch_times = []\n",
    "train_times = []\n",
    "test_times = []\n",
    "print()\n",
    "print(\"...Starting training...\")\n",
    "print(\"Device used:\", device)\n",
    "for epoch in np.arange(1, n_epochs+1):\n",
    "      epoch_start = time.time()\n",
    "      losses = []\n",
    "      train_start = time.time()\n",
    "      for data in train_loader:\n",
    "            loss = train(data)\n",
    "            losses.append(loss.cpu().detach().numpy())\n",
    "      print(f'Epoch: {epoch:03d}, Loss: {np.mean(losses):.5f}')\n",
    "      train_end = time.time()\n",
    "      train_times.append(train_end - train_start)\n",
    "      \n",
    "      lr_scheduler.step()\n",
    "      \n",
    "      if print_every_N_epochs and (epoch % N == 0 or epoch == 1) and epoch != n_epochs:\n",
    "            test_start = time.time()\n",
    "            r2_temp_train, loss_temp_train = test(train_loader) \n",
    "            train_r2.append(r2_temp_train)\n",
    "            train_loss.append(loss_temp_train)         \n",
    "            r2_temp_test, loss_temp_test = test(test_loader)\n",
    "            test_r2.append(r2_temp_test)\n",
    "            test_loss.append(loss_temp_test)\n",
    "            print(f'Average Train R2: {r2_temp_train}')\n",
    "            print(f\"Average Train Loss: {loss_temp_train}\")\n",
    "            print(f'Average Test R2: {r2_temp_test}')\n",
    "            print(f\"Average Test Loss: {loss_temp_test}\")\n",
    "            test_end = time.time()\n",
    "            test_times.append(test_end - test_start)\n",
    "            \n",
    "      if epoch == n_epochs:         # calculate results of training\n",
    "            print(\"...Training done...\")\n",
    "            print(\"...Calculating final results...\")\n",
    "            test_start = time.time()\n",
    "            r2_temp_train, loss_temp_train = test(train_loader) \n",
    "            train_r2.append(r2_temp_train)\n",
    "            train_loss.append(loss_temp_train)\n",
    "            \n",
    "            r2_temp_test, loss_temp_test = test(test_loader)\n",
    "            test_r2.append(r2_temp_test)\n",
    "            test_loss.append(loss_temp_test)\n",
    "\n",
    "            print(\"====================================================\")\n",
    "            print(\"Final training R2:\", train_r2[-1])\n",
    "            print(\"Average final training R2: \", np.mean(train_r2[-1]))\n",
    "            print(\"Final training loss:\", train_loss[-1])\n",
    "\n",
    "            print(\"Final test R2:\", test_r2[-1])\n",
    "            print(\"Average final test R2: \", np.mean(test_r2[-1]))\n",
    "            print(\"Final test loss:\", test_loss[-1])\n",
    "            \n",
    "            test_end = time.time()\n",
    "            test_times.append(test_end - test_start)\n",
    "            \n",
    "      epoch_end = time.time()\n",
    "      epoch_times.append(epoch_end - epoch_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used: cuda\n",
      "\n",
      "Total number of epochs: 50\n",
      "Total training time: 24.17 minutes\n",
      "Total time in training: 21.75 minutes\n",
      "Total time in testing: 2.42 minutes\n",
      "\n",
      "Average epoch time: 29.0 seconds\n",
      "Average time in training: 26.1 seconds\n",
      "Average time in testing: 24.2 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"Device used:\", device)\n",
    "print()\n",
    "print(f\"Total number of epochs: {len(epoch_times)}\")\n",
    "print(f\"Total training time: {np.sum(epoch_times)/60:.2f} minutes\")\n",
    "print(f\"Total time in training: {np.sum(train_times)/60:.2f} minutes\")\n",
    "print(f\"Total time in testing: {np.sum(test_times)/60:.2f} minutes\")\n",
    "print()\n",
    "print(f\"Average epoch time: {np.mean(epoch_times):.1f} seconds\")\n",
    "print(f\"Average time in training: {np.mean(train_times):.1f} seconds\")\n",
    "print(f\"Average time in testing: {np.mean(test_times):.1f} seconds\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model for just one target"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GNN function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, feature_dim, target_dim):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GATConv(feature_dim, hidden_channels)\n",
    "        self.conv1_norm = GraphNorm(hidden_channels)\n",
    "        self.conv2 = GATConv(hidden_channels, hidden_channels)\n",
    "        self.conv2_norm = GraphNorm(hidden_channels)\n",
    "        self.conv3 = GATConv(hidden_channels, hidden_channels)\n",
    "        self.conv3_norm = GraphNorm(hidden_channels)\n",
    "        self.conv4 = GATConv(hidden_channels, hidden_channels)\n",
    "        self.conv4_norm = GraphNorm(hidden_channels)\n",
    "        self.conv5 = GATConv(hidden_channels, hidden_channels)\n",
    "        self.conv5_norm = GraphNorm(hidden_channels)\n",
    "        \n",
    "        self.lin1 = Linear(hidden_channels, hidden_channels)\n",
    "        self.lin1_norm = BatchNorm(hidden_channels)\n",
    "        self.lin2 = Linear(hidden_channels, 32)\n",
    "        self.lin2_norm = BatchNorm(32)\n",
    "        self.lin3 = Linear(32, target_dim)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch): \n",
    "        x = self.conv1(x, edge_index, edge_attr)\n",
    "        x = self.conv1_norm(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index, edge_attr)\n",
    "        x = self.conv2_norm(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3(x, edge_index, edge_attr)\n",
    "        x = self.conv3_norm(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv4(x, edge_index, edge_attr)\n",
    "        x = self.conv4_norm(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv5(x, edge_index, edge_attr)\n",
    "        x = self.conv5_norm(x)\n",
    "\n",
    "        #Returns batch-wise graph-level-outputs by averaging node features across the node dimension, so that for a single graph G\n",
    "        #its output is computed by\n",
    "        x = global_mean_pool(x, batch) \n",
    "        \n",
    "        x = self.lin1(x)\n",
    "        x = self.lin1_norm(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        \n",
    "        x = self.lin2(x)\n",
    "        x = self.lin2_norm(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        \n",
    "        x = self.lin3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target index: 0\n",
      "Initial training R2:  -0.006660032108331978\n",
      "Initial test R2:  -0.00475941961664909\n",
      "Epoch: 001, Loss: 0.68831\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 69\u001b[0m\n\u001b[0;32m     67\u001b[0m counter \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     68\u001b[0m losses \u001b[39m=\u001b[39m []\n\u001b[1;32m---> 69\u001b[0m \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m train_loader:\n\u001b[0;32m     70\u001b[0m       loss \u001b[39m=\u001b[39m train(data, target_index)\n\u001b[0;32m     71\u001b[0m       losses\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy())\n",
      "File \u001b[1;32mc:\\Users\\jespe\\anaconda3\\envs\\proj_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\jespe\\anaconda3\\envs\\proj_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    677\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 678\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    679\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    680\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\jespe\\anaconda3\\envs\\proj_env\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[1;32mc:\\Users\\jespe\\anaconda3\\envs\\proj_env\\lib\\site-packages\\torch_geometric\\loader\\dataloader.py:20\u001b[0m, in \u001b[0;36mCollater.__call__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m     18\u001b[0m elem \u001b[39m=\u001b[39m batch[\u001b[39m0\u001b[39m]\n\u001b[0;32m     19\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, BaseData):\n\u001b[1;32m---> 20\u001b[0m     \u001b[39mreturn\u001b[39;00m Batch\u001b[39m.\u001b[39;49mfrom_data_list(batch, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfollow_batch,\n\u001b[0;32m     21\u001b[0m                                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexclude_keys)\n\u001b[0;32m     22\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m     23\u001b[0m     \u001b[39mreturn\u001b[39;00m default_collate(batch)\n",
      "File \u001b[1;32mc:\\Users\\jespe\\anaconda3\\envs\\proj_env\\lib\\site-packages\\torch_geometric\\data\\batch.py:76\u001b[0m, in \u001b[0;36mBatch.from_data_list\u001b[1;34m(cls, data_list, follow_batch, exclude_keys)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m     65\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_data_list\u001b[39m(\u001b[39mcls\u001b[39m, data_list: List[BaseData],\n\u001b[0;32m     66\u001b[0m                    follow_batch: Optional[List[\u001b[39mstr\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     67\u001b[0m                    exclude_keys: Optional[List[\u001b[39mstr\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m     68\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Constructs a :class:`~torch_geometric.data.Batch` object from a\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[39m    Python list of :class:`~torch_geometric.data.Data` or\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[39m    :class:`~torch_geometric.data.HeteroData` objects.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[39m    :obj:`follow_batch`.\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[39m    Will exclude any keys given in :obj:`exclude_keys`.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 76\u001b[0m     batch, slice_dict, inc_dict \u001b[39m=\u001b[39m collate(\n\u001b[0;32m     77\u001b[0m         \u001b[39mcls\u001b[39;49m,\n\u001b[0;32m     78\u001b[0m         data_list\u001b[39m=\u001b[39;49mdata_list,\n\u001b[0;32m     79\u001b[0m         increment\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m     80\u001b[0m         add_batch\u001b[39m=\u001b[39;49m\u001b[39mnot\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(data_list[\u001b[39m0\u001b[39;49m], Batch),\n\u001b[0;32m     81\u001b[0m         follow_batch\u001b[39m=\u001b[39;49mfollow_batch,\n\u001b[0;32m     82\u001b[0m         exclude_keys\u001b[39m=\u001b[39;49mexclude_keys,\n\u001b[0;32m     83\u001b[0m     )\n\u001b[0;32m     85\u001b[0m     batch\u001b[39m.\u001b[39m_num_graphs \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(data_list)\n\u001b[0;32m     86\u001b[0m     batch\u001b[39m.\u001b[39m_slice_dict \u001b[39m=\u001b[39m slice_dict\n",
      "File \u001b[1;32mc:\\Users\\jespe\\anaconda3\\envs\\proj_env\\lib\\site-packages\\torch_geometric\\data\\collate.py:108\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(cls, data_list, increment, add_batch, follow_batch, exclude_keys)\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[39mif\u001b[39;00m (add_batch \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(stores[\u001b[39m0\u001b[39m], NodeStorage)\n\u001b[0;32m    106\u001b[0m             \u001b[39mand\u001b[39;00m stores[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mcan_infer_num_nodes):\n\u001b[0;32m    107\u001b[0m         repeats \u001b[39m=\u001b[39m [store\u001b[39m.\u001b[39mnum_nodes \u001b[39mfor\u001b[39;00m store \u001b[39min\u001b[39;00m stores]\n\u001b[1;32m--> 108\u001b[0m         out_store\u001b[39m.\u001b[39mbatch \u001b[39m=\u001b[39m repeat_interleave(repeats, device\u001b[39m=\u001b[39;49mdevice)\n\u001b[0;32m    109\u001b[0m         out_store\u001b[39m.\u001b[39mptr \u001b[39m=\u001b[39m cumsum(torch\u001b[39m.\u001b[39mtensor(repeats, device\u001b[39m=\u001b[39mdevice))\n\u001b[0;32m    111\u001b[0m \u001b[39mreturn\u001b[39;00m out, slice_dict, inc_dict\n",
      "File \u001b[1;32mc:\\Users\\jespe\\anaconda3\\envs\\proj_env\\lib\\site-packages\\torch_geometric\\data\\collate.py:247\u001b[0m, in \u001b[0;36mrepeat_interleave\u001b[1;34m(repeats, device)\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrepeat_interleave\u001b[39m(\n\u001b[0;32m    244\u001b[0m     repeats: List[\u001b[39mint\u001b[39m],\n\u001b[0;32m    245\u001b[0m     device: Optional[torch\u001b[39m.\u001b[39mdevice] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    246\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 247\u001b[0m     outs \u001b[39m=\u001b[39m [torch\u001b[39m.\u001b[39mfull((n, ), i, device\u001b[39m=\u001b[39mdevice) \u001b[39mfor\u001b[39;00m i, n \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(repeats)]\n\u001b[0;32m    248\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcat(outs, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\jespe\\anaconda3\\envs\\proj_env\\lib\\site-packages\\torch_geometric\\data\\collate.py:247\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrepeat_interleave\u001b[39m(\n\u001b[0;32m    244\u001b[0m     repeats: List[\u001b[39mint\u001b[39m],\n\u001b[0;32m    245\u001b[0m     device: Optional[torch\u001b[39m.\u001b[39mdevice] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    246\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 247\u001b[0m     outs \u001b[39m=\u001b[39m [torch\u001b[39m.\u001b[39;49mfull((n, ), i, device\u001b[39m=\u001b[39;49mdevice) \u001b[39mfor\u001b[39;00m i, n \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(repeats)]\n\u001b[0;32m    248\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcat(outs, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "feature_dim = train_data[0].x.shape[1]\n",
    "target_dim = 1\n",
    "\n",
    "def train(data_in, target):\n",
    "      model.train()\n",
    "      optimizer.zero_grad()  # Clear gradients.\n",
    "      out = model(data_in.x, data_in.edge_index, data_in.edge_attr, data_in.batch).to(device)\n",
    "      targets = data_in.y[:,target].reshape(-1,1)\n",
    "      \n",
    "      loss = criterion(out, targets)   \n",
    "\n",
    "      loss.backward()  # Derive gradients.\n",
    "      optimizer.step()  # Update parameters based on gradients.\n",
    "      return loss\n",
    "\n",
    "def test(data, target):\n",
    "      all_r2 = []\n",
    "      all_loss = []\n",
    "      counter = -1    \n",
    "      for data_in in data:\n",
    "            counter += 1\n",
    "            model.eval()\n",
    "            out = model(data_in.x, data_in.edge_index, data_in.edge_attr, data_in.batch).cpu()\n",
    "            targets = data_in.y[:,target].cpu().reshape(-1,1)\n",
    "            \n",
    "            # Caculate R2\n",
    "            r2_score_var = r2_score(targets.detach().numpy(), out.detach().numpy())\n",
    "            all_r2.append(r2_score_var)\n",
    "            \n",
    "            loss = float(criterion(out, targets).detach().numpy())\n",
    "            all_loss.append(loss)\n",
    "\n",
    "      average_test_r2 = np.mean(all_r2)\n",
    "      average_test_loss = np.mean(all_loss)\n",
    "\n",
    "      return average_test_r2, average_test_loss\n",
    "\n",
    "num_targets = train_data[0].y.shape[1]\n",
    "start_time = time.time()\n",
    "for target_index in range(num_targets):\n",
    "      print(\"Target index:\", target_index)\n",
    "\n",
    "      model = GNN(hidden_channels=64, feature_dim=feature_dim, target_dim=target_dim).to(device) \n",
    "      optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=5e-4)\n",
    "      criterion = torch.nn.MSELoss().to(device)\n",
    "\n",
    "      # Vectors to append accuracy to:\n",
    "      train_r2 = []\n",
    "      test_r2 = []\n",
    "      train_loss = []\n",
    "      test_loss = []\n",
    "\n",
    "      # Calculate accuracy and loss before training \n",
    "      r2_temp, loss_temp = test(train_loader, target_index)\n",
    "      train_r2.append(r2_temp)\n",
    "      train_loss.append(loss_temp)\n",
    "      r2_temp, loss_temp = test(test_loader, target_index)\n",
    "      test_r2.append(r2_temp)\n",
    "      test_loss.append(loss_temp)\n",
    "      \n",
    "      print(\"Initial training R2: \", train_r2[0])\n",
    "      print(\"Initial test R2: \", test_r2[0])\n",
    "\n",
    "      print_r2_option = True\n",
    "      counter = 0\n",
    "      for epoch in range(1, 21):\n",
    "            counter += 1\n",
    "            losses = []\n",
    "            for data in train_loader:\n",
    "                  loss = train(data, target_index)\n",
    "                  losses.append(loss.cpu().detach().numpy())\n",
    "            print(f'Epoch: {epoch:03d}, Loss: {np.mean(losses):.5f}')\n",
    "\n",
    "            if print_r2_option & epoch == 20:\n",
    "                  temp_train_r2, temp_train_loss = test(train_loader, target_index)\n",
    "                  train_r2.append(temp_train_r2)\n",
    "                  train_loss.append(temp_train_loss)\n",
    "\n",
    "                  temp_test_r2, temp_test_loss = test(test_loader, target_index)\n",
    "                  test_r2.append(temp_test_r2)\n",
    "                  test_loss.append(temp_test_loss)\n",
    "\n",
    "      print(f\"Best training R2 for target {target_index}: {np.max(train_r2)}\")\n",
    "      print(f\"Best test R2 for target {target_index}: {np.max(test_r2)}\")\n",
    "print(\"...Done...\")\n",
    "end_time = time.time()\n",
    "print(f\"Time taken: {(end_time - start_time)/60} minutes\")\n",
    "print(f\"Average time per target: {(end_time - start_time)/(num_targets*60)} minutes\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
