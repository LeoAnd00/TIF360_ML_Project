{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TIF360 Project"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main source: https://www.kaggle.com/code/rmonge/predicting-molecule-properties-based-on-its-smiles/notebook"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time as time\n",
    "import torch\n",
    "import torch_geometric\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch.nn import Linear, LeakyReLU\n",
    "from torch_geometric.nn import global_mean_pool, GATConv, BatchNorm, GraphNorm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "import utility_functions as uf  # defined in utility_functions.py\n",
    "import GNN_structures as GNNs # defined in GNN_structures.py\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"once\") # only displays the warning once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available: True\n",
      "device: cuda\n",
      "cuda version: 11.8\n",
      "gpu: NVIDIA GeForce RTX 3080\n"
     ]
    }
   ],
   "source": [
    "# check if cuda is available\n",
    "print('cuda available:', torch.cuda.is_available())\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('device:', \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    print('cuda version:', torch.version.cuda)\n",
    "    print('gpu:', torch.cuda.get_device_name(0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(132820, 21)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/smiles_and_targets.csv\")\n",
    "print(np.shape(df))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert data to graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph_dataset_functions import create_graph_dataset_from_smiles\n",
    "\n",
    "properties_names = ['A', 'B', 'C', 'mu', 'alfa', 'homo', 'lumo', 'gap', 'RÂ²', 'zpve', 'U0', 'U', 'H', 'G', 'Cv']\n",
    "\n",
    "x_smiles = df.smiles.values\n",
    "y = df.loc[:, properties_names].values  # shape = (n_samples, n_properties)\n",
    "\n",
    "dataset = create_graph_dataset_from_smiles(x_smiles, y[0:len(x_smiles), :])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information of the graph dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of graphs (molecules): 132820\n",
      "=================================================================================\n",
      "Properties of graph 50, molecule smiles: CC1=CNC=C1\n",
      "Number of nodes: 6\n",
      "Number of edges: 12\n",
      "Number of node features: 78\n",
      "Number of edge features: 10\n",
      "Number of target properties: 15\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of graphs (molecules): {len(dataset)}')\n",
    "graph = dataset[50]\n",
    "print('=================================================================================')\n",
    "print(f'Properties of graph {50}, molecule smiles: {df.smiles.values[50]}')\n",
    "print(f'Number of nodes: {dataset[50].x.shape[0]}')\n",
    "print(f'Number of edges: {dataset[50].edge_index.shape[1]}')\n",
    "print(f'Number of node features: {dataset[50].x.shape[1]}')\n",
    "print(f'Number of edge features: {dataset[50].edge_attr.shape[1]}')\n",
    "print(f'Number of target properties: {dataset[50].y.shape[1]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create functions to load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_and_split_data(dataset, val_share, test_share):\n",
    "    # split the dataset into test and validation:\n",
    "    num_samples = len(dataset)\n",
    "\n",
    "    num_samples = len(dataset)\n",
    "\n",
    "    train_indices, val_indices, test_indices = uf.get_data_split_indices(num_samples, val_share=val_share, test_share=test_share)\n",
    "\n",
    "    train_data = [dataset[i] for i in train_indices]\n",
    "    val_data = [dataset[i] for i in val_indices]\n",
    "    test_data = [dataset[i] for i in test_indices]\n",
    "\n",
    "    # scale the targets\n",
    "    train_targets = np.concatenate([data.y for data in train_data], axis=0)\n",
    "    val_targets = np.concatenate([data.y for data in val_data], axis=0)\n",
    "    test_targets = np.concatenate([data.y for data in test_data], axis=0)    \n",
    "    \n",
    "    train_targets, val_targets, test_targets, scaler_targets = uf.scale_targets(train_targets, val_targets, test_targets)\n",
    "\n",
    "    train_targets = torch.tensor(train_targets, dtype=torch.float, device=device)\n",
    "    val_targets = torch.tensor(val_targets, dtype=torch.float, device=device)\n",
    "    test_targets = torch.tensor(test_targets, dtype=torch.float, device=device)\n",
    "\n",
    "\n",
    "    train_data = [Data(x=data.x.to(device), edge_index=data.edge_index.to(device), edge_attr=data.edge_attr.to(device), \n",
    "                       y=train_targets[index].reshape(1,-1)) for index, data in enumerate(train_data)]\n",
    "    \n",
    "    val_data = [Data(x=data.x.to(device), edge_index=data.edge_index.to(device), edge_attr=data.edge_attr.to(device),\n",
    "                        y=val_targets[index].reshape(1,-1)) for index, data in enumerate(val_data)]\n",
    "    \n",
    "    test_data = [Data(x=data.x.to(device), edge_index=data.edge_index.to(device), edge_attr=data.edge_attr.to(device), \n",
    "                      y=test_targets[index].reshape(1,-1)) for index, data in enumerate(test_data)]\n",
    "\n",
    "    return train_data, val_data, test_data, scaler_targets\n",
    "\n",
    "def create_data_loaders(train_data, val_data, test_data, batch_size): \n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Loading data...\n",
      "...Data loading done...\n"
     ]
    }
   ],
   "source": [
    "print(\"...Loading data...\")\n",
    "train_data, val_data, test_data, scaler_targets = scale_and_split_data(dataset, 0.15, 0.1)\n",
    "train_loader, val_loader, test_loader = create_data_loaders(train_data, val_data, test_data, batch_size=64)\n",
    "print(\"...Data loading done...\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test GNN Structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=================================================================================\n",
      "Network 1 of 12\n",
      "Number of GAT layers: 1\n",
      "Number of hidden channels: 256\n",
      "Number of trainable parameters: 122,895\n",
      "=================================================================================\n",
      "Epoch: 01 | Train Loss: 0.33341 | Validation Loss: 0.28104\n",
      "Epoch: 02 | Train Loss: 0.27640 | Validation Loss: 0.25188\n",
      "Epoch: 03 | Train Loss: 0.25518 | Validation Loss: 0.23293\n",
      "Epoch: 04 | Train Loss: 0.24089 | Validation Loss: 0.22403\n",
      "Epoch: 05 | Train Loss: 0.23007 | Validation Loss: 0.21362\n",
      "Epoch: 06 | Train Loss: 0.22260 | Validation Loss: 0.20694\n",
      "Epoch: 07 | Train Loss: 0.21422 | Validation Loss: 0.20300\n",
      "Epoch: 08 | Train Loss: 0.20776 | Validation Loss: 0.18928\n",
      "Epoch: 09 | Train Loss: 0.20094 | Validation Loss: 0.19206\n",
      "Epoch: 10 | Train Loss: 0.19568 | Validation Loss: 0.17617\n",
      "Epoch: 11 | Train Loss: 0.19083 | Validation Loss: 0.16943\n",
      "Epoch: 12 | Train Loss: 0.18590 | Validation Loss: 0.16666\n",
      "Epoch: 13 | Train Loss: 0.18068 | Validation Loss: 0.15737\n",
      "Epoch: 14 | Train Loss: 0.17761 | Validation Loss: 0.15324\n",
      "Epoch: 15 | Train Loss: 0.17400 | Validation Loss: 0.14877\n",
      "Epoch: 16 | Train Loss: 0.16977 | Validation Loss: 0.14924\n",
      "Epoch: 17 | Train Loss: 0.16687 | Validation Loss: 0.14475\n",
      "Epoch: 18 | Train Loss: 0.16468 | Validation Loss: 0.13986\n",
      "Epoch: 19 | Train Loss: 0.16261 | Validation Loss: 0.14286\n",
      "Epoch: 20 | Train Loss: 0.15932 | Validation Loss: 0.13837\n",
      "Epoch: 21 | Train Loss: 0.15749 | Validation Loss: 0.13557\n",
      "Epoch: 22 | Train Loss: 0.15641 | Validation Loss: 0.13437\n",
      "Epoch: 23 | Train Loss: 0.15361 | Validation Loss: 0.13278\n",
      "Epoch: 24 | Train Loss: 0.15251 | Validation Loss: 0.13325\n",
      "Epoch: 25 | Train Loss: 0.15070 | Validation Loss: 0.13003\n",
      "Epoch: 26 | Train Loss: 0.14910 | Validation Loss: 0.12998\n",
      "Epoch: 27 | Train Loss: 0.14781 | Validation Loss: 0.12931\n",
      "Epoch: 28 | Train Loss: 0.14622 | Validation Loss: 0.12844\n",
      "Epoch: 29 | Train Loss: 0.14528 | Validation Loss: 0.12503\n",
      "Epoch: 30 | Train Loss: 0.14460 | Validation Loss: 0.12582\n",
      "Epoch: 31 | Train Loss: 0.14299 | Validation Loss: 0.12360\n",
      "Epoch: 32 | Train Loss: 0.14195 | Validation Loss: 0.12170\n",
      "Epoch: 33 | Train Loss: 0.14112 | Validation Loss: 0.12309\n",
      "Epoch: 34 | Train Loss: 0.13985 | Validation Loss: 0.12002\n",
      "Epoch: 35 | Train Loss: 0.13950 | Validation Loss: 0.12029\n",
      "Epoch: 36 | Train Loss: 0.13907 | Validation Loss: 0.11904\n",
      "Epoch: 37 | Train Loss: 0.13825 | Validation Loss: 0.12024\n",
      "Epoch: 38 | Train Loss: 0.13770 | Validation Loss: 0.11888\n",
      "Epoch: 39 | Train Loss: 0.13712 | Validation Loss: 0.11894\n",
      "Epoch: 40 | Train Loss: 0.13627 | Validation Loss: 0.11821\n",
      "Epoch: 41 | Train Loss: 0.13579 | Validation Loss: 0.11751\n",
      "Epoch: 42 | Train Loss: 0.13519 | Validation Loss: 0.11800\n",
      "Epoch: 43 | Train Loss: 0.13551 | Validation Loss: 0.11639\n",
      "Epoch: 44 | Train Loss: 0.13447 | Validation Loss: 0.11996\n",
      "Epoch: 45 | Train Loss: 0.13534 | Validation Loss: 0.11669\n",
      "Epoch: 46 | Train Loss: 0.13426 | Validation Loss: 0.11592\n",
      "Epoch: 47 | Train Loss: 0.13364 | Validation Loss: 0.11731\n",
      "Epoch: 48 | Train Loss: 0.13336 | Validation Loss: 0.11470\n",
      "Epoch: 49 | Train Loss: 0.13276 | Validation Loss: 0.11664\n",
      "Epoch: 50 | Train Loss: 0.13257 | Validation Loss: 0.11523\n",
      "Epoch: 51 | Train Loss: 0.13274 | Validation Loss: 0.11384\n",
      "Epoch: 52 | Train Loss: 0.13209 | Validation Loss: 0.11681\n",
      "Epoch: 53 | Train Loss: 0.13206 | Validation Loss: 0.11479\n",
      "Epoch: 54 | Train Loss: 0.13120 | Validation Loss: 0.11424\n",
      "Early stopping at epoch 54 after 3 epochs without improvement.\n",
      "...Training done...\n",
      "Time elapsed for training: 10.569746621449788  minutes\n",
      "=================================================================================\n",
      "Final training R2: [0.77280689 0.81968079 0.84319537 0.67538046 0.90568846 0.85693719\n",
      " 0.95283447 0.93215838 0.90070192 0.96147536 0.9380951  0.93809523\n",
      " 0.93808569 0.93809196 0.92835801]\n",
      "Average final training R2:  0.8867723514844863\n",
      "Final training loss: 0.108999666912803\n",
      "Final validation R2: [0.76244086 0.81793964 0.83706938 0.65954434 0.90099914 0.85299482\n",
      " 0.95244339 0.92933436 0.89470072 0.96273556 0.93304085 0.93303687\n",
      " 0.9330403  0.933039   0.9268544 ]\n",
      "Average final validation R2:  0.8819475752924838\n",
      "Final validation loss: 0.11383718518444742\n",
      "=================================================================================\n",
      "Network 2 of 12\n",
      "Number of GAT layers: 1\n",
      "Number of hidden channels: 512\n",
      "Number of trainable parameters: 374,543\n",
      "=================================================================================\n",
      "Epoch: 01 | Train Loss: 0.32425 | Validation Loss: 0.27603\n",
      "Epoch: 02 | Train Loss: 0.26773 | Validation Loss: 0.25364\n",
      "Epoch: 03 | Train Loss: 0.24271 | Validation Loss: 0.22619\n",
      "Epoch: 04 | Train Loss: 0.22764 | Validation Loss: 0.20824\n",
      "Epoch: 05 | Train Loss: 0.21438 | Validation Loss: 0.19831\n",
      "Epoch: 06 | Train Loss: 0.20314 | Validation Loss: 0.20235\n",
      "Epoch: 07 | Train Loss: 0.19314 | Validation Loss: 0.17316\n",
      "Epoch: 08 | Train Loss: 0.18273 | Validation Loss: 0.16706\n",
      "Epoch: 09 | Train Loss: 0.17565 | Validation Loss: 0.16209\n",
      "Epoch: 10 | Train Loss: 0.16999 | Validation Loss: 0.15838\n",
      "Epoch: 11 | Train Loss: 0.16529 | Validation Loss: 0.15019\n",
      "Epoch: 12 | Train Loss: 0.15829 | Validation Loss: 0.14434\n",
      "Epoch: 13 | Train Loss: 0.15436 | Validation Loss: 0.13932\n",
      "Epoch: 14 | Train Loss: 0.15124 | Validation Loss: 0.13903\n",
      "Epoch: 15 | Train Loss: 0.14792 | Validation Loss: 0.13540\n",
      "Epoch: 16 | Train Loss: 0.14348 | Validation Loss: 0.13184\n",
      "Epoch: 17 | Train Loss: 0.14194 | Validation Loss: 0.12892\n",
      "Epoch: 18 | Train Loss: 0.13901 | Validation Loss: 0.13002\n",
      "Epoch: 19 | Train Loss: 0.13664 | Validation Loss: 0.12624\n",
      "Epoch: 20 | Train Loss: 0.13453 | Validation Loss: 0.11968\n",
      "Epoch: 21 | Train Loss: 0.13281 | Validation Loss: 0.12117\n",
      "Epoch: 22 | Train Loss: 0.13044 | Validation Loss: 0.12047\n",
      "Epoch: 23 | Train Loss: 0.12799 | Validation Loss: 0.12701\n",
      "Early stopping at epoch 23 after 3 epochs without improvement.\n",
      "...Training done...\n",
      "Time elapsed for training: 4.503863008817037  minutes\n",
      "=================================================================================\n",
      "Final training R2: [0.76221643 0.81034719 0.83313154 0.67505952 0.90108126 0.85824402\n",
      " 0.955528   0.93203838 0.89153366 0.95855596 0.92857036 0.92872821\n",
      " 0.92861844 0.9286901  0.92115063]\n",
      "Average final training R2:  0.8808995811982101\n",
      "Final training loss: 0.1146873188687772\n",
      "Final validation R2: [0.75302244 0.80852077 0.82836285 0.65881845 0.89810653 0.85374637\n",
      " 0.9543105  0.92898452 0.88406936 0.95989802 0.92388649 0.92401494\n",
      " 0.92392478 0.92398799 0.91938597]\n",
      "Average final validation R2:  0.8762026650073373\n",
      "Final validation loss: 0.11968335163039275\n",
      "=================================================================================\n",
      "Network 3 of 12\n",
      "Number of GAT layers: 1\n",
      "Number of hidden channels: 1024\n",
      "Number of trainable parameters: 1,271,055\n",
      "=================================================================================\n",
      "Epoch: 01 | Train Loss: 0.31848 | Validation Loss: 0.29194\n",
      "Epoch: 02 | Train Loss: 0.26271 | Validation Loss: 0.24832\n",
      "Epoch: 03 | Train Loss: 0.24000 | Validation Loss: 0.21620\n",
      "Epoch: 04 | Train Loss: 0.22422 | Validation Loss: 0.22177\n",
      "Epoch: 05 | Train Loss: 0.21111 | Validation Loss: 0.20057\n",
      "Epoch: 06 | Train Loss: 0.19797 | Validation Loss: 0.20737\n",
      "Epoch: 07 | Train Loss: 0.18702 | Validation Loss: 0.17640\n",
      "Epoch: 08 | Train Loss: 0.17582 | Validation Loss: 0.16551\n",
      "Epoch: 09 | Train Loss: 0.16749 | Validation Loss: 0.15725\n",
      "Epoch: 10 | Train Loss: 0.15962 | Validation Loss: 0.15712\n",
      "Epoch: 11 | Train Loss: 0.15354 | Validation Loss: 0.14803\n",
      "Epoch: 12 | Train Loss: 0.14771 | Validation Loss: 0.14256\n",
      "Epoch: 13 | Train Loss: 0.14310 | Validation Loss: 0.13710\n",
      "Epoch: 14 | Train Loss: 0.13911 | Validation Loss: 0.13227\n",
      "Epoch: 15 | Train Loss: 0.13484 | Validation Loss: 0.13644\n",
      "Epoch: 16 | Train Loss: 0.13227 | Validation Loss: 0.12487\n",
      "Epoch: 17 | Train Loss: 0.12899 | Validation Loss: 0.12376\n",
      "Epoch: 18 | Train Loss: 0.12579 | Validation Loss: 0.12394\n",
      "Epoch: 19 | Train Loss: 0.12384 | Validation Loss: 0.11911\n",
      "Epoch: 20 | Train Loss: 0.12186 | Validation Loss: 0.11887\n",
      "Epoch: 21 | Train Loss: 0.11924 | Validation Loss: 0.11377\n",
      "Epoch: 22 | Train Loss: 0.11701 | Validation Loss: 0.11284\n",
      "Epoch: 23 | Train Loss: 0.11591 | Validation Loss: 0.11375\n",
      "Epoch: 24 | Train Loss: 0.11309 | Validation Loss: 0.11141\n",
      "Epoch: 25 | Train Loss: 0.11235 | Validation Loss: 0.10768\n",
      "Epoch: 26 | Train Loss: 0.11085 | Validation Loss: 0.10631\n",
      "Epoch: 27 | Train Loss: 0.10937 | Validation Loss: 0.11109\n",
      "Epoch: 28 | Train Loss: 0.10832 | Validation Loss: 0.10569\n",
      "Epoch: 29 | Train Loss: 0.10704 | Validation Loss: 0.10361\n",
      "Epoch: 30 | Train Loss: 0.10603 | Validation Loss: 0.10444\n",
      "Epoch: 31 | Train Loss: 0.10471 | Validation Loss: 0.10253\n",
      "Epoch: 32 | Train Loss: 0.10393 | Validation Loss: 0.10105\n",
      "Epoch: 33 | Train Loss: 0.10299 | Validation Loss: 0.10176\n",
      "Epoch: 34 | Train Loss: 0.10173 | Validation Loss: 0.09922\n",
      "Epoch: 35 | Train Loss: 0.10171 | Validation Loss: 0.10125\n",
      "Epoch: 36 | Train Loss: 0.10079 | Validation Loss: 0.09751\n",
      "Epoch: 37 | Train Loss: 0.09995 | Validation Loss: 0.09745\n",
      "Epoch: 38 | Train Loss: 0.09952 | Validation Loss: 0.10131\n",
      "Epoch: 39 | Train Loss: 0.09865 | Validation Loss: 0.09775\n",
      "Epoch: 40 | Train Loss: 0.09826 | Validation Loss: 0.09713\n",
      "Epoch: 41 | Train Loss: 0.09776 | Validation Loss: 0.09760\n",
      "Epoch: 42 | Train Loss: 0.09738 | Validation Loss: 0.09870\n",
      "Epoch: 43 | Train Loss: 0.09719 | Validation Loss: 0.09760\n",
      "Early stopping at epoch 43 after 3 epochs without improvement.\n",
      "...Training done...\n",
      "Time elapsed for training: 8.00833392937978  minutes\n",
      "=================================================================================\n",
      "Final training R2: [0.81496704 0.84884859 0.86436912 0.71162719 0.92538709 0.87560868\n",
      " 0.95778031 0.93705736 0.91699333 0.96709041 0.94871952 0.94869937\n",
      " 0.94872302 0.9487385  0.94051959]\n",
      "Average final training R2:  0.9036752739039984\n",
      "Final training loss: 0.09205311015678969\n",
      "Final validation R2: [0.80212843 0.84426053 0.85801296 0.68799629 0.92319825 0.86805149\n",
      " 0.9569776  0.93344232 0.91225656 0.96908026 0.94659491 0.94658248\n",
      " 0.94661731 0.94661882 0.94026443]\n",
      "Average final validation R2:  0.8988055093035522\n",
      "Final validation loss: 0.09713445341166778\n",
      "=================================================================================\n",
      "Network 4 of 12\n",
      "Number of GAT layers: 3\n",
      "Number of hidden channels: 256\n",
      "Number of trainable parameters: 257,039\n",
      "=================================================================================\n",
      "Epoch: 01 | Train Loss: 0.30773 | Validation Loss: 0.24309\n",
      "Epoch: 02 | Train Loss: 0.24256 | Validation Loss: 0.21670\n",
      "Epoch: 03 | Train Loss: 0.21903 | Validation Loss: 0.19522\n",
      "Epoch: 04 | Train Loss: 0.20072 | Validation Loss: 0.17527\n",
      "Epoch: 05 | Train Loss: 0.18688 | Validation Loss: 0.16608\n",
      "Epoch: 06 | Train Loss: 0.17450 | Validation Loss: 0.15467\n",
      "Epoch: 07 | Train Loss: 0.16455 | Validation Loss: 0.14732\n",
      "Epoch: 08 | Train Loss: 0.15755 | Validation Loss: 0.14177\n",
      "Epoch: 09 | Train Loss: 0.15001 | Validation Loss: 0.12915\n",
      "Epoch: 10 | Train Loss: 0.14364 | Validation Loss: 0.12165\n",
      "Epoch: 11 | Train Loss: 0.13868 | Validation Loss: 0.12525\n",
      "Epoch: 12 | Train Loss: 0.13289 | Validation Loss: 0.11496\n",
      "Epoch: 13 | Train Loss: 0.12802 | Validation Loss: 0.10948\n",
      "Epoch: 14 | Train Loss: 0.12471 | Validation Loss: 0.10552\n",
      "Epoch: 15 | Train Loss: 0.12112 | Validation Loss: 0.10389\n",
      "Epoch: 16 | Train Loss: 0.11825 | Validation Loss: 0.10048\n",
      "Epoch: 17 | Train Loss: 0.11474 | Validation Loss: 0.09720\n",
      "Epoch: 18 | Train Loss: 0.11148 | Validation Loss: 0.09494\n",
      "Epoch: 19 | Train Loss: 0.11001 | Validation Loss: 0.09437\n",
      "Epoch: 20 | Train Loss: 0.10787 | Validation Loss: 0.09230\n",
      "Epoch: 21 | Train Loss: 0.10580 | Validation Loss: 0.09239\n",
      "Epoch: 22 | Train Loss: 0.10351 | Validation Loss: 0.08783\n",
      "Epoch: 23 | Train Loss: 0.10114 | Validation Loss: 0.09027\n",
      "Epoch: 24 | Train Loss: 0.09969 | Validation Loss: 0.08641\n",
      "Epoch: 25 | Train Loss: 0.09805 | Validation Loss: 0.08512\n",
      "Epoch: 26 | Train Loss: 0.09739 | Validation Loss: 0.08211\n",
      "Epoch: 27 | Train Loss: 0.09599 | Validation Loss: 0.08316\n",
      "Epoch: 28 | Train Loss: 0.09481 | Validation Loss: 0.08244\n",
      "Epoch: 29 | Train Loss: 0.09347 | Validation Loss: 0.07980\n",
      "Epoch: 30 | Train Loss: 0.09226 | Validation Loss: 0.07943\n",
      "Epoch: 31 | Train Loss: 0.09164 | Validation Loss: 0.07988\n",
      "Epoch: 32 | Train Loss: 0.09077 | Validation Loss: 0.07967\n",
      "Epoch: 33 | Train Loss: 0.08976 | Validation Loss: 0.07901\n",
      "Epoch: 34 | Train Loss: 0.08898 | Validation Loss: 0.07524\n",
      "Epoch: 35 | Train Loss: 0.08779 | Validation Loss: 0.07522\n",
      "Epoch: 36 | Train Loss: 0.08777 | Validation Loss: 0.07534\n",
      "Epoch: 37 | Train Loss: 0.08666 | Validation Loss: 0.07597\n",
      "Epoch: 38 | Train Loss: 0.08629 | Validation Loss: 0.07508\n",
      "Epoch: 39 | Train Loss: 0.08585 | Validation Loss: 0.07468\n",
      "Epoch: 40 | Train Loss: 0.08537 | Validation Loss: 0.07348\n",
      "Epoch: 41 | Train Loss: 0.08474 | Validation Loss: 0.07331\n",
      "Epoch: 42 | Train Loss: 0.08395 | Validation Loss: 0.07362\n",
      "Epoch: 43 | Train Loss: 0.08328 | Validation Loss: 0.07268\n",
      "Epoch: 44 | Train Loss: 0.08280 | Validation Loss: 0.07322\n",
      "Epoch: 45 | Train Loss: 0.08313 | Validation Loss: 0.07198\n",
      "Epoch: 46 | Train Loss: 0.08217 | Validation Loss: 0.07218\n",
      "Epoch: 47 | Train Loss: 0.08237 | Validation Loss: 0.07139\n",
      "Epoch: 48 | Train Loss: 0.08175 | Validation Loss: 0.07185\n",
      "Epoch: 49 | Train Loss: 0.08120 | Validation Loss: 0.07302\n",
      "Epoch: 50 | Train Loss: 0.08148 | Validation Loss: 0.07035\n",
      "Epoch: 51 | Train Loss: 0.08103 | Validation Loss: 0.07013\n",
      "Epoch: 52 | Train Loss: 0.08121 | Validation Loss: 0.07191\n",
      "Epoch: 53 | Train Loss: 0.08041 | Validation Loss: 0.06998\n",
      "Epoch: 54 | Train Loss: 0.08012 | Validation Loss: 0.07041\n",
      "Epoch: 55 | Train Loss: 0.08039 | Validation Loss: 0.07016\n",
      "Epoch: 56 | Train Loss: 0.07988 | Validation Loss: 0.06954\n",
      "Epoch: 57 | Train Loss: 0.07967 | Validation Loss: 0.07000\n",
      "Epoch: 58 | Train Loss: 0.07949 | Validation Loss: 0.06867\n",
      "Epoch: 59 | Train Loss: 0.07946 | Validation Loss: 0.07035\n",
      "Epoch: 60 | Train Loss: 0.07936 | Validation Loss: 0.07091\n",
      "Epoch: 61 | Train Loss: 0.07910 | Validation Loss: 0.07100\n",
      "Early stopping at epoch 61 after 3 epochs without improvement.\n",
      "...Training done...\n",
      "Time elapsed for training: 19.067505248387654  minutes\n",
      "=================================================================================\n",
      "Final training R2: [0.87227495 0.91433323 0.92254044 0.75318813 0.96042301 0.89436213\n",
      " 0.96911466 0.95424086 0.95335119 0.97731058 0.97581254 0.97581479\n",
      " 0.97581872 0.9758081  0.96398655]\n",
      "Average final training R2:  0.935891991440182\n",
      "Final training loss: 0.06123352344832745\n",
      "Final validation R2: [0.85493243 0.904674   0.90855109 0.72753072 0.95557088 0.88588067\n",
      " 0.9668487  0.94940867 0.94510322 0.97723434 0.97056172 0.97056401\n",
      " 0.97056326 0.97056308 0.96004833]\n",
      "Average final validation R2:  0.9278690077560697\n",
      "Final validation loss: 0.06866830786785637\n",
      "=================================================================================\n",
      "Network 5 of 12\n",
      "Number of GAT layers: 3\n",
      "Number of hidden channels: 512\n",
      "Number of trainable parameters: 904,975\n",
      "=================================================================================\n",
      "Epoch: 01 | Train Loss: 0.30676 | Validation Loss: 0.24107\n",
      "Epoch: 02 | Train Loss: 0.24151 | Validation Loss: 0.22513\n",
      "Epoch: 03 | Train Loss: 0.21940 | Validation Loss: 0.20254\n",
      "Epoch: 04 | Train Loss: 0.20189 | Validation Loss: 0.18083\n",
      "Epoch: 05 | Train Loss: 0.18852 | Validation Loss: 0.16936\n",
      "Epoch: 06 | Train Loss: 0.17426 | Validation Loss: 0.15475\n",
      "Epoch: 07 | Train Loss: 0.16147 | Validation Loss: 0.14486\n",
      "Epoch: 08 | Train Loss: 0.15144 | Validation Loss: 0.13607\n",
      "Epoch: 09 | Train Loss: 0.14295 | Validation Loss: 0.12167\n",
      "Epoch: 10 | Train Loss: 0.13620 | Validation Loss: 0.11561\n",
      "Epoch: 11 | Train Loss: 0.12995 | Validation Loss: 0.11580\n",
      "Epoch: 12 | Train Loss: 0.12472 | Validation Loss: 0.10476\n",
      "Epoch: 13 | Train Loss: 0.12044 | Validation Loss: 0.09996\n",
      "Epoch: 14 | Train Loss: 0.11604 | Validation Loss: 0.09626\n",
      "Epoch: 15 | Train Loss: 0.11132 | Validation Loss: 0.09623\n",
      "Epoch: 16 | Train Loss: 0.10908 | Validation Loss: 0.09214\n",
      "Epoch: 17 | Train Loss: 0.10529 | Validation Loss: 0.08938\n",
      "Epoch: 18 | Train Loss: 0.10226 | Validation Loss: 0.08578\n",
      "Epoch: 19 | Train Loss: 0.09991 | Validation Loss: 0.08452\n",
      "Epoch: 20 | Train Loss: 0.09826 | Validation Loss: 0.08409\n",
      "Epoch: 21 | Train Loss: 0.09539 | Validation Loss: 0.07949\n",
      "Epoch: 22 | Train Loss: 0.09383 | Validation Loss: 0.08080\n",
      "Epoch: 23 | Train Loss: 0.09143 | Validation Loss: 0.07884\n",
      "Epoch: 24 | Train Loss: 0.08947 | Validation Loss: 0.07601\n",
      "Epoch: 25 | Train Loss: 0.08800 | Validation Loss: 0.07486\n",
      "Epoch: 26 | Train Loss: 0.08633 | Validation Loss: 0.07386\n",
      "Epoch: 27 | Train Loss: 0.08490 | Validation Loss: 0.07248\n",
      "Epoch: 28 | Train Loss: 0.08355 | Validation Loss: 0.07629\n",
      "Epoch: 29 | Train Loss: 0.08252 | Validation Loss: 0.06992\n",
      "Epoch: 30 | Train Loss: 0.08170 | Validation Loss: 0.06972\n",
      "Epoch: 31 | Train Loss: 0.08068 | Validation Loss: 0.06931\n",
      "Epoch: 32 | Train Loss: 0.07959 | Validation Loss: 0.07013\n",
      "Epoch: 33 | Train Loss: 0.07893 | Validation Loss: 0.06789\n",
      "Epoch: 34 | Train Loss: 0.07806 | Validation Loss: 0.06640\n",
      "Epoch: 35 | Train Loss: 0.07700 | Validation Loss: 0.06676\n",
      "Epoch: 36 | Train Loss: 0.07593 | Validation Loss: 0.06591\n",
      "Epoch: 37 | Train Loss: 0.07563 | Validation Loss: 0.06583\n",
      "Epoch: 38 | Train Loss: 0.07508 | Validation Loss: 0.06577\n",
      "Epoch: 39 | Train Loss: 0.07408 | Validation Loss: 0.06614\n",
      "Epoch: 40 | Train Loss: 0.07387 | Validation Loss: 0.06389\n",
      "Epoch: 41 | Train Loss: 0.07323 | Validation Loss: 0.06346\n",
      "Epoch: 42 | Train Loss: 0.07251 | Validation Loss: 0.06393\n",
      "Epoch: 43 | Train Loss: 0.07233 | Validation Loss: 0.06495\n",
      "Epoch: 44 | Train Loss: 0.07210 | Validation Loss: 0.06298\n",
      "Epoch: 45 | Train Loss: 0.07122 | Validation Loss: 0.06236\n",
      "Epoch: 46 | Train Loss: 0.07123 | Validation Loss: 0.06283\n",
      "Epoch: 47 | Train Loss: 0.07048 | Validation Loss: 0.06222\n",
      "Epoch: 48 | Train Loss: 0.06993 | Validation Loss: 0.06255\n",
      "Epoch: 49 | Train Loss: 0.06994 | Validation Loss: 0.06161\n",
      "Epoch: 50 | Train Loss: 0.06937 | Validation Loss: 0.06274\n",
      "Epoch: 51 | Train Loss: 0.06902 | Validation Loss: 0.06183\n",
      "Epoch: 52 | Train Loss: 0.06932 | Validation Loss: 0.06093\n",
      "Epoch: 53 | Train Loss: 0.06860 | Validation Loss: 0.06178\n",
      "Epoch: 54 | Train Loss: 0.06835 | Validation Loss: 0.06128\n",
      "Epoch: 55 | Train Loss: 0.06816 | Validation Loss: 0.06055\n",
      "Epoch: 56 | Train Loss: 0.06803 | Validation Loss: 0.06041\n",
      "Epoch: 57 | Train Loss: 0.06775 | Validation Loss: 0.06033\n",
      "Epoch: 58 | Train Loss: 0.06792 | Validation Loss: 0.06267\n",
      "Epoch: 59 | Train Loss: 0.06771 | Validation Loss: 0.06143\n",
      "Epoch: 60 | Train Loss: 0.06742 | Validation Loss: 0.06015\n",
      "Epoch: 61 | Train Loss: 0.06695 | Validation Loss: 0.06064\n",
      "Epoch: 62 | Train Loss: 0.06740 | Validation Loss: 0.05959\n",
      "Epoch: 63 | Train Loss: 0.06680 | Validation Loss: 0.05940\n",
      "Epoch: 64 | Train Loss: 0.06688 | Validation Loss: 0.06170\n",
      "Epoch: 65 | Train Loss: 0.06672 | Validation Loss: 0.05975\n",
      "Epoch: 66 | Train Loss: 0.06665 | Validation Loss: 0.06090\n",
      "Early stopping at epoch 66 after 3 epochs without improvement.\n",
      "...Training done...\n",
      "Time elapsed for training: 19.22512193918228  minutes\n",
      "=================================================================================\n",
      "Final training R2: [0.88944734 0.92589479 0.93491906 0.78367629 0.96951601 0.9117366\n",
      " 0.97466775 0.96124519 0.9615492  0.98227548 0.9822316  0.98225897\n",
      " 0.98224249 0.98224482 0.9727835 ]\n",
      "Average final training R2:  0.9464459401006432\n",
      "Final training loss: 0.051047569937530284\n",
      "Final validation R2: [0.87083147 0.91768299 0.92292896 0.74386373 0.96415004 0.90212105\n",
      " 0.9721981  0.95609024 0.95402668 0.98148585 0.97754511 0.9775661\n",
      " 0.97755447 0.97755516 0.96889398]\n",
      "Average final validation R2:  0.9376329287112519\n",
      "Final validation loss: 0.05940454783968818\n",
      "=================================================================================\n",
      "Network 6 of 12\n",
      "Number of GAT layers: 3\n",
      "Number of hidden channels: 1024\n",
      "Number of trainable parameters: 3,380,495\n",
      "=================================================================================\n",
      "Epoch: 01 | Train Loss: 0.31011 | Validation Loss: 0.25171\n",
      "Epoch: 02 | Train Loss: 0.23994 | Validation Loss: 0.20726\n",
      "Epoch: 03 | Train Loss: 0.21299 | Validation Loss: 0.20253\n",
      "Epoch: 04 | Train Loss: 0.19058 | Validation Loss: 0.17026\n",
      "Epoch: 05 | Train Loss: 0.17116 | Validation Loss: 0.15668\n",
      "Epoch: 06 | Train Loss: 0.15475 | Validation Loss: 0.14304\n",
      "Epoch: 07 | Train Loss: 0.14203 | Validation Loss: 0.12710\n",
      "Epoch: 08 | Train Loss: 0.13177 | Validation Loss: 0.11388\n",
      "Epoch: 09 | Train Loss: 0.12294 | Validation Loss: 0.11115\n",
      "Epoch: 10 | Train Loss: 0.11555 | Validation Loss: 0.10296\n",
      "Epoch: 11 | Train Loss: 0.10916 | Validation Loss: 0.10551\n",
      "Epoch: 12 | Train Loss: 0.10243 | Validation Loss: 0.08828\n",
      "Epoch: 13 | Train Loss: 0.09710 | Validation Loss: 0.09132\n",
      "Epoch: 14 | Train Loss: 0.09327 | Validation Loss: 0.08366\n",
      "Epoch: 15 | Train Loss: 0.08874 | Validation Loss: 0.08458\n",
      "Epoch: 16 | Train Loss: 0.08502 | Validation Loss: 0.07869\n",
      "Epoch: 17 | Train Loss: 0.08219 | Validation Loss: 0.07454\n",
      "Epoch: 18 | Train Loss: 0.07916 | Validation Loss: 0.07059\n",
      "Epoch: 19 | Train Loss: 0.07672 | Validation Loss: 0.07039\n",
      "Epoch: 20 | Train Loss: 0.07356 | Validation Loss: 0.06990\n",
      "Epoch: 21 | Train Loss: 0.07152 | Validation Loss: 0.06521\n",
      "Epoch: 22 | Train Loss: 0.06937 | Validation Loss: 0.06495\n",
      "Epoch: 23 | Train Loss: 0.06770 | Validation Loss: 0.06327\n",
      "Epoch: 24 | Train Loss: 0.06609 | Validation Loss: 0.06121\n",
      "Epoch: 25 | Train Loss: 0.06446 | Validation Loss: 0.06086\n",
      "Epoch: 26 | Train Loss: 0.06246 | Validation Loss: 0.05852\n",
      "Epoch: 27 | Train Loss: 0.06161 | Validation Loss: 0.05837\n",
      "Epoch: 28 | Train Loss: 0.06023 | Validation Loss: 0.05651\n",
      "Epoch: 29 | Train Loss: 0.05900 | Validation Loss: 0.05676\n",
      "Epoch: 30 | Train Loss: 0.05803 | Validation Loss: 0.05634\n",
      "Epoch: 31 | Train Loss: 0.05682 | Validation Loss: 0.05586\n",
      "Epoch: 32 | Train Loss: 0.05540 | Validation Loss: 0.05578\n",
      "Epoch: 33 | Train Loss: 0.05487 | Validation Loss: 0.05434\n",
      "Epoch: 34 | Train Loss: 0.05431 | Validation Loss: 0.05369\n",
      "Epoch: 35 | Train Loss: 0.05332 | Validation Loss: 0.05371\n",
      "Epoch: 36 | Train Loss: 0.05267 | Validation Loss: 0.05228\n",
      "Epoch: 37 | Train Loss: 0.05190 | Validation Loss: 0.05142\n",
      "Epoch: 38 | Train Loss: 0.05163 | Validation Loss: 0.05287\n",
      "Epoch: 39 | Train Loss: 0.05070 | Validation Loss: 0.05148\n",
      "Epoch: 40 | Train Loss: 0.05020 | Validation Loss: 0.05054\n",
      "Epoch: 41 | Train Loss: 0.04951 | Validation Loss: 0.05000\n",
      "Epoch: 42 | Train Loss: 0.04888 | Validation Loss: 0.04977\n",
      "Epoch: 43 | Train Loss: 0.04885 | Validation Loss: 0.05034\n",
      "Epoch: 44 | Train Loss: 0.04806 | Validation Loss: 0.04951\n",
      "Epoch: 45 | Train Loss: 0.04756 | Validation Loss: 0.04989\n",
      "Epoch: 46 | Train Loss: 0.04725 | Validation Loss: 0.05015\n",
      "Epoch: 47 | Train Loss: 0.04638 | Validation Loss: 0.04889\n",
      "Epoch: 48 | Train Loss: 0.04688 | Validation Loss: 0.04884\n",
      "Epoch: 49 | Train Loss: 0.04631 | Validation Loss: 0.04980\n",
      "Epoch: 50 | Train Loss: 0.04621 | Validation Loss: 0.04852\n",
      "Epoch: 51 | Train Loss: 0.04591 | Validation Loss: 0.04906\n",
      "Epoch: 52 | Train Loss: 0.04548 | Validation Loss: 0.04854\n",
      "Epoch: 53 | Train Loss: 0.04519 | Validation Loss: 0.04869\n",
      "Early stopping at epoch 53 after 3 epochs without improvement.\n",
      "...Training done...\n",
      "Time elapsed for training: 15.939645409584045  minutes\n",
      "=================================================================================\n",
      "Final training R2: [0.92424038 0.94927602 0.95160171 0.84199167 0.97728536 0.93234428\n",
      " 0.97988158 0.97085282 0.97096607 0.98583811 0.98656976 0.98655872\n",
      " 0.98654811 0.98656708 0.97815136]\n",
      "Average final training R2:  0.9605782024304006\n",
      "Final training loss: 0.037577289467056595\n",
      "Final validation R2: [0.90103589 0.93782022 0.93808307 0.77516698 0.97328303 0.91719191\n",
      " 0.97664146 0.9632076  0.96192449 0.98523686 0.98321303 0.98320156\n",
      " 0.98319075 0.98321049 0.97450185]\n",
      "Average final validation R2:  0.9491272802179604\n",
      "Final validation loss: 0.04851710680537881\n",
      "=================================================================================\n",
      "Network 7 of 12\n",
      "Number of GAT layers: 5\n",
      "Number of hidden channels: 256\n",
      "Number of trainable parameters: 391,183\n",
      "=================================================================================\n",
      "Epoch: 01 | Train Loss: 0.30713 | Validation Loss: 0.24241\n",
      "Epoch: 02 | Train Loss: 0.23763 | Validation Loss: 0.21966\n",
      "Epoch: 03 | Train Loss: 0.21396 | Validation Loss: 0.19634\n",
      "Epoch: 04 | Train Loss: 0.19498 | Validation Loss: 0.17795\n",
      "Epoch: 05 | Train Loss: 0.17843 | Validation Loss: 0.16132\n",
      "Epoch: 06 | Train Loss: 0.16479 | Validation Loss: 0.14548\n",
      "Epoch: 07 | Train Loss: 0.15423 | Validation Loss: 0.14254\n",
      "Epoch: 08 | Train Loss: 0.14702 | Validation Loss: 0.12934\n",
      "Epoch: 09 | Train Loss: 0.13882 | Validation Loss: 0.12305\n",
      "Epoch: 10 | Train Loss: 0.13255 | Validation Loss: 0.11866\n",
      "Epoch: 11 | Train Loss: 0.12686 | Validation Loss: 0.11416\n",
      "Epoch: 12 | Train Loss: 0.12173 | Validation Loss: 0.11504\n",
      "Epoch: 13 | Train Loss: 0.11742 | Validation Loss: 0.10195\n",
      "Epoch: 14 | Train Loss: 0.11254 | Validation Loss: 0.10222\n",
      "Epoch: 15 | Train Loss: 0.10911 | Validation Loss: 0.10182\n",
      "Epoch: 16 | Train Loss: 0.10591 | Validation Loss: 0.09466\n",
      "Epoch: 17 | Train Loss: 0.10343 | Validation Loss: 0.09167\n",
      "Epoch: 18 | Train Loss: 0.10082 | Validation Loss: 0.09322\n",
      "Epoch: 19 | Train Loss: 0.09809 | Validation Loss: 0.08557\n",
      "Epoch: 20 | Train Loss: 0.09561 | Validation Loss: 0.08760\n",
      "Epoch: 21 | Train Loss: 0.09492 | Validation Loss: 0.08513\n",
      "Epoch: 22 | Train Loss: 0.09183 | Validation Loss: 0.08267\n",
      "Epoch: 23 | Train Loss: 0.09109 | Validation Loss: 0.08019\n",
      "Epoch: 24 | Train Loss: 0.08919 | Validation Loss: 0.08014\n",
      "Epoch: 25 | Train Loss: 0.08701 | Validation Loss: 0.07788\n",
      "Epoch: 26 | Train Loss: 0.08648 | Validation Loss: 0.07843\n",
      "Epoch: 27 | Train Loss: 0.08543 | Validation Loss: 0.07580\n",
      "Epoch: 28 | Train Loss: 0.08416 | Validation Loss: 0.07729\n",
      "Epoch: 29 | Train Loss: 0.08233 | Validation Loss: 0.07496\n",
      "Epoch: 30 | Train Loss: 0.08148 | Validation Loss: 0.07399\n",
      "Epoch: 31 | Train Loss: 0.08050 | Validation Loss: 0.07328\n",
      "Epoch: 32 | Train Loss: 0.07992 | Validation Loss: 0.07234\n",
      "Epoch: 33 | Train Loss: 0.07932 | Validation Loss: 0.07174\n",
      "Epoch: 34 | Train Loss: 0.07769 | Validation Loss: 0.07195\n",
      "Epoch: 35 | Train Loss: 0.07765 | Validation Loss: 0.07131\n",
      "Epoch: 36 | Train Loss: 0.07662 | Validation Loss: 0.07055\n",
      "Epoch: 37 | Train Loss: 0.07668 | Validation Loss: 0.07183\n",
      "Epoch: 38 | Train Loss: 0.07533 | Validation Loss: 0.07035\n",
      "Epoch: 39 | Train Loss: 0.07512 | Validation Loss: 0.06972\n",
      "Epoch: 40 | Train Loss: 0.07468 | Validation Loss: 0.06889\n",
      "Epoch: 41 | Train Loss: 0.07361 | Validation Loss: 0.06880\n",
      "Epoch: 42 | Train Loss: 0.07348 | Validation Loss: 0.06855\n",
      "Epoch: 43 | Train Loss: 0.07316 | Validation Loss: 0.06819\n",
      "Epoch: 44 | Train Loss: 0.07276 | Validation Loss: 0.06737\n",
      "Epoch: 45 | Train Loss: 0.07261 | Validation Loss: 0.06817\n",
      "Epoch: 46 | Train Loss: 0.07193 | Validation Loss: 0.06763\n",
      "Epoch: 47 | Train Loss: 0.07212 | Validation Loss: 0.06768\n",
      "Early stopping at epoch 47 after 3 epochs without improvement.\n",
      "...Training done...\n",
      "Time elapsed for training: 19.038998583952587  minutes\n",
      "=================================================================================\n",
      "Final training R2: [0.87970863 0.91855021 0.9265602  0.76145336 0.96102291 0.89745927\n",
      " 0.96840643 0.95388103 0.95647158 0.97517611 0.97720865 0.9772104\n",
      " 0.97719878 0.97722308 0.96614464]\n",
      "Average final training R2:  0.9382450181935409\n",
      "Final training loss: 0.05876464523284115\n",
      "Final validation R2: [0.85997535 0.9077553  0.91338674 0.72578293 0.9559555  0.88780826\n",
      " 0.96669626 0.94928565 0.94810273 0.97516512 0.971674   0.97167348\n",
      " 0.97165588 0.97169526 0.96238226]\n",
      "Average final validation R2:  0.9292663144462525\n",
      "Final validation loss: 0.06736911102556266\n",
      "=================================================================================\n",
      "Network 8 of 12\n",
      "Number of GAT layers: 5\n",
      "Number of hidden channels: 512\n",
      "Number of trainable parameters: 1,435,407\n",
      "=================================================================================\n",
      "Epoch: 01 | Train Loss: 0.30942 | Validation Loss: 0.24725\n",
      "Epoch: 02 | Train Loss: 0.24060 | Validation Loss: 0.24319\n",
      "Epoch: 03 | Train Loss: 0.21355 | Validation Loss: 0.19644\n",
      "Epoch: 04 | Train Loss: 0.19038 | Validation Loss: 0.17114\n",
      "Epoch: 05 | Train Loss: 0.17210 | Validation Loss: 0.15664\n",
      "Epoch: 06 | Train Loss: 0.15948 | Validation Loss: 0.14262\n",
      "Epoch: 07 | Train Loss: 0.14834 | Validation Loss: 0.13130\n",
      "Epoch: 08 | Train Loss: 0.13830 | Validation Loss: 0.12388\n",
      "Epoch: 09 | Train Loss: 0.13018 | Validation Loss: 0.11251\n",
      "Epoch: 10 | Train Loss: 0.12150 | Validation Loss: 0.10949\n",
      "Epoch: 11 | Train Loss: 0.11710 | Validation Loss: 0.10645\n",
      "Epoch: 12 | Train Loss: 0.11033 | Validation Loss: 0.09642\n",
      "Epoch: 13 | Train Loss: 0.10460 | Validation Loss: 0.09161\n",
      "Epoch: 14 | Train Loss: 0.10073 | Validation Loss: 0.09369\n",
      "Epoch: 15 | Train Loss: 0.09697 | Validation Loss: 0.08686\n",
      "Epoch: 16 | Train Loss: 0.09392 | Validation Loss: 0.08339\n",
      "Epoch: 17 | Train Loss: 0.09000 | Validation Loss: 0.08113\n",
      "Epoch: 18 | Train Loss: 0.08767 | Validation Loss: 0.08149\n",
      "Epoch: 19 | Train Loss: 0.08497 | Validation Loss: 0.07858\n",
      "Epoch: 20 | Train Loss: 0.08262 | Validation Loss: 0.07536\n",
      "Epoch: 21 | Train Loss: 0.08043 | Validation Loss: 0.07365\n",
      "Epoch: 22 | Train Loss: 0.07810 | Validation Loss: 0.07129\n",
      "Epoch: 23 | Train Loss: 0.07640 | Validation Loss: 0.07212\n",
      "Epoch: 24 | Train Loss: 0.07518 | Validation Loss: 0.06857\n",
      "Epoch: 25 | Train Loss: 0.07257 | Validation Loss: 0.06962\n",
      "Epoch: 26 | Train Loss: 0.07151 | Validation Loss: 0.06673\n",
      "Epoch: 27 | Train Loss: 0.06994 | Validation Loss: 0.06371\n",
      "Epoch: 28 | Train Loss: 0.06884 | Validation Loss: 0.06447\n",
      "Epoch: 29 | Train Loss: 0.06792 | Validation Loss: 0.06374\n",
      "Epoch: 30 | Train Loss: 0.06630 | Validation Loss: 0.06265\n",
      "Epoch: 31 | Train Loss: 0.06587 | Validation Loss: 0.06262\n",
      "Epoch: 32 | Train Loss: 0.06404 | Validation Loss: 0.06327\n",
      "Epoch: 33 | Train Loss: 0.06317 | Validation Loss: 0.06152\n",
      "Epoch: 34 | Train Loss: 0.06245 | Validation Loss: 0.06071\n",
      "Epoch: 35 | Train Loss: 0.06161 | Validation Loss: 0.05894\n",
      "Epoch: 36 | Train Loss: 0.06071 | Validation Loss: 0.05955\n",
      "Epoch: 37 | Train Loss: 0.06009 | Validation Loss: 0.05920\n",
      "Epoch: 38 | Train Loss: 0.05966 | Validation Loss: 0.05737\n",
      "Epoch: 39 | Train Loss: 0.05898 | Validation Loss: 0.05886\n",
      "Epoch: 40 | Train Loss: 0.05810 | Validation Loss: 0.05756\n",
      "Epoch: 41 | Train Loss: 0.05737 | Validation Loss: 0.05840\n",
      "Early stopping at epoch 41 after 3 epochs without improvement.\n",
      "...Training done...\n",
      "Time elapsed for training: 16.061793847878775  minutes\n",
      "=================================================================================\n",
      "Final training R2: [0.90144757 0.93565727 0.94164049 0.79189936 0.9712116  0.91381657\n",
      " 0.97405727 0.9616617  0.96579964 0.98028691 0.98394243 0.98395159\n",
      " 0.9839262  0.98394573 0.97323653]\n",
      "Average final training R2:  0.9497653912513119\n",
      "Final training loss: 0.047772525255218405\n",
      "Final validation R2: [0.88056432 0.92413729 0.92724338 0.74464546 0.96642418 0.90230655\n",
      " 0.97152503 0.9564697  0.95681764 0.97977244 0.97948309 0.9794953\n",
      " 0.97945695 0.97948353 0.96924863]\n",
      "Average final validation R2:  0.9398049002152603\n",
      "Final validation loss: 0.057372022503748156\n",
      "=================================================================================\n",
      "Network 9 of 12\n",
      "Number of GAT layers: 5\n",
      "Number of hidden channels: 1024\n",
      "Number of trainable parameters: 5,489,935\n",
      "=================================================================================\n",
      "Epoch: 01 | Train Loss: 0.32092 | Validation Loss: 0.27343\n",
      "Epoch: 02 | Train Loss: 0.25489 | Validation Loss: 0.23197\n",
      "Epoch: 03 | Train Loss: 0.23064 | Validation Loss: 0.21437\n",
      "Epoch: 04 | Train Loss: 0.20988 | Validation Loss: 0.19656\n",
      "Epoch: 05 | Train Loss: 0.19280 | Validation Loss: 0.17361\n",
      "Epoch: 06 | Train Loss: 0.18364 | Validation Loss: 0.16705\n",
      "Epoch: 07 | Train Loss: 0.16566 | Validation Loss: 0.15326\n",
      "Epoch: 08 | Train Loss: 0.15584 | Validation Loss: 0.13971\n",
      "Epoch: 09 | Train Loss: 0.14676 | Validation Loss: 0.12874\n",
      "Epoch: 10 | Train Loss: 0.14014 | Validation Loss: 0.12954\n",
      "Epoch: 11 | Train Loss: 0.12962 | Validation Loss: 0.11591\n",
      "Epoch: 12 | Train Loss: 0.12267 | Validation Loss: 0.11021\n",
      "Epoch: 13 | Train Loss: 0.11652 | Validation Loss: 0.10224\n",
      "Epoch: 14 | Train Loss: 0.11071 | Validation Loss: 0.09604\n",
      "Epoch: 15 | Train Loss: 0.10559 | Validation Loss: 0.08907\n",
      "Epoch: 16 | Train Loss: 0.10083 | Validation Loss: 0.08780\n",
      "Epoch: 17 | Train Loss: 0.09630 | Validation Loss: 0.08676\n",
      "Epoch: 18 | Train Loss: 0.09323 | Validation Loss: 0.08345\n",
      "Epoch: 19 | Train Loss: 0.08921 | Validation Loss: 0.07667\n",
      "Epoch: 20 | Train Loss: 0.08625 | Validation Loss: 0.07703\n",
      "Epoch: 21 | Train Loss: 0.08304 | Validation Loss: 0.07015\n",
      "Epoch: 22 | Train Loss: 0.08094 | Validation Loss: 0.07158\n",
      "Epoch: 23 | Train Loss: 0.07781 | Validation Loss: 0.06834\n",
      "Epoch: 24 | Train Loss: 0.07612 | Validation Loss: 0.06761\n",
      "Epoch: 25 | Train Loss: 0.07445 | Validation Loss: 0.06440\n",
      "Epoch: 26 | Train Loss: 0.07217 | Validation Loss: 0.06462\n",
      "Epoch: 27 | Train Loss: 0.07072 | Validation Loss: 0.06198\n",
      "Epoch: 28 | Train Loss: 0.06911 | Validation Loss: 0.06219\n",
      "Epoch: 29 | Train Loss: 0.06814 | Validation Loss: 0.06038\n",
      "Epoch: 30 | Train Loss: 0.06636 | Validation Loss: 0.05989\n",
      "Epoch: 31 | Train Loss: 0.06521 | Validation Loss: 0.05980\n",
      "Epoch: 32 | Train Loss: 0.06428 | Validation Loss: 0.05692\n",
      "Epoch: 33 | Train Loss: 0.06303 | Validation Loss: 0.05710\n",
      "Epoch: 34 | Train Loss: 0.06214 | Validation Loss: 0.05656\n",
      "Epoch: 35 | Train Loss: 0.06111 | Validation Loss: 0.05487\n",
      "Epoch: 36 | Train Loss: 0.06012 | Validation Loss: 0.05554\n",
      "Epoch: 37 | Train Loss: 0.05932 | Validation Loss: 0.05633\n",
      "Epoch: 38 | Train Loss: 0.05889 | Validation Loss: 0.05365\n",
      "Epoch: 39 | Train Loss: 0.05764 | Validation Loss: 0.05360\n",
      "Epoch: 40 | Train Loss: 0.05723 | Validation Loss: 0.05244\n",
      "Epoch: 41 | Train Loss: 0.05668 | Validation Loss: 0.05288\n",
      "Epoch: 42 | Train Loss: 0.05637 | Validation Loss: 0.05177\n",
      "Epoch: 43 | Train Loss: 0.05550 | Validation Loss: 0.05116\n",
      "Epoch: 44 | Train Loss: 0.05481 | Validation Loss: 0.05150\n",
      "Epoch: 45 | Train Loss: 0.05454 | Validation Loss: 0.05098\n",
      "Epoch: 46 | Train Loss: 0.05395 | Validation Loss: 0.05102\n",
      "Epoch: 47 | Train Loss: 0.05359 | Validation Loss: 0.05008\n",
      "Epoch: 48 | Train Loss: 0.05344 | Validation Loss: 0.04955\n",
      "Epoch: 49 | Train Loss: 0.05291 | Validation Loss: 0.04976\n",
      "Epoch: 50 | Train Loss: 0.05271 | Validation Loss: 0.05071\n",
      "Epoch: 51 | Train Loss: 0.05236 | Validation Loss: 0.04981\n",
      "Early stopping at epoch 51 after 3 epochs without improvement.\n",
      "...Training done...\n",
      "Time elapsed for training: 20.862614965438844  minutes\n",
      "=================================================================================\n",
      "Final training R2: [0.91196847 0.94457038 0.9503764  0.81097099 0.9767863  0.92576286\n",
      " 0.97900759 0.96798785 0.97108529 0.98658098 0.98741301 0.98741603\n",
      " 0.98741409 0.98740971 0.98079018]\n",
      "Average final training R2:  0.9570360086734869\n",
      "Final training loss: 0.04086622729174932\n",
      "Final validation R2: [0.89263534 0.93508236 0.93897387 0.75846295 0.97384435 0.91298633\n",
      " 0.97623164 0.96129861 0.96446969 0.98633635 0.98532458 0.98533182\n",
      " 0.98533358 0.98532718 0.97847096]\n",
      "Average final validation R2:  0.9480073069768091\n",
      "Final validation loss: 0.04954532619255284\n",
      "=================================================================================\n",
      "Network 10 of 12\n",
      "Number of GAT layers: 7\n",
      "Number of hidden channels: 256\n",
      "Number of trainable parameters: 525,327\n",
      "=================================================================================\n",
      "Epoch: 01 | Train Loss: 0.31913 | Validation Loss: 0.25240\n",
      "Epoch: 02 | Train Loss: 0.24618 | Validation Loss: 0.22476\n",
      "Epoch: 03 | Train Loss: 0.22240 | Validation Loss: 0.20004\n",
      "Epoch: 04 | Train Loss: 0.20176 | Validation Loss: 0.17439\n",
      "Epoch: 05 | Train Loss: 0.18431 | Validation Loss: 0.17228\n",
      "Epoch: 06 | Train Loss: 0.17075 | Validation Loss: 0.15791\n",
      "Epoch: 07 | Train Loss: 0.15802 | Validation Loss: 0.14482\n",
      "Epoch: 08 | Train Loss: 0.15082 | Validation Loss: 0.14305\n",
      "Epoch: 09 | Train Loss: 0.14269 | Validation Loss: 0.12681\n",
      "Epoch: 10 | Train Loss: 0.13571 | Validation Loss: 0.12695\n",
      "Epoch: 11 | Train Loss: 0.12915 | Validation Loss: 0.11528\n",
      "Epoch: 12 | Train Loss: 0.12373 | Validation Loss: 0.10957\n",
      "Epoch: 13 | Train Loss: 0.12044 | Validation Loss: 0.10659\n",
      "Epoch: 14 | Train Loss: 0.11546 | Validation Loss: 0.10685\n",
      "Epoch: 15 | Train Loss: 0.11207 | Validation Loss: 0.10890\n",
      "Epoch: 16 | Train Loss: 0.10970 | Validation Loss: 0.09762\n",
      "Epoch: 17 | Train Loss: 0.10621 | Validation Loss: 0.09983\n",
      "Epoch: 18 | Train Loss: 0.10207 | Validation Loss: 0.09337\n",
      "Epoch: 19 | Train Loss: 0.10015 | Validation Loss: 0.08993\n",
      "Epoch: 20 | Train Loss: 0.09796 | Validation Loss: 0.08972\n",
      "Epoch: 21 | Train Loss: 0.09593 | Validation Loss: 0.08829\n",
      "Epoch: 22 | Train Loss: 0.09298 | Validation Loss: 0.08517\n",
      "Epoch: 23 | Train Loss: 0.09163 | Validation Loss: 0.08526\n",
      "Epoch: 24 | Train Loss: 0.09001 | Validation Loss: 0.08706\n",
      "Epoch: 25 | Train Loss: 0.08834 | Validation Loss: 0.08291\n",
      "Epoch: 26 | Train Loss: 0.08640 | Validation Loss: 0.08157\n",
      "Epoch: 27 | Train Loss: 0.08571 | Validation Loss: 0.07870\n",
      "Epoch: 28 | Train Loss: 0.08302 | Validation Loss: 0.07700\n",
      "Epoch: 29 | Train Loss: 0.08342 | Validation Loss: 0.07879\n",
      "Epoch: 30 | Train Loss: 0.08202 | Validation Loss: 0.07829\n",
      "Epoch: 31 | Train Loss: 0.08061 | Validation Loss: 0.07808\n",
      "Early stopping at epoch 31 after 3 epochs without improvement.\n",
      "...Training done...\n",
      "Time elapsed for training: 15.901388955116271  minutes\n",
      "=================================================================================\n",
      "Final training R2: [0.85431477 0.90011484 0.90815277 0.72986317 0.95242126 0.88421429\n",
      " 0.96336856 0.94711601 0.94605557 0.97003737 0.97073258 0.9707412\n",
      " 0.9707284  0.9707414  0.95614659]\n",
      "Average final training R2:  0.9263165845132398\n",
      "Final training loss: 0.06995238404656422\n",
      "Final validation R2: [0.83886274 0.89123548 0.89500707 0.70576933 0.9490345  0.87567499\n",
      " 0.96231232 0.94332129 0.93983004 0.97111685 0.96597197 0.96600156\n",
      " 0.96596323 0.96600559 0.95328138]\n",
      "Average final validation R2:  0.9192925562825456\n",
      "Final validation loss: 0.07700324805978781\n",
      "=================================================================================\n",
      "Network 11 of 12\n",
      "Number of GAT layers: 7\n",
      "Number of hidden channels: 512\n",
      "Number of trainable parameters: 1,965,839\n",
      "=================================================================================\n",
      "Epoch: 01 | Train Loss: 0.32143 | Validation Loss: 0.25391\n",
      "Epoch: 02 | Train Loss: 0.25205 | Validation Loss: 0.22322\n",
      "Epoch: 03 | Train Loss: 0.22599 | Validation Loss: 0.20060\n",
      "Epoch: 04 | Train Loss: 0.20615 | Validation Loss: 0.19661\n",
      "Epoch: 05 | Train Loss: 0.18524 | Validation Loss: 0.19628\n",
      "Epoch: 06 | Train Loss: 0.16980 | Validation Loss: 0.14655\n",
      "Epoch: 07 | Train Loss: 0.15661 | Validation Loss: 0.15248\n",
      "Epoch: 08 | Train Loss: 0.14600 | Validation Loss: 0.13596\n",
      "Epoch: 09 | Train Loss: 0.13837 | Validation Loss: 0.11988\n",
      "Epoch: 10 | Train Loss: 0.12946 | Validation Loss: 0.11343\n",
      "Epoch: 11 | Train Loss: 0.12402 | Validation Loss: 0.11321\n",
      "Epoch: 12 | Train Loss: 0.11691 | Validation Loss: 0.10536\n",
      "Epoch: 13 | Train Loss: 0.11142 | Validation Loss: 0.10684\n",
      "Epoch: 14 | Train Loss: 0.10645 | Validation Loss: 0.09595\n",
      "Epoch: 15 | Train Loss: 0.10285 | Validation Loss: 0.09674\n",
      "Epoch: 16 | Train Loss: 0.09879 | Validation Loss: 0.08955\n",
      "Epoch: 17 | Train Loss: 0.09363 | Validation Loss: 0.09108\n",
      "Epoch: 18 | Train Loss: 0.09214 | Validation Loss: 0.08702\n",
      "Epoch: 19 | Train Loss: 0.08862 | Validation Loss: 0.08283\n",
      "Epoch: 20 | Train Loss: 0.08625 | Validation Loss: 0.08005\n",
      "Epoch: 21 | Train Loss: 0.08281 | Validation Loss: 0.07684\n",
      "Epoch: 22 | Train Loss: 0.08247 | Validation Loss: 0.07787\n",
      "Epoch: 23 | Train Loss: 0.07892 | Validation Loss: 0.07360\n",
      "Epoch: 24 | Train Loss: 0.07695 | Validation Loss: 0.07264\n",
      "Epoch: 25 | Train Loss: 0.07542 | Validation Loss: 0.07224\n",
      "Epoch: 26 | Train Loss: 0.07352 | Validation Loss: 0.07188\n",
      "Epoch: 27 | Train Loss: 0.07216 | Validation Loss: 0.07074\n",
      "Epoch: 28 | Train Loss: 0.07049 | Validation Loss: 0.06866\n",
      "Epoch: 29 | Train Loss: 0.06917 | Validation Loss: 0.06770\n",
      "Epoch: 30 | Train Loss: 0.06760 | Validation Loss: 0.06671\n",
      "Epoch: 31 | Train Loss: 0.06642 | Validation Loss: 0.06651\n",
      "Epoch: 32 | Train Loss: 0.06515 | Validation Loss: 0.06375\n",
      "Epoch: 33 | Train Loss: 0.06426 | Validation Loss: 0.06532\n",
      "Epoch: 34 | Train Loss: 0.06308 | Validation Loss: 0.06535\n",
      "Epoch: 35 | Train Loss: 0.06249 | Validation Loss: 0.06355\n",
      "Epoch: 36 | Train Loss: 0.06174 | Validation Loss: 0.06221\n",
      "Epoch: 37 | Train Loss: 0.06120 | Validation Loss: 0.06248\n",
      "Epoch: 38 | Train Loss: 0.06001 | Validation Loss: 0.06299\n",
      "Epoch: 39 | Train Loss: 0.05913 | Validation Loss: 0.06105\n",
      "Epoch: 40 | Train Loss: 0.05866 | Validation Loss: 0.06140\n",
      "Epoch: 41 | Train Loss: 0.05782 | Validation Loss: 0.06031\n",
      "Epoch: 42 | Train Loss: 0.05748 | Validation Loss: 0.05905\n",
      "Epoch: 43 | Train Loss: 0.05684 | Validation Loss: 0.05883\n",
      "Epoch: 44 | Train Loss: 0.05654 | Validation Loss: 0.05920\n",
      "Epoch: 45 | Train Loss: 0.05633 | Validation Loss: 0.05843\n",
      "Epoch: 46 | Train Loss: 0.05543 | Validation Loss: 0.05839\n",
      "Epoch: 47 | Train Loss: 0.05496 | Validation Loss: 0.05827\n",
      "Epoch: 48 | Train Loss: 0.05505 | Validation Loss: 0.05763\n",
      "Epoch: 49 | Train Loss: 0.05485 | Validation Loss: 0.05773\n",
      "Epoch: 50 | Train Loss: 0.05385 | Validation Loss: 0.05670\n",
      "Epoch: 51 | Train Loss: 0.05368 | Validation Loss: 0.05774\n",
      "Epoch: 52 | Train Loss: 0.05336 | Validation Loss: 0.05698\n",
      "Epoch: 53 | Train Loss: 0.05332 | Validation Loss: 0.05644\n",
      "Epoch: 54 | Train Loss: 0.05286 | Validation Loss: 0.05640\n",
      "Epoch: 55 | Train Loss: 0.05257 | Validation Loss: 0.05608\n",
      "Epoch: 56 | Train Loss: 0.05219 | Validation Loss: 0.05625\n",
      "Epoch: 57 | Train Loss: 0.05200 | Validation Loss: 0.05665\n",
      "Epoch: 58 | Train Loss: 0.05212 | Validation Loss: 0.05650\n",
      "Early stopping at epoch 58 after 3 epochs without improvement.\n",
      "...Training done...\n",
      "Time elapsed for training: 28.915243005752565  minutes\n",
      "=================================================================================\n",
      "Final training R2: [0.90757789 0.93847614 0.9444653  0.80412162 0.97410708 0.91816945\n",
      " 0.97484367 0.96300251 0.9669421  0.98234567 0.98630307 0.9862988\n",
      " 0.98630523 0.98630356 0.97520906]\n",
      "Average final training R2:  0.9529647417586671\n",
      "Final training loss: 0.04450246922470816\n",
      "Final validation R2: [0.883021   0.92412737 0.92784277 0.75023805 0.96789537 0.90335936\n",
      " 0.97153617 0.95658194 0.95718296 0.98122245 0.98121565 0.98119309\n",
      " 0.98120135 0.98121518 0.97040601]\n",
      "Average final validation R2:  0.9412159146589237\n",
      "Final validation loss: 0.056083820545329496\n",
      "=================================================================================\n",
      "Network 12 of 12\n",
      "Number of GAT layers: 7\n",
      "Number of hidden channels: 1024\n",
      "Number of trainable parameters: 7,599,375\n",
      "=================================================================================\n",
      "Epoch: 01 | Train Loss: 0.33354 | Validation Loss: 0.30406\n",
      "Epoch: 02 | Train Loss: 0.27571 | Validation Loss: 0.24697\n",
      "Epoch: 03 | Train Loss: 0.25191 | Validation Loss: 0.25328\n",
      "Epoch: 04 | Train Loss: 0.22766 | Validation Loss: 0.21702\n",
      "Epoch: 05 | Train Loss: 0.21593 | Validation Loss: 0.21004\n",
      "Epoch: 06 | Train Loss: 0.20922 | Validation Loss: 0.19255\n",
      "Epoch: 07 | Train Loss: 0.20195 | Validation Loss: 0.18857\n",
      "Epoch: 08 | Train Loss: 0.21336 | Validation Loss: 0.20639\n",
      "Epoch: 09 | Train Loss: 0.18571 | Validation Loss: 0.16561\n",
      "Epoch: 10 | Train Loss: 0.17141 | Validation Loss: 0.18730\n",
      "Epoch: 11 | Train Loss: 0.17112 | Validation Loss: 0.15766\n",
      "Epoch: 12 | Train Loss: 0.15932 | Validation Loss: 0.14307\n",
      "Epoch: 13 | Train Loss: 0.14467 | Validation Loss: 0.15475\n",
      "Epoch: 14 | Train Loss: 0.14391 | Validation Loss: 0.13796\n",
      "Epoch: 15 | Train Loss: 0.13655 | Validation Loss: 0.11715\n",
      "Epoch: 16 | Train Loss: 0.12831 | Validation Loss: 0.11943\n",
      "Epoch: 17 | Train Loss: 0.12150 | Validation Loss: 0.11430\n",
      "Epoch: 18 | Train Loss: 0.12153 | Validation Loss: 0.10986\n",
      "Epoch: 19 | Train Loss: 0.11346 | Validation Loss: 0.10749\n",
      "Epoch: 20 | Train Loss: 0.10888 | Validation Loss: 0.10097\n",
      "Epoch: 21 | Train Loss: 0.10341 | Validation Loss: 0.09370\n",
      "Epoch: 22 | Train Loss: 0.10146 | Validation Loss: 0.09331\n",
      "Epoch: 23 | Train Loss: 0.09908 | Validation Loss: 0.08969\n",
      "Epoch: 24 | Train Loss: 0.09437 | Validation Loss: 0.08548\n",
      "Epoch: 25 | Train Loss: 0.09194 | Validation Loss: 0.08290\n",
      "Epoch: 26 | Train Loss: 0.08855 | Validation Loss: 0.08457\n",
      "Epoch: 27 | Train Loss: 0.08558 | Validation Loss: 0.08202\n",
      "Epoch: 28 | Train Loss: 0.08620 | Validation Loss: 0.07732\n",
      "Epoch: 29 | Train Loss: 0.08205 | Validation Loss: 0.07597\n",
      "Epoch: 30 | Train Loss: 0.07974 | Validation Loss: 0.07669\n",
      "Epoch: 31 | Train Loss: 0.07837 | Validation Loss: 0.07421\n",
      "Epoch: 32 | Train Loss: 0.07671 | Validation Loss: 0.07569\n",
      "Epoch: 33 | Train Loss: 0.07582 | Validation Loss: 0.07053\n",
      "Epoch: 34 | Train Loss: 0.07317 | Validation Loss: 0.07212\n",
      "Epoch: 35 | Train Loss: 0.07223 | Validation Loss: 0.06907\n",
      "Epoch: 36 | Train Loss: 0.07084 | Validation Loss: 0.06637\n",
      "Epoch: 37 | Train Loss: 0.06939 | Validation Loss: 0.06637\n",
      "Epoch: 38 | Train Loss: 0.06892 | Validation Loss: 0.06591\n",
      "Epoch: 39 | Train Loss: 0.06753 | Validation Loss: 0.06526\n",
      "Epoch: 40 | Train Loss: 0.06663 | Validation Loss: 0.06490\n",
      "Epoch: 41 | Train Loss: 0.06614 | Validation Loss: 0.06452\n",
      "Epoch: 42 | Train Loss: 0.06507 | Validation Loss: 0.06240\n",
      "Epoch: 43 | Train Loss: 0.06446 | Validation Loss: 0.06225\n",
      "Epoch: 44 | Train Loss: 0.06349 | Validation Loss: 0.06293\n",
      "Epoch: 45 | Train Loss: 0.06312 | Validation Loss: 0.06292\n",
      "Epoch: 46 | Train Loss: 0.06222 | Validation Loss: 0.06153\n",
      "Epoch: 47 | Train Loss: 0.06163 | Validation Loss: 0.06108\n",
      "Epoch: 48 | Train Loss: 0.06100 | Validation Loss: 0.06025\n",
      "Epoch: 49 | Train Loss: 0.06061 | Validation Loss: 0.05982\n",
      "Epoch: 50 | Train Loss: 0.05997 | Validation Loss: 0.05961\n",
      "Epoch: 51 | Train Loss: 0.05989 | Validation Loss: 0.06143\n",
      "Epoch: 52 | Train Loss: 0.05866 | Validation Loss: 0.06084\n",
      "Epoch: 53 | Train Loss: 0.05890 | Validation Loss: 0.05863\n",
      "Epoch: 54 | Train Loss: 0.05880 | Validation Loss: 0.05963\n",
      "Epoch: 55 | Train Loss: 0.05753 | Validation Loss: 0.05816\n",
      "Epoch: 56 | Train Loss: 0.05772 | Validation Loss: 0.05916\n",
      "Epoch: 57 | Train Loss: 0.05777 | Validation Loss: 0.05902\n",
      "Epoch: 58 | Train Loss: 0.05716 | Validation Loss: 0.05812\n",
      "Epoch: 59 | Train Loss: 0.05706 | Validation Loss: 0.05804\n",
      "Epoch: 60 | Train Loss: 0.05693 | Validation Loss: 0.05837\n",
      "Epoch: 61 | Train Loss: 0.05625 | Validation Loss: 0.05779\n",
      "Epoch: 62 | Train Loss: 0.05649 | Validation Loss: 0.05885\n",
      "Epoch: 63 | Train Loss: 0.05572 | Validation Loss: 0.05792\n",
      "Epoch: 64 | Train Loss: 0.05558 | Validation Loss: 0.05776\n",
      "Epoch: 65 | Train Loss: 0.05543 | Validation Loss: 0.05728\n",
      "Epoch: 66 | Train Loss: 0.05520 | Validation Loss: 0.05689\n",
      "Epoch: 67 | Train Loss: 0.05482 | Validation Loss: 0.05649\n",
      "Epoch: 68 | Train Loss: 0.05519 | Validation Loss: 0.05675\n",
      "Epoch: 69 | Train Loss: 0.05487 | Validation Loss: 0.05706\n",
      "Epoch: 70 | Train Loss: 0.05470 | Validation Loss: 0.05614\n",
      "Epoch: 71 | Train Loss: 0.05446 | Validation Loss: 0.05724\n",
      "Epoch: 72 | Train Loss: 0.05440 | Validation Loss: 0.05653\n",
      "Epoch: 73 | Train Loss: 0.05421 | Validation Loss: 0.05727\n",
      "Early stopping at epoch 73 after 3 epochs without improvement.\n",
      "...Training done...\n",
      "Time elapsed for training: 554.5915090719859  minutes\n",
      "=================================================================================\n",
      "Final training R2: [0.90290381 0.93557902 0.94155131 0.79609251 0.97215807 0.91704351\n",
      " 0.97479182 0.96305458 0.96631605 0.98199787 0.98488173 0.98488322\n",
      " 0.98488877 0.98487497 0.97351201]\n",
      "Average final training R2:  0.9509686169273717\n",
      "Final training loss: 0.04659369297794562\n",
      "Final validation R2: [0.88301405 0.92455087 0.92549666 0.75100766 0.96725649 0.90512375\n",
      " 0.97199398 0.95759626 0.95670074 0.98130386 0.97989333 0.97990199\n",
      " 0.9799003  0.97989057 0.96907047]\n",
      "Average final validation R2:  0.9408467322046923\n",
      "Final validation loss: 0.05614265169088657\n",
      "\n",
      "Experiment done!\n"
     ]
    }
   ],
   "source": [
    "def train(model, batch):\n",
    "      model.train()\n",
    "      optimizer.zero_grad()  # Clear gradients.\n",
    "      out = model(batch.x, batch.edge_index, batch.edge_attr, batch.batch).to(device)  # Perform a single forward pass.\n",
    "\n",
    "      targets = batch.y\n",
    "      loss = criterion(out, targets) \n",
    "\n",
    "      loss.backward()  # Derive gradients.\n",
    "      optimizer.step()  # Update parameters based on gradients.\n",
    "      \n",
    "      return loss\n",
    "\n",
    "def test(model, data):\n",
    "      all_r2 = []\n",
    "      all_loss = []\n",
    "      counter = -1    \n",
    "      for batch in data:\n",
    "            counter += 1\n",
    "            model.eval()\n",
    "            out = model(batch.x, batch.edge_index, batch.edge_attr, batch.batch).cpu()\n",
    "            targets = batch.y.cpu()\n",
    "            \n",
    "            # Caculate R2\n",
    "            r2_score_var = []\n",
    "            for item in range(target_dim):\n",
    "                  if item == 0:\n",
    "                        r2_score_var = r2_score(targets[:,item].detach().numpy(), out[:,item].detach().numpy())\n",
    "                  else:\n",
    "                        new_score = r2_score(targets[:,item].detach().numpy(), out[:,item].detach().numpy())\n",
    "                        r2_score_var = np.vstack((r2_score_var, new_score))\n",
    "\n",
    "            if counter == 0:\n",
    "                  all_r2 = r2_score_var\n",
    "            else:\n",
    "                  all_r2 = np.hstack((all_r2, r2_score_var))\n",
    "                  \n",
    "            loss = float(criterion(out, targets).detach().numpy())\n",
    "            all_loss = np.hstack((all_loss, loss))\n",
    "\n",
    "      average_test_r2 = np.mean(all_r2, axis=1)\n",
    "      average_test_loss = np.mean(all_loss)\n",
    "      \n",
    "      return average_test_r2, average_test_loss  \n",
    "  \n",
    "def early_stopping(val_losses, patience): # returns True if there is no improvement in val_loss\n",
    "      if len(val_losses) < patience:\n",
    "            return False\n",
    "      else:\n",
    "            best_loss = np.min(val_losses)\n",
    "            current_loss = val_losses[-1]\n",
    "            \n",
    "            if current_loss > best_loss:\n",
    "                  return True\n",
    "            else:\n",
    "                  return False\n",
    "\n",
    "layer_counts = [1, 3, 5, 7]\n",
    "channel_counts = [256, 512, 1024]\n",
    "n_epochs = 100\n",
    "\n",
    "# Run experiment\n",
    "feature_dim = train_data[0].x.shape[1]\n",
    "target_dim = train_data[0].y.shape[1]\n",
    "\n",
    "loss_train_res = np.zeros((len(layer_counts), len(channel_counts)))\n",
    "loss_val_res = np.zeros((len(layer_counts), len(channel_counts)))\n",
    "avg_r2_train = np.zeros((len(layer_counts), len(channel_counts)))\n",
    "avg_r2_val = np.zeros((len(layer_counts), len(channel_counts)))\n",
    "\n",
    "network_counter = 0\n",
    "for i, num_layers in enumerate(layer_counts):\n",
    "    for j, hidden_channels in enumerate(channel_counts):\n",
    "        network_counter += 1    \n",
    "        train_start = time.time()\n",
    "        \n",
    "        model_class = GNNs.define_GNN_structure(num_layers, hidden_channels, feature_dim, target_dim)\n",
    "        model = model_class().to(device)\n",
    "        \n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "        criterion = torch.nn.MSELoss().to(device)\n",
    "        \n",
    "        decay_rate = 0.94\n",
    "        lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=decay_rate)\n",
    "        \n",
    "        print('=================================================================================')\n",
    "        print(f\"Network {network_counter} of {len(layer_counts)*len(channel_counts)}\")\n",
    "        print(f'Number of GAT layers: {num_layers}')\n",
    "        print(f'Number of hidden channels: {hidden_channels}')\n",
    "        trainable_params, _ = uf.get_num_parameters(model)\n",
    "        print(f\"Number of trainable parameters: {trainable_params:,}\")\n",
    "        print('=================================================================================')\n",
    "\n",
    "        val_losses_epoch = []\n",
    "        epochs_without_improvement = 0\n",
    "        best_val_loss = np.inf\n",
    "        patience = 3\n",
    "        for epoch in np.arange(1, n_epochs+1):\n",
    "            losses = []\n",
    "            for batch in train_loader:\n",
    "                loss = train(model, batch)\n",
    "                losses.append(loss.cpu().detach().numpy())  \n",
    "            # Compute validation loss\n",
    "            model.eval()\n",
    "            val_losses = []\n",
    "            for batch in val_loader:          \n",
    "                targets = batch.y\n",
    "                out = model(batch.x, batch.edge_index, batch.edge_attr, batch.batch)\n",
    "                val_losses.append(criterion(out, targets).cpu().detach().numpy()) \n",
    "            \n",
    "            val_loss_epoch = np.mean(val_losses)\n",
    "            val_losses_epoch.append(val_loss_epoch)\n",
    "            print(f\"Epoch: {epoch:02d} | Train Loss: {np.mean(losses):.5f} | Validation Loss: {val_loss_epoch:.5f}\")\n",
    "            train_end = time.time()\n",
    "        \n",
    "            lr_scheduler.step() # Decay to learning rate\n",
    "            \n",
    "            # check for early stopping\n",
    "            if early_stopping(val_losses_epoch, patience) and epoch > patience:\n",
    "                    epochs_without_improvement += 1\n",
    "                    if epochs_without_improvement >= patience:\n",
    "                        print(f\"Early stopping at epoch {epoch} after {patience} epochs without improvement.\")\n",
    "                        break\n",
    "            else:\n",
    "                    epochs_without_improvement = 0\n",
    "                    best_val_loss = val_loss_epoch\n",
    "                    # save the model\n",
    "                    torch.save(model.state_dict(), 'best_temp_model.pt')\n",
    "                    \n",
    "        print(\"...Training done...\")\n",
    "        print(\"Time elapsed for training:\", (train_end - train_start)/60, \" minutes\")\n",
    "        model.load_state_dict(torch.load('best_temp_model.pt'))\n",
    "        r2_train, loss_train = test(model, train_loader) \n",
    "        loss_train_res[i, j] = loss_train\n",
    "        avg_r2_train[i, j] = np.mean(r2_train)\n",
    "        \n",
    "        r2_val, loss_val = test(model, val_loader)\n",
    "        loss_val_res[i, j] = loss_val\n",
    "        avg_r2_val[i, j] = np.mean(r2_val)\n",
    "\n",
    "        print('=================================================================================')\n",
    "        print(\"Final training R2:\", r2_train)\n",
    "        print(\"Average final training R2: \", np.mean(r2_train))\n",
    "        print(\"Final training loss:\", loss_train)\n",
    "\n",
    "        print(\"Final validation R2:\", r2_val)\n",
    "        print(\"Average final validation R2: \", np.mean(r2_val))\n",
    "        print(\"Final validation loss:\", loss_val)\n",
    "print()\n",
    "print(\"Experiment done!\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Saving results...\n"
     ]
    }
   ],
   "source": [
    "print(\"...Saving results...\")\n",
    "df_loss_train = pd.DataFrame(loss_train_res, index=layer_counts, columns=channel_counts)\n",
    "df_loss_val = pd.DataFrame(loss_val_res, index=layer_counts, columns=channel_counts)\n",
    "df_avg_r2_train = pd.DataFrame(avg_r2_train, index=layer_counts, columns=channel_counts)\n",
    "df_avg_r2_val = pd.DataFrame(avg_r2_val, index=layer_counts, columns=channel_counts)\n",
    "\n",
    "df_loss_train.to_csv('experiment_results/loss_train.csv')\n",
    "df_loss_val.to_csv('experiment_results/loss_val.csv')\n",
    "df_avg_r2_train.to_csv('experiment_results/avg_r2_train.csv')\n",
    "df_avg_r2_val.to_csv('experiment_results/avg_r2_val.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:\n",
      "       256       512       1024\n",
      "1  0.109000  0.114687  0.092053\n",
      "3  0.061234  0.051048  0.037577\n",
      "5  0.058765  0.047773  0.040866\n",
      "7  0.069952  0.044502  0.046594\n"
     ]
    }
   ],
   "source": [
    "print(\"Training loss:\")\n",
    "print(df_loss_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training R2:\n",
      "       256       512       1024\n",
      "1  0.886772  0.880900  0.903675\n",
      "3  0.935892  0.946446  0.960578\n",
      "5  0.938245  0.949765  0.957036\n",
      "7  0.926317  0.952965  0.950969\n"
     ]
    }
   ],
   "source": [
    "print(\"Average training R2:\")\n",
    "print(df_avg_r2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss:\n",
      "       256       512       1024\n",
      "1  0.113837  0.119683  0.097134\n",
      "3  0.068668  0.059405  0.048517\n",
      "5  0.067369  0.057372  0.049545\n",
      "7  0.077003  0.056084  0.056143\n"
     ]
    }
   ],
   "source": [
    "print(\"Validation loss:\")\n",
    "print(df_loss_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average test R2:\n",
      "       256       512       1024\n",
      "1  0.881948  0.876203  0.898806\n",
      "3  0.927869  0.937633  0.949127\n",
      "5  0.929266  0.939805  0.948007\n",
      "7  0.919293  0.941216  0.940847\n"
     ]
    }
   ],
   "source": [
    "print(\"Average validation R2:\")\n",
    "print(df_avg_r2_val)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Neural Network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model for all targets at once"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 5,489,935\n",
      "\n",
      "...Starting training...\n",
      "Device used: cuda\n",
      "Epoch: 01 | Train Loss: 0.30232 | Validation Loss: 0.25115\n",
      "Epoch: 02 | Train Loss: 0.23202 | Validation Loss: 0.21343\n",
      "Epoch: 03 | Train Loss: 0.20464 | Validation Loss: 0.18316\n",
      "Epoch: 04 | Train Loss: 0.18135 | Validation Loss: 0.16385\n",
      "Epoch: 05 | Train Loss: 0.16348 | Validation Loss: 0.14297\n",
      "Epoch: 06 | Train Loss: 0.14979 | Validation Loss: 0.13316\n",
      "Epoch: 07 | Train Loss: 0.13735 | Validation Loss: 0.12096\n",
      "Epoch: 08 | Train Loss: 0.12913 | Validation Loss: 0.11523\n",
      "Epoch: 09 | Train Loss: 0.12115 | Validation Loss: 0.10875\n",
      "Epoch: 10 | Train Loss: 0.11224 | Validation Loss: 0.09823\n",
      "Epoch: 11 | Train Loss: 0.10771 | Validation Loss: 0.09409\n",
      "Epoch: 12 | Train Loss: 0.10207 | Validation Loss: 0.08912\n",
      "Epoch: 13 | Train Loss: 0.09671 | Validation Loss: 0.08178\n",
      "Epoch: 14 | Train Loss: 0.09317 | Validation Loss: 0.08296\n",
      "Epoch: 15 | Train Loss: 0.08834 | Validation Loss: 0.07752\n",
      "Epoch: 16 | Train Loss: 0.08534 | Validation Loss: 0.07607\n",
      "Epoch: 17 | Train Loss: 0.08257 | Validation Loss: 0.07021\n",
      "Epoch: 18 | Train Loss: 0.08057 | Validation Loss: 0.06977\n",
      "Epoch: 19 | Train Loss: 0.07736 | Validation Loss: 0.06990\n",
      "Epoch: 20 | Train Loss: 0.07496 | Validation Loss: 0.07021\n",
      "Epoch: 21 | Train Loss: 0.07285 | Validation Loss: 0.06288\n",
      "Epoch: 22 | Train Loss: 0.07094 | Validation Loss: 0.06313\n",
      "Epoch: 23 | Train Loss: 0.06854 | Validation Loss: 0.06096\n",
      "Epoch: 24 | Train Loss: 0.06779 | Validation Loss: 0.06083\n",
      "Epoch: 25 | Train Loss: 0.06616 | Validation Loss: 0.06027\n",
      "Epoch: 26 | Train Loss: 0.06394 | Validation Loss: 0.05866\n",
      "Epoch: 27 | Train Loss: 0.06318 | Validation Loss: 0.05816\n",
      "Epoch: 28 | Train Loss: 0.06122 | Validation Loss: 0.05687\n",
      "Epoch: 29 | Train Loss: 0.06099 | Validation Loss: 0.05500\n",
      "Epoch: 30 | Train Loss: 0.05921 | Validation Loss: 0.05589\n",
      "Epoch: 31 | Train Loss: 0.05869 | Validation Loss: 0.05448\n",
      "Epoch: 32 | Train Loss: 0.05758 | Validation Loss: 0.05416\n",
      "Epoch: 33 | Train Loss: 0.05644 | Validation Loss: 0.05293\n",
      "Epoch: 34 | Train Loss: 0.05594 | Validation Loss: 0.05212\n",
      "Epoch: 35 | Train Loss: 0.05487 | Validation Loss: 0.05104\n",
      "Epoch: 36 | Train Loss: 0.05416 | Validation Loss: 0.05173\n",
      "Epoch: 37 | Train Loss: 0.05336 | Validation Loss: 0.05259\n",
      "Epoch: 38 | Train Loss: 0.05303 | Validation Loss: 0.05079\n",
      "Epoch: 39 | Train Loss: 0.05189 | Validation Loss: 0.04993\n",
      "Epoch: 40 | Train Loss: 0.05176 | Validation Loss: 0.04904\n",
      "Epoch: 41 | Train Loss: 0.05115 | Validation Loss: 0.05027\n",
      "Epoch: 42 | Train Loss: 0.05043 | Validation Loss: 0.04843\n",
      "Epoch: 43 | Train Loss: 0.04998 | Validation Loss: 0.04871\n",
      "Epoch: 44 | Train Loss: 0.04937 | Validation Loss: 0.04817\n",
      "Epoch: 45 | Train Loss: 0.04891 | Validation Loss: 0.04815\n",
      "Epoch: 46 | Train Loss: 0.04853 | Validation Loss: 0.04795\n",
      "Epoch: 47 | Train Loss: 0.04827 | Validation Loss: 0.04740\n",
      "Epoch: 48 | Train Loss: 0.04813 | Validation Loss: 0.04743\n",
      "Epoch: 49 | Train Loss: 0.04767 | Validation Loss: 0.04741\n",
      "Epoch: 50 | Train Loss: 0.04747 | Validation Loss: 0.04835\n",
      "Epoch: 51 | Train Loss: 0.04695 | Validation Loss: 0.04684\n",
      "Epoch: 52 | Train Loss: 0.04679 | Validation Loss: 0.04710\n",
      "Epoch: 53 | Train Loss: 0.04659 | Validation Loss: 0.04673\n",
      "Epoch: 54 | Train Loss: 0.04632 | Validation Loss: 0.04654\n",
      "Epoch: 55 | Train Loss: 0.04596 | Validation Loss: 0.04675\n",
      "Epoch: 56 | Train Loss: 0.04565 | Validation Loss: 0.04633\n",
      "Epoch: 57 | Train Loss: 0.04550 | Validation Loss: 0.04634\n",
      "Epoch: 58 | Train Loss: 0.04542 | Validation Loss: 0.04691\n",
      "Epoch: 59 | Train Loss: 0.04524 | Validation Loss: 0.04661\n",
      "Epoch: 60 | Train Loss: 0.04497 | Validation Loss: 0.04603\n",
      "Epoch: 61 | Train Loss: 0.04493 | Validation Loss: 0.04712\n",
      "Epoch: 62 | Train Loss: 0.04487 | Validation Loss: 0.04599\n",
      "Epoch: 63 | Train Loss: 0.04458 | Validation Loss: 0.04588\n",
      "Epoch: 64 | Train Loss: 0.04445 | Validation Loss: 0.04663\n",
      "Epoch: 65 | Train Loss: 0.04445 | Validation Loss: 0.04617\n",
      "Epoch: 66 | Train Loss: 0.04432 | Validation Loss: 0.04595\n",
      "Epoch: 67 | Train Loss: 0.04421 | Validation Loss: 0.04655\n",
      "Epoch: 68 | Train Loss: 0.04401 | Validation Loss: 0.04601\n",
      "Early stopping at epoch 68 after 5 epochs without improvement.\n",
      "...Training done...\n",
      "...Calculating final results...\n",
      "====================================================\n",
      "Final training R2: [0.92640928 0.95158271 0.95553146 0.83859656 0.97872722 0.93130514\n",
      " 0.97957066 0.97040701 0.97334086 0.98694306 0.98844278 0.98842935\n",
      " 0.98846725 0.98847078 0.98055473]\n",
      "Average final training R2:  0.9617852565855253\n",
      "Final training loss: 0.03621387270900708\n",
      "Final validation R2: [0.90881243 0.94275313 0.94695014 0.76004791 0.97487781 0.91910367\n",
      " 0.97649867 0.96488863 0.96717272 0.98548883 0.98636119 0.98635836\n",
      " 0.98643627 0.98642457 0.97849677]\n",
      "Average validation R2:  0.9513780730357984\n",
      "Final validation loss: 0.04587508086115122\n",
      "Final test R2: [0.90130915 0.93815061 0.94154153 0.77061965 0.97370292 0.91576174\n",
      " 0.97576228 0.96251719 0.9646199  0.98594863 0.98462893 0.98462761\n",
      " 0.98463931 0.98467542 0.97673216]\n",
      "Average final test R2:  0.9496824679426369\n",
      "Final test loss: 0.048079499503215536\n"
     ]
    }
   ],
   "source": [
    "def train(model, batch):\n",
    "      model.train()\n",
    "      optimizer.zero_grad()  # Clear gradients.\n",
    "      out = model(batch.x, batch.edge_index, batch.edge_attr, batch.batch).to(device)  # Perform a single forward pass.\n",
    "\n",
    "      targets = batch.y\n",
    "      loss = criterion(out, targets) \n",
    "\n",
    "      loss.backward()  # Derive gradients.\n",
    "      optimizer.step()  # Update parameters based on gradients.\n",
    "      return loss\n",
    "\n",
    "def test(model, batch):\n",
    "      all_r2 = []\n",
    "      all_loss = []\n",
    "      counter = -1    \n",
    "      for i, batch in enumerate(batch):\n",
    "            counter += 1\n",
    "            model.eval()\n",
    "            out = model(batch.x, batch.edge_index, batch.edge_attr, batch.batch).cpu()\n",
    "            targets = batch.y.cpu()\n",
    "            \n",
    "            # Caculate R2 for each target\n",
    "            for target_idx in range(target_dim):\n",
    "                  if target_idx != 0:\n",
    "                        r2_score_var = np.vstack((r2_score_var, r2_score(targets[:,target_idx].detach().numpy(), \n",
    "                                                          out[:,target_idx].detach().numpy())))\n",
    "                  else:\n",
    "                        r2_score_var = np.array([r2_score(targets[:,target_idx].detach().numpy(),\n",
    "                                                          out[:,target_idx].detach().numpy())])        \n",
    "            all_r2 = np.hstack((all_r2, r2_score_var)) if i != 0 else r2_score_var\n",
    "                  \n",
    "            loss = float(criterion(out, targets).detach().numpy())\n",
    "            all_loss = np.hstack((all_loss, loss)) if i != 0 else np.array(loss)\n",
    "\n",
    "      average_test_r2 = np.mean(all_r2, axis=1)\n",
    "      average_test_loss = np.mean(all_loss)\n",
    "      \n",
    "      return average_test_r2, average_test_loss\n",
    "\n",
    "def early_stopping(val_losses, patience): # returns True if there is no improvement in val_loss\n",
    "      if len(val_losses) < patience:\n",
    "            return False\n",
    "      else:\n",
    "            best_loss = np.min(val_losses)\n",
    "            current_loss = val_losses[-1]\n",
    "            \n",
    "            if current_loss > best_loss:\n",
    "                  return True\n",
    "            else:\n",
    "                  return False\n",
    "\n",
    "feature_dim = train_data[0].x.shape[1]\n",
    "target_dim = train_data[0].y.shape[1]\n",
    "\n",
    "num_layers, hidden_channels = 5, 1024\n",
    "model_class = GNNs.define_GNN_structure(num_layers, hidden_channels, feature_dim, target_dim)\n",
    "model = model_class().to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "criterion = torch.nn.MSELoss().to(device)\n",
    "\n",
    "decay_rate = 0.94\n",
    "lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=decay_rate)\n",
    "\n",
    "train_params, _ = uf.get_num_parameters(model)\n",
    "print(f\"Trainable parameters: {train_params:,}\")\n",
    "      \n",
    "# Vectors to append accuracy to:\n",
    "train_r2 = []\n",
    "train_loss = []\n",
    "test_r2 = []\n",
    "test_loss = []\n",
    "val_r2 = []\n",
    "val_loss = []\n",
    "\n",
    "n_epochs = 100\n",
    "print_every_N_epochs = False\n",
    "N = 10 # print R2 every N epochs\n",
    "\n",
    "epoch_times = []\n",
    "train_times = []\n",
    "print()\n",
    "print(\"...Starting training...\")\n",
    "print(\"Device used:\", device)\n",
    "\n",
    "val_losses_epoch = [] # for early stopping\n",
    "patience = 5 # how many epochs to wait for the val loss to improve\n",
    "best_val_loss = np.inf  \n",
    "epochs_without_improvement = 0\n",
    "\n",
    "for epoch in np.arange(1, n_epochs+1):\n",
    "      epoch_start = time.time()\n",
    "      losses = []\n",
    "      train_start = time.time()\n",
    "      for batch in train_loader:\n",
    "            loss = train(model, batch)\n",
    "            losses.append(loss.cpu().detach().numpy())  \n",
    "      # Compute validation loss\n",
    "      model.eval()\n",
    "      val_losses = []\n",
    "      for batch in test_loader:\n",
    "            targets = batch.y\n",
    "            out = model(batch.x, batch.edge_index, batch.edge_attr, batch.batch)\n",
    "            val_losses.append(criterion(out, targets).cpu().detach().numpy())\n",
    "      \n",
    "      val_loss_epoch = np.mean(val_losses)\n",
    "      val_losses_epoch.append(val_loss_epoch)\n",
    "      print(f\"Epoch: {epoch:02d} | Train Loss: {np.mean(losses):.5f} | Validation Loss: {val_loss_epoch:.5f}\")\n",
    "      train_end = time.time()\n",
    "      train_times.append(train_end - train_start)\n",
    "      lr_scheduler.step() # Decay to learning rate\n",
    "      \n",
    "      # check for early stopping\n",
    "      if early_stopping(val_losses_epoch, patience) and epoch > patience:\n",
    "            epochs_without_improvement += 1\n",
    "            if epochs_without_improvement >= patience:\n",
    "                  print(f\"Early stopping at epoch {epoch} after {patience} epochs without improvement.\")\n",
    "                  break\n",
    "      else:\n",
    "            epochs_without_improvement = 0\n",
    "            best_val_loss = val_loss_epoch\n",
    "            # save the model\n",
    "            torch.save(model.state_dict(), 'best_graphs_only_model.pt')\n",
    "                   \n",
    "      if print_every_N_epochs and (epoch % N == 0 or epoch == 1) and epoch != n_epochs:\n",
    "            test_start = time.time()\n",
    "                \n",
    "            r2_temp_val, loss_temp_val = test(model, test_loader)\n",
    "            val_r2.append(r2_temp_val)\n",
    "            val_loss.append(loss_temp_val)\n",
    "            \n",
    "            print(f\"Validation loss: {loss_temp_val}\")\n",
    "            print(f'Validation R2: {r2_temp_val}')\n",
    "            print(f\"Average validation R2: {np.mean(r2_temp_val):.5f}\")\n",
    "            \n",
    "      epoch_end = time.time()\n",
    "      epoch_times.append(epoch_end - epoch_start)\n",
    "      \n",
    "print(\"...Training done...\")\n",
    "print(\"...Calculating final results...\")\n",
    "model.load_state_dict(torch.load('best_graphs_only_model.pt'))\n",
    "\n",
    "r2_temp_train, loss_temp_train = test(model, train_loader) \n",
    "train_r2.append(r2_temp_train)\n",
    "train_loss.append(loss_temp_train)\n",
    "\n",
    "r2_temp_val, loss_temp_val = test(model, test_loader)\n",
    "val_r2.append(r2_temp_val)\n",
    "val_loss.append(loss_temp_val)\n",
    "\n",
    "r2_temp_test, loss_temp_test = test(model, val_loader)\n",
    "test_r2.append(r2_temp_test)\n",
    "test_loss.append(loss_temp_test)\n",
    "\n",
    "print(\"====================================================\")\n",
    "print(\"Final training R2:\", train_r2[-1])\n",
    "print(\"Average final training R2: \", np.mean(train_r2[-1]))\n",
    "print(\"Final training loss:\", train_loss[-1])\n",
    "\n",
    "print(\"Final validation R2:\", val_r2[-1])\n",
    "print(\"Average validation R2: \", np.mean(val_r2[-1]))\n",
    "print(\"Final validation loss:\", val_loss[-1])\n",
    "\n",
    "print(\"Final test R2:\", test_r2[-1])\n",
    "print(\"Average final test R2: \", np.mean(test_r2[-1]))\n",
    "print(\"Final test loss:\", test_loss[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mDevice used:\u001b[39m\u001b[39m\"\u001b[39m, device)\n\u001b[0;32m      2\u001b[0m \u001b[39mprint\u001b[39m()\n\u001b[0;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTotal number of epochs: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(epoch_times)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'device' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Device used:\", device)\n",
    "print()\n",
    "print(f\"Total number of epochs: {len(epoch_times)}\")\n",
    "print(f\"Total training time: {np.sum(epoch_times)/60:.2f} minutes\")\n",
    "print(f\"Total time in training: {np.sum(train_times)/60:.2f} minutes\")\n",
    "print()\n",
    "print(f\"Average epoch time: {np.mean(epoch_times):.1f} seconds\")\n",
    "print(f\"Average time in training: {np.mean(train_times):.1f} seconds\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model for just one target"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GNN function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target index: 0\n",
      "Initial training R2:  -0.006660032108331978\n",
      "Initial test R2:  -0.00475941961664909\n",
      "Epoch: 001, Loss: 0.68831\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 69\u001b[0m\n\u001b[0;32m     67\u001b[0m counter \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     68\u001b[0m losses \u001b[39m=\u001b[39m []\n\u001b[1;32m---> 69\u001b[0m \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m train_loader:\n\u001b[0;32m     70\u001b[0m       loss \u001b[39m=\u001b[39m train(data, target_index)\n\u001b[0;32m     71\u001b[0m       losses\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy())\n",
      "File \u001b[1;32mc:\\Users\\jespe\\anaconda3\\envs\\proj_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\jespe\\anaconda3\\envs\\proj_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    677\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 678\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    679\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    680\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\jespe\\anaconda3\\envs\\proj_env\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[1;32mc:\\Users\\jespe\\anaconda3\\envs\\proj_env\\lib\\site-packages\\torch_geometric\\loader\\dataloader.py:20\u001b[0m, in \u001b[0;36mCollater.__call__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m     18\u001b[0m elem \u001b[39m=\u001b[39m batch[\u001b[39m0\u001b[39m]\n\u001b[0;32m     19\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, BaseData):\n\u001b[1;32m---> 20\u001b[0m     \u001b[39mreturn\u001b[39;00m Batch\u001b[39m.\u001b[39;49mfrom_data_list(batch, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfollow_batch,\n\u001b[0;32m     21\u001b[0m                                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexclude_keys)\n\u001b[0;32m     22\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m     23\u001b[0m     \u001b[39mreturn\u001b[39;00m default_collate(batch)\n",
      "File \u001b[1;32mc:\\Users\\jespe\\anaconda3\\envs\\proj_env\\lib\\site-packages\\torch_geometric\\data\\batch.py:76\u001b[0m, in \u001b[0;36mBatch.from_data_list\u001b[1;34m(cls, data_list, follow_batch, exclude_keys)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m     65\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfrom_data_list\u001b[39m(\u001b[39mcls\u001b[39m, data_list: List[BaseData],\n\u001b[0;32m     66\u001b[0m                    follow_batch: Optional[List[\u001b[39mstr\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m     67\u001b[0m                    exclude_keys: Optional[List[\u001b[39mstr\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m     68\u001b[0m     \u001b[39mr\u001b[39m\u001b[39m\"\"\"Constructs a :class:`~torch_geometric.data.Batch` object from a\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[39m    Python list of :class:`~torch_geometric.data.Data` or\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[39m    :class:`~torch_geometric.data.HeteroData` objects.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[39m    :obj:`follow_batch`.\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[39m    Will exclude any keys given in :obj:`exclude_keys`.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 76\u001b[0m     batch, slice_dict, inc_dict \u001b[39m=\u001b[39m collate(\n\u001b[0;32m     77\u001b[0m         \u001b[39mcls\u001b[39;49m,\n\u001b[0;32m     78\u001b[0m         data_list\u001b[39m=\u001b[39;49mdata_list,\n\u001b[0;32m     79\u001b[0m         increment\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m     80\u001b[0m         add_batch\u001b[39m=\u001b[39;49m\u001b[39mnot\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(data_list[\u001b[39m0\u001b[39;49m], Batch),\n\u001b[0;32m     81\u001b[0m         follow_batch\u001b[39m=\u001b[39;49mfollow_batch,\n\u001b[0;32m     82\u001b[0m         exclude_keys\u001b[39m=\u001b[39;49mexclude_keys,\n\u001b[0;32m     83\u001b[0m     )\n\u001b[0;32m     85\u001b[0m     batch\u001b[39m.\u001b[39m_num_graphs \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(data_list)\n\u001b[0;32m     86\u001b[0m     batch\u001b[39m.\u001b[39m_slice_dict \u001b[39m=\u001b[39m slice_dict\n",
      "File \u001b[1;32mc:\\Users\\jespe\\anaconda3\\envs\\proj_env\\lib\\site-packages\\torch_geometric\\data\\collate.py:108\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(cls, data_list, increment, add_batch, follow_batch, exclude_keys)\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[39mif\u001b[39;00m (add_batch \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(stores[\u001b[39m0\u001b[39m], NodeStorage)\n\u001b[0;32m    106\u001b[0m             \u001b[39mand\u001b[39;00m stores[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mcan_infer_num_nodes):\n\u001b[0;32m    107\u001b[0m         repeats \u001b[39m=\u001b[39m [store\u001b[39m.\u001b[39mnum_nodes \u001b[39mfor\u001b[39;00m store \u001b[39min\u001b[39;00m stores]\n\u001b[1;32m--> 108\u001b[0m         out_store\u001b[39m.\u001b[39mbatch \u001b[39m=\u001b[39m repeat_interleave(repeats, device\u001b[39m=\u001b[39;49mdevice)\n\u001b[0;32m    109\u001b[0m         out_store\u001b[39m.\u001b[39mptr \u001b[39m=\u001b[39m cumsum(torch\u001b[39m.\u001b[39mtensor(repeats, device\u001b[39m=\u001b[39mdevice))\n\u001b[0;32m    111\u001b[0m \u001b[39mreturn\u001b[39;00m out, slice_dict, inc_dict\n",
      "File \u001b[1;32mc:\\Users\\jespe\\anaconda3\\envs\\proj_env\\lib\\site-packages\\torch_geometric\\data\\collate.py:247\u001b[0m, in \u001b[0;36mrepeat_interleave\u001b[1;34m(repeats, device)\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrepeat_interleave\u001b[39m(\n\u001b[0;32m    244\u001b[0m     repeats: List[\u001b[39mint\u001b[39m],\n\u001b[0;32m    245\u001b[0m     device: Optional[torch\u001b[39m.\u001b[39mdevice] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    246\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 247\u001b[0m     outs \u001b[39m=\u001b[39m [torch\u001b[39m.\u001b[39mfull((n, ), i, device\u001b[39m=\u001b[39mdevice) \u001b[39mfor\u001b[39;00m i, n \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(repeats)]\n\u001b[0;32m    248\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcat(outs, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\jespe\\anaconda3\\envs\\proj_env\\lib\\site-packages\\torch_geometric\\data\\collate.py:247\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrepeat_interleave\u001b[39m(\n\u001b[0;32m    244\u001b[0m     repeats: List[\u001b[39mint\u001b[39m],\n\u001b[0;32m    245\u001b[0m     device: Optional[torch\u001b[39m.\u001b[39mdevice] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    246\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 247\u001b[0m     outs \u001b[39m=\u001b[39m [torch\u001b[39m.\u001b[39;49mfull((n, ), i, device\u001b[39m=\u001b[39;49mdevice) \u001b[39mfor\u001b[39;00m i, n \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(repeats)]\n\u001b[0;32m    248\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcat(outs, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "feature_dim = train_data[0].x.shape[1]\n",
    "target_dim = 1\n",
    "\n",
    "def train(data_in, target):\n",
    "      model.train()\n",
    "      optimizer.zero_grad()  # Clear gradients.\n",
    "      out = model(data_in.x, data_in.edge_index, data_in.edge_attr, data_in.batch).to(device)\n",
    "      targets = data_in.y[:,target].reshape(-1,1)\n",
    "      \n",
    "      loss = criterion(out, targets)   \n",
    "\n",
    "      loss.backward()  # Derive gradients.\n",
    "      optimizer.step()  # Update parameters based on gradients.\n",
    "      return loss\n",
    "\n",
    "def test(data, target):\n",
    "      all_r2 = []\n",
    "      all_loss = []\n",
    "      counter = -1    \n",
    "      for data_in in data:\n",
    "            counter += 1\n",
    "            model.eval()\n",
    "            out = model(data_in.x, data_in.edge_index, data_in.edge_attr, data_in.batch).cpu()\n",
    "            targets = data_in.y[:,target].cpu().reshape(-1,1)\n",
    "            \n",
    "            # Caculate R2\n",
    "            r2_score_var = r2_score(targets.detach().numpy(), out.detach().numpy())\n",
    "            all_r2.append(r2_score_var)\n",
    "            \n",
    "            loss = float(criterion(out, targets).detach().numpy())\n",
    "            all_loss.append(loss)\n",
    "\n",
    "      average_test_r2 = np.mean(all_r2)\n",
    "      average_test_loss = np.mean(all_loss)\n",
    "\n",
    "      return average_test_r2, average_test_loss\n",
    "\n",
    "num_targets = train_data[0].y.shape[1]\n",
    "start_time = time.time()\n",
    "for target_index in range(num_targets):\n",
    "      print(\"Target index:\", target_index)\n",
    "\n",
    "      model = GNN(hidden_channels=64, feature_dim=feature_dim, target_dim=target_dim).to(device) \n",
    "      optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=5e-4)\n",
    "      criterion = torch.nn.MSELoss().to(device)\n",
    "\n",
    "      # Vectors to append accuracy to:\n",
    "      train_r2 = []\n",
    "      test_r2 = []\n",
    "      train_loss = []\n",
    "      test_loss = []\n",
    "\n",
    "      # Calculate accuracy and loss before training \n",
    "      r2_temp, loss_temp = test(train_loader, target_index)\n",
    "      train_r2.append(r2_temp)\n",
    "      train_loss.append(loss_temp)\n",
    "      r2_temp, loss_temp = test(test_loader, target_index)\n",
    "      test_r2.append(r2_temp)\n",
    "      test_loss.append(loss_temp)\n",
    "      \n",
    "      print(\"Initial training R2: \", train_r2[0])\n",
    "      print(\"Initial test R2: \", test_r2[0])\n",
    "\n",
    "      print_r2_option = True\n",
    "      counter = 0\n",
    "      for epoch in range(1, 21):\n",
    "            counter += 1\n",
    "            losses = []\n",
    "            for data in train_loader:\n",
    "                  loss = train(data, target_index)\n",
    "                  losses.append(loss.cpu().detach().numpy())\n",
    "            print(f'Epoch: {epoch:03d}, Loss: {np.mean(losses):.5f}')\n",
    "\n",
    "            if print_r2_option & epoch == 20:\n",
    "                  temp_train_r2, temp_train_loss = test(train_loader, target_index)\n",
    "                  train_r2.append(temp_train_r2)\n",
    "                  train_loss.append(temp_train_loss)\n",
    "\n",
    "                  temp_test_r2, temp_test_loss = test(test_loader, target_index)\n",
    "                  test_r2.append(temp_test_r2)\n",
    "                  test_loss.append(temp_test_loss)\n",
    "\n",
    "      print(f\"Best training R2 for target {target_index}: {np.max(train_r2)}\")\n",
    "      print(f\"Best test R2 for target {target_index}: {np.max(test_r2)}\")\n",
    "print(\"...Done...\")\n",
    "end_time = time.time()\n",
    "print(f\"Time taken: {(end_time - start_time)/60} minutes\")\n",
    "print(f\"Average time per target: {(end_time - start_time)/(num_targets*60)} minutes\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
