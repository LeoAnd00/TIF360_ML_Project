{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TIF360 Project"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main source: https://www.kaggle.com/code/rmonge/predicting-molecule-properties-based-on-its-smiles/notebook"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time as time\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import random_split\n",
    "from torch.nn import Linear, BatchNorm1d as BatchNorm\n",
    "from torch_geometric.nn import global_mean_pool, GATConv, BatchNorm, GraphNorm\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "import utility_functions as uf\n",
    "import GNN_structures as GNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available: True\n",
      "device: cuda\n",
      "NVIDIA GeForce RTX 3080\n"
     ]
    }
   ],
   "source": [
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"device:\", \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load smiles data and targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(132820, 21)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/smiles_and_targets.csv\")\n",
    "print(np.shape(df))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the graph dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph_dataset_functions import create_graph_dataset_from_smiles\n",
    "\n",
    "properties_names = ['A', 'B', 'C', 'mu', 'alfa', 'homo', 'lumo', 'gap', 'R²', 'zpve', 'U0', 'U', 'H', 'G', 'Cv']\n",
    "\n",
    "x_smiles = df.smiles.values\n",
    "y = df.loc[:, properties_names].values  # shape = (n_samples, n_properties)\n",
    "\n",
    "dataset = create_graph_dataset_from_smiles(x_smiles, y[0:len(x_smiles), :])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information of the graph dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of graphs (molecules): 132820\n",
      "=================================================================================\n",
      "Properties of graph 50, molecule smiles: CC1=CNC=C1\n",
      "Number of nodes: 6\n",
      "Number of edges: 12\n",
      "Number of node features: 78\n",
      "Number of edge features: 10\n",
      "Number of target properties: 15\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of graphs (molecules): {len(dataset)}')\n",
    "graph = dataset[50]\n",
    "print('=================================================================================')\n",
    "print(f'Properties of graph {50}, molecule smiles: {df.smiles.values[50]}')\n",
    "print(f'Number of nodes: {dataset[50].x.shape[0]}')\n",
    "print(f'Number of edges: {dataset[50].edge_index.shape[1]}')\n",
    "print(f'Number of node features: {dataset[50].x.shape[1]}')\n",
    "print(f'Number of edge features: {dataset[50].edge_attr.shape[1]}')\n",
    "print(f'Number of target properties: {dataset[50].y.shape[1]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose mode: Which features to include"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# singles\n",
    "# mode = \"rdkit_descriptors\"\n",
    "# mode = \"morgan_fingerprints\"\n",
    "mode = \"mordred_descriptors\"\n",
    "\n",
    "# combinations\n",
    "# mode = \"rdkit+morgan\"\n",
    "# mode = \"mordred+morgan\"\n",
    "\n",
    "# excessive\n",
    "# mode = \"rdkit+mordred\"\n",
    "# mode = \"rdkit+mordred+morgan\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features: (132820, 985)\n"
     ]
    }
   ],
   "source": [
    "features = uf.load_molecular_features(mode)\n",
    "print(\"features:\", np.shape(features))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create functions to load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_scale_and_split_dataset(features:np.ndarray, targets:np.ndarray, val_share, test_share):\n",
    "    \n",
    "    num_samples = features.shape[0]\n",
    "    print(\"num_samples:\", num_samples)\n",
    "    train_indices, val_indices, test_indices = uf.get_data_split_indices(num_samples, val_share, test_share)\n",
    "    \n",
    "    train_data = [dataset[i] for i in train_indices]\n",
    "    val_data = [dataset[i] for i in val_indices]\n",
    "    test_data = [dataset[i] for i in test_indices]\n",
    "        \n",
    "    X_train, y_train = features[train_indices], targets[train_indices]\n",
    "    X_val, y_val = features[val_indices], targets[val_indices]\n",
    "    X_test, y_test = features[test_indices], targets[test_indices]\n",
    "    \n",
    "    X_train, X_val, X_test, scaler_features = uf.scale_features(X_train, X_val, X_test)\n",
    "    y_train, y_val, y_test, scaler_targets = uf.scale_targets(y_train, y_val, y_test)\n",
    "    \n",
    "    scalers = {\"features\": scaler_features, \"targets\": scaler_targets}\n",
    "    \n",
    "    y_train = torch.tensor(y_train, dtype=torch.float, device=device)\n",
    "    y_val = torch.tensor(y_val, dtype=torch.float, device=device)\n",
    "    y_test = torch.tensor(y_test, dtype=torch.float, device=device)\n",
    "    \n",
    "    train_data = [Data(x=data.x.to(device), edge_index=data.edge_index.to(device), edge_attr=data.edge_attr.to(device), \n",
    "                       y=y_train[index].reshape(1,-1).to(device)) for index, data in enumerate(train_data)]\n",
    "    \n",
    "    val_data = [Data(x=data.x.to(device), edge_index=data.edge_index.to(device), edge_attr=data.edge_attr.to(device),\n",
    "                        y=y_val[index].reshape(1,-1).to(device)) for index, data in enumerate(val_data)]\n",
    "    \n",
    "    test_data = [Data(x=data.x.to(device), edge_index=data.edge_index.to(device), edge_attr=data.edge_attr.to(device), \n",
    "                      y=y_test[index].reshape(1,-1).to(device)) for index, data in enumerate(test_data)]\n",
    "        \n",
    "    X_train = torch.tensor(X_train, dtype=torch.float, device=device)\n",
    "    X_val = torch.tensor(X_val, dtype=torch.float, device=device)\n",
    "    X_test = torch.tensor(X_test, dtype=torch.float, device=device)\n",
    "      \n",
    "    for index in range(len(train_data)):\n",
    "        train_data[index].features = X_train[index,:].unsqueeze(0) # extra dimension for batch\n",
    "    for index in range(len(val_data)):\n",
    "        val_data[index].features = X_val[index,:].unsqueeze(0)\n",
    "    for index in range(len(test_data)):\n",
    "        test_data[index].features = X_test[index,:].unsqueeze(0)\n",
    "\n",
    "    return train_data, val_data, test_data, scalers\n",
    "\n",
    "def create_data_loaders(train_data, val_data, test_data, batch_size):\n",
    "    \n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Loading data...\n",
      "num_samples: 132820\n",
      "...Data loading done...\n"
     ]
    }
   ],
   "source": [
    "print(\"...Loading data...\")\n",
    "properties_names = ['A', 'B', 'C', 'mu', 'alfa', 'homo', 'lumo', 'gap', 'R²', 'zpve', 'U0', 'U', 'H', 'G', 'Cv']\n",
    "x_smiles = df.smiles.values\n",
    "targets = df.loc[:, properties_names].values\n",
    "\n",
    "train_data, val_data, test_data, scalers = create_scale_and_split_dataset(features, targets, 0.15, 0.2)\n",
    "train_loader, val_loader, test_loader = create_data_loaders(train_data, val_data, test_data, 64)\n",
    "print(\"...Data loading done...\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Neural Network"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model for all targets at once"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 2,144,193\n",
      "\n",
      "...Starting training...\n",
      "Device used: cuda\n",
      "Epoch: 01 | Train Loss: 0.13400 | Validation Loss: 0.08864\n",
      "Epoch: 02 | Train Loss: 0.11036 | Validation Loss: 0.08509\n",
      "Epoch: 03 | Train Loss: 0.10290 | Validation Loss: 0.07705\n",
      "Epoch: 04 | Train Loss: 0.09938 | Validation Loss: 0.07622\n",
      "Epoch: 05 | Train Loss: 0.09597 | Validation Loss: 0.07355\n",
      "Epoch: 06 | Train Loss: 0.09452 | Validation Loss: 0.07130\n",
      "Epoch: 07 | Train Loss: 0.09208 | Validation Loss: 0.08102\n",
      "Epoch: 08 | Train Loss: 0.09015 | Validation Loss: 0.07589\n",
      "Epoch: 09 | Train Loss: 0.08835 | Validation Loss: 0.06753\n",
      "Epoch: 10 | Train Loss: 0.08694 | Validation Loss: 0.06902\n",
      "Epoch: 11 | Train Loss: 0.08585 | Validation Loss: 0.06623\n",
      "Epoch: 12 | Train Loss: 0.08487 | Validation Loss: 0.06600\n",
      "Epoch: 13 | Train Loss: 0.08394 | Validation Loss: 0.06437\n",
      "Epoch: 14 | Train Loss: 0.08334 | Validation Loss: 0.06486\n",
      "Epoch: 15 | Train Loss: 0.08232 | Validation Loss: 0.06278\n",
      "Epoch: 16 | Train Loss: 0.08176 | Validation Loss: 0.06360\n",
      "Epoch: 17 | Train Loss: 0.08060 | Validation Loss: 0.06326\n",
      "Epoch: 18 | Train Loss: 0.07994 | Validation Loss: 0.06230\n",
      "Epoch: 19 | Train Loss: 0.07951 | Validation Loss: 0.06238\n",
      "Epoch: 20 | Train Loss: 0.07912 | Validation Loss: 0.06136\n",
      "Epoch: 21 | Train Loss: 0.07843 | Validation Loss: 0.06183\n",
      "Epoch: 22 | Train Loss: 0.07755 | Validation Loss: 0.06148\n",
      "Epoch: 23 | Train Loss: 0.07727 | Validation Loss: 0.06026\n",
      "Epoch: 24 | Train Loss: 0.07665 | Validation Loss: 0.05974\n",
      "Epoch: 25 | Train Loss: 0.07635 | Validation Loss: 0.05938\n",
      "Epoch: 26 | Train Loss: 0.07611 | Validation Loss: 0.06152\n",
      "Epoch: 27 | Train Loss: 0.07614 | Validation Loss: 0.05905\n",
      "Epoch: 28 | Train Loss: 0.07554 | Validation Loss: 0.05850\n",
      "Epoch: 29 | Train Loss: 0.07495 | Validation Loss: 0.06029\n",
      "Epoch: 30 | Train Loss: 0.07483 | Validation Loss: 0.05835\n",
      "Epoch: 31 | Train Loss: 0.07444 | Validation Loss: 0.05864\n",
      "Epoch: 32 | Train Loss: 0.07416 | Validation Loss: 0.05877\n",
      "Epoch: 33 | Train Loss: 0.07390 | Validation Loss: 0.06276\n",
      "Epoch: 34 | Train Loss: 0.07373 | Validation Loss: 0.05795\n",
      "Epoch: 35 | Train Loss: 0.07307 | Validation Loss: 0.05774\n",
      "Epoch: 36 | Train Loss: 0.07328 | Validation Loss: 0.05750\n",
      "Epoch: 37 | Train Loss: 0.07292 | Validation Loss: 0.05781\n",
      "Epoch: 38 | Train Loss: 0.07283 | Validation Loss: 0.05732\n",
      "Epoch: 39 | Train Loss: 0.07251 | Validation Loss: 0.05705\n",
      "Epoch: 40 | Train Loss: 0.07240 | Validation Loss: 0.05642\n",
      "Epoch: 41 | Train Loss: 0.07213 | Validation Loss: 0.05750\n",
      "Epoch: 42 | Train Loss: 0.07193 | Validation Loss: 0.05589\n",
      "Epoch: 43 | Train Loss: 0.07173 | Validation Loss: 0.05757\n",
      "Epoch: 44 | Train Loss: 0.07171 | Validation Loss: 0.05763\n",
      "Epoch: 45 | Train Loss: 0.07092 | Validation Loss: 0.05639\n",
      "Epoch: 46 | Train Loss: 0.07130 | Validation Loss: 0.05586\n",
      "Epoch: 47 | Train Loss: 0.07103 | Validation Loss: 0.05804\n",
      "Epoch: 48 | Train Loss: 0.07078 | Validation Loss: 0.05567\n",
      "Epoch: 49 | Train Loss: 0.07082 | Validation Loss: 0.05594\n",
      "Epoch: 50 | Train Loss: 0.07053 | Validation Loss: 0.05575\n",
      "Epoch: 51 | Train Loss: 0.07070 | Validation Loss: 0.05530\n",
      "Epoch: 52 | Train Loss: 0.07027 | Validation Loss: 0.05575\n",
      "Epoch: 53 | Train Loss: 0.07064 | Validation Loss: 0.05558\n",
      "Epoch: 54 | Train Loss: 0.07046 | Validation Loss: 0.05662\n",
      "Epoch: 55 | Train Loss: 0.07033 | Validation Loss: 0.05585\n",
      "Epoch: 56 | Train Loss: 0.07038 | Validation Loss: 0.05531\n",
      "Early stopping at epoch 56 after 5 epochs without improvement.\n",
      "...Training done...\n",
      "...Calculating final results...\n",
      "====================================================\n",
      "Final training R2: [0.85320693 0.92776569 0.94361184 0.68548882 0.98372716 0.89935728\n",
      " 0.96745356 0.95069007 0.96219925 0.9962519  0.99830922 0.99830637\n",
      " 0.99830642 0.99830779 0.98846052]\n",
      "Average final training R2:  0.9434295216723636\n",
      "Final training loss: 0.053904533863973404\n",
      "Final validation R2: [0.84379673 0.92839127 0.94253763 0.67421775 0.98376654 0.8976928\n",
      " 0.96730256 0.94877848 0.96237648 0.99628794 0.99826176 0.99825897\n",
      " 0.998259   0.99826066 0.98879035]\n",
      "Average validation R2:  0.9417985948375386\n",
      "Final validation loss: 0.05530067235947801\n",
      "Final test R2: [0.84994068 0.92622862 0.94124479 0.64652307 0.98269177 0.89596063\n",
      " 0.96504901 0.94749884 0.95977951 0.99624921 0.99821405 0.99821096\n",
      " 0.99821111 0.99821254 0.98741241]\n",
      "Average final test R2:  0.9394284804618291\n",
      "Final test loss: 0.05652807776529629\n"
     ]
    }
   ],
   "source": [
    "def train(model, batch):\n",
    "      targets = batch.y\n",
    "      \n",
    "      model.train()\n",
    "      optimizer.zero_grad()  # Clear gradients.\n",
    "      out = model(batch.x, batch.edge_index, batch.edge_attr, \n",
    "                        batch.batch, batch.features).to(device)  # Perform a single forward pass.\n",
    "\n",
    "      loss = criterion(out, targets) \n",
    "\n",
    "      loss.backward()  # Derive gradients.\n",
    "      optimizer.step()  # Update parameters based on gradients.\n",
    "      \n",
    "      return loss\n",
    "\n",
    "def test(model, data):\n",
    "      for i, batch in enumerate(data):\n",
    "            targets = batch.y.cpu()\n",
    "            \n",
    "            model.eval()\n",
    "            out = model(batch.x, batch.edge_index, batch.edge_attr, \n",
    "                        batch.batch, batch.features).cpu()\n",
    "            \n",
    "            # Caculate R2 for each target\n",
    "            for target_idx in range(target_dim):\n",
    "                  if target_idx != 0:\n",
    "                        r2_score_var = np.vstack((r2_score_var, r2_score(targets[:,target_idx].detach().numpy(), \n",
    "                                                          out[:,target_idx].detach().numpy())))\n",
    "                  else:\n",
    "                        r2_score_var = np.array([r2_score(targets[:,target_idx].detach().numpy(),\n",
    "                                                          out[:,target_idx].detach().numpy())])        \n",
    "            all_r2 = np.hstack((all_r2, r2_score_var)) if i != 0 else r2_score_var\n",
    "                  \n",
    "            loss = float(criterion(out, targets).detach().numpy())\n",
    "            all_loss = np.hstack((all_loss, loss)) if i != 0 else np.array(loss)\n",
    "\n",
    "      average_test_r2 = np.mean(all_r2, axis=1)\n",
    "      average_test_loss = np.mean(all_loss)\n",
    "      \n",
    "      return average_test_r2, average_test_loss\n",
    "\n",
    "def early_stopping(val_losses, patience): # returns True if there is no improvement in val_loss\n",
    "      if len(val_losses) < patience:\n",
    "            return False\n",
    "      else:\n",
    "            best_loss = np.min(val_losses)\n",
    "            current_loss = val_losses[-1]\n",
    "            \n",
    "            if current_loss > best_loss:\n",
    "                  return True\n",
    "            else:\n",
    "                  return False\n",
    "\n",
    "node_feature_dim = train_data[0].x.shape[1]\n",
    "feature_dim = train_data[0].features.shape[1]\n",
    "target_dim = train_data[0].y.shape[1]\n",
    "\n",
    "num_layers, hidden_channels = 5, 512\n",
    "\n",
    "model_class = GNNs.define_GNN_structure_with_global_features(num_layers, hidden_channels, node_feature_dim, \n",
    "                                                             feature_dim, target_dim)\n",
    "model = model_class().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "\n",
    "# Decay for learning rate\n",
    "decay_rate = 0.94\n",
    "lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=decay_rate)\n",
    "\n",
    "criterion = torch.nn.MSELoss().to(device)\n",
    "\n",
    "train_params, _ = uf.get_num_parameters(model)\n",
    "print(f\"Trainable parameters: {train_params:,}\")\n",
    "      \n",
    "# Vectors to append accuracy to:\n",
    "train_r2 = []\n",
    "train_loss = []\n",
    "test_r2 = []\n",
    "test_loss = []\n",
    "val_r2 = []\n",
    "val_loss = []\n",
    "\n",
    "n_epochs = 100\n",
    "print_every_N_epochs = False\n",
    "N = 10 # print R2 every N epochs\n",
    "\n",
    "epoch_times = []\n",
    "train_times = []\n",
    "print()\n",
    "print(\"...Starting training...\")\n",
    "print(\"Device used:\", device)\n",
    "\n",
    "val_losses_epoch = [] # for early stopping\n",
    "patience = 5 # how many epochs to wait for the val loss to improve\n",
    "best_val_loss = np.inf  \n",
    "epochs_without_improvement = 0\n",
    "\n",
    "for epoch in np.arange(1, n_epochs+1):\n",
    "      epoch_start = time.time()\n",
    "      losses = []\n",
    "      train_start = time.time()\n",
    "      for batch in train_loader:\n",
    "            loss = train(model, batch)\n",
    "            losses.append(loss.cpu().detach().numpy())  \n",
    "      # Compute validation loss\n",
    "      model.eval()\n",
    "      val_losses = []\n",
    "      for batch in val_loader:\n",
    "            targets = batch.y\n",
    "            out = model(batch.x, batch.edge_index, batch.edge_attr, \n",
    "                        batch.batch, batch.features)\n",
    "            val_losses.append(criterion(out, targets).cpu().detach().numpy())\n",
    "      \n",
    "      val_loss_epoch = np.mean(val_losses)\n",
    "      val_losses_epoch.append(val_loss_epoch)\n",
    "      print(f\"Epoch: {epoch:02d} | Train Loss: {np.mean(losses):.5f} | Validation Loss: {val_loss_epoch:.5f}\")\n",
    "      train_end = time.time()\n",
    "      train_times.append(train_end - train_start)\n",
    "      lr_scheduler.step() # Decay to learning rate\n",
    "      \n",
    "      # check for early stopping\n",
    "      if early_stopping(val_losses_epoch, patience) and epoch > patience:\n",
    "            epochs_without_improvement += 1\n",
    "            if epochs_without_improvement >= patience:\n",
    "                  print(f\"Early stopping at epoch {epoch} after {patience} epochs without improvement.\")\n",
    "                  break\n",
    "      else:\n",
    "            epochs_without_improvement = 0\n",
    "            best_val_loss = val_loss_epoch\n",
    "            # save the model\n",
    "            torch.save(model.state_dict(), 'best_graphs+features_model.pt')\n",
    "                   \n",
    "      if print_every_N_epochs and (epoch % N == 0 or epoch == 1) and epoch != n_epochs:\n",
    "            test_start = time.time()\n",
    "                \n",
    "            r2_temp_val, loss_temp_val = test(model, val_loader)\n",
    "            val_r2.append(r2_temp_val)\n",
    "            val_loss.append(loss_temp_val)\n",
    "            \n",
    "            print(f'Validation R2: {r2_temp_val}')\n",
    "            print(f\"Validation loss: {loss_temp_val}\")\n",
    "            \n",
    "      epoch_end = time.time()\n",
    "      epoch_times.append(epoch_end - epoch_start)\n",
    "      \n",
    "            \n",
    "print(\"...Training done...\")\n",
    "print(\"...Calculating final results...\")\n",
    "model.load_state_dict(torch.load('best_graphs+features_model.pt'))\n",
    "\n",
    "r2_temp_train, loss_temp_train = test(model, train_loader) \n",
    "train_r2.append(r2_temp_train)\n",
    "train_loss.append(loss_temp_train)\n",
    "\n",
    "r2_temp_val, loss_temp_val = test(model, val_loader)\n",
    "val_r2.append(r2_temp_val)\n",
    "val_loss.append(loss_temp_val)\n",
    "\n",
    "r2_temp_test, loss_temp_test = test(model, test_loader)\n",
    "test_r2.append(r2_temp_test)\n",
    "test_loss.append(loss_temp_test)\n",
    "\n",
    "print(\"====================================================\")\n",
    "print(\"Final training R2:\", train_r2[-1])\n",
    "print(\"Average final training R2: \", np.mean(train_r2[-1]))\n",
    "print(\"Final training loss:\", train_loss[-1])\n",
    "\n",
    "print(\"Final validation R2:\", val_r2[-1])\n",
    "print(\"Average validation R2: \", np.mean(val_r2[-1]))\n",
    "print(\"Final validation loss:\", val_loss[-1])\n",
    "\n",
    "print(\"Final test R2:\", test_r2[-1])\n",
    "print(\"Average final test R2: \", np.mean(test_r2[-1]))\n",
    "print(\"Final test loss:\", test_loss[-1])          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used: cuda\n",
      "\n",
      "Total number of epochs: 55\n",
      "Total training time: 24.07 minutes\n",
      "Total time in training: 24.55 minutes\n",
      "\n",
      "Average epoch time: 26.3 seconds\n",
      "Average time in training: 26.3 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"Device used:\", device)\n",
    "print()\n",
    "print(f\"Total number of epochs: {len(epoch_times)}\")\n",
    "print(f\"Total training time: {np.sum(epoch_times)/60:.2f} minutes\")\n",
    "print(f\"Total time in training: {np.sum(train_times)/60:.2f} minutes\")\n",
    "print()\n",
    "print(f\"Average epoch time: {np.mean(epoch_times):.1f} seconds\")\n",
    "print(f\"Average time in training: {np.mean(train_times):.1f} seconds\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model for just one target"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GlobalStorage' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\jespe\\anaconda3\\envs\\proj_env\\lib\\site-packages\\torch_geometric\\data\\storage.py:78\u001b[0m, in \u001b[0;36mBaseStorage.__getattr__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 78\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m[key]\n\u001b[0;32m     79\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\jespe\\anaconda3\\envs\\proj_env\\lib\\site-packages\\torch_geometric\\data\\storage.py:103\u001b[0m, in \u001b[0;36mBaseStorage.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, key: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m--> 103\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mapping[key]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'shape'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m feature_dim \u001b[39m=\u001b[39m train_data[:][\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mshape[\u001b[39m1\u001b[39m]\n\u001b[0;32m      2\u001b[0m target_dim \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m(batch, target_idx):\n",
      "File \u001b[1;32mc:\\Users\\jespe\\anaconda3\\envs\\proj_env\\lib\\site-packages\\torch_geometric\\data\\data.py:441\u001b[0m, in \u001b[0;36mData.__getattr__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    435\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m_store\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m:\n\u001b[0;32m    436\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[0;32m    437\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe \u001b[39m\u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m\u001b[39m object was created by an older version of PyG. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    438\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIf this error occurred while loading an already existing \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    439\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdataset, remove the \u001b[39m\u001b[39m'\u001b[39m\u001b[39mprocessed/\u001b[39m\u001b[39m'\u001b[39m\u001b[39m directory in the dataset\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    440\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mroot folder and try again.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 441\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_store, key)\n",
      "File \u001b[1;32mc:\\Users\\jespe\\anaconda3\\envs\\proj_env\\lib\\site-packages\\torch_geometric\\data\\storage.py:80\u001b[0m, in \u001b[0;36mBaseStorage.__getattr__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m[key]\n\u001b[0;32m     79\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n\u001b[1;32m---> 80\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\n\u001b[0;32m     81\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'GlobalStorage' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "feature_dim = train_data[:][0].shape[1]\n",
    "target_dim = 1\n",
    "\n",
    "def train(batch, target_idx):\n",
    "      features = batch[:][0]\n",
    "      targets = batch[:][1][:,target_idx].unsqueeze(-1)\n",
    "      \n",
    "      model.train()\n",
    "      optimizer.zero_grad()  # Clear gradients.\n",
    "      out = model(features).to(device)  # Perform a single forward pass.\n",
    "\n",
    "      loss = criterion(out, targets) \n",
    "\n",
    "      loss.backward()  # Derive gradients.\n",
    "      optimizer.step()  # Update parameters based on gradients.\n",
    "      \n",
    "      return loss\n",
    "\n",
    "def test(data, target_idx):\n",
    "      for i, batch in enumerate(data):\n",
    "            features = batch[:][0]\n",
    "            targets = batch[:][1][:,target_idx].cpu().unsqueeze(-1)\n",
    "            \n",
    "            model.eval()\n",
    "            out = model(features).cpu()\n",
    "            \n",
    "            # Caculate R2    \n",
    "            r2_score_var = np.array([r2_score(targets.detach().numpy(), out.detach().numpy())])        \n",
    "            all_r2 = np.hstack((all_r2, r2_score_var)) if i != 0 else r2_score_var\n",
    "                  \n",
    "            loss = float(criterion(out, targets).detach().numpy())\n",
    "            all_loss = np.hstack((all_loss, loss)) if i != 0 else np.array(loss)\n",
    "\n",
    "      average_test_r2 = np.mean(all_r2)\n",
    "      average_test_loss = np.mean(all_loss)\n",
    "      \n",
    "      return average_test_r2, average_test_loss\n",
    "\n",
    "num_targets = train_data[:][1].shape[1]\n",
    "start_time = time.time()\n",
    "for target_index in range(num_targets):\n",
    "      print(\"Target index:\", target_index)\n",
    "\n",
    "      model = DNN(hidden_channels=1024, feature_dim=feature_dim, target_dim=target_dim).to(device) \n",
    "      optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=5e-4)\n",
    "      criterion = torch.nn.MSELoss().to(device)\n",
    "      \n",
    "      # Decay for learning rate\n",
    "      decayRate = 0.9\n",
    "      lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=decayRate)\n",
    "\n",
    "      # Vectors to append accuracy to:\n",
    "      train_r2 = []\n",
    "      train_loss = []\n",
    "      test_r2 = []\n",
    "      test_loss = []\n",
    "      val_r2 = []\n",
    "      val_loss = []\n",
    "\n",
    "      n_epochs = 10\n",
    "\n",
    "      print(\"Device used:\", device)\n",
    "\n",
    "      for epoch in np.arange(1, n_epochs+1):\n",
    "            losses = []\n",
    "            for data in train_loader:\n",
    "                  loss = train(data, target_index)\n",
    "                  losses.append(loss.cpu().detach().numpy())  \n",
    "            # Compute validation loss\n",
    "            model.eval()\n",
    "            val_losses = []\n",
    "            for batch in val_loader:\n",
    "                  features = batch[:][0]\n",
    "                  targets = batch[:][1][:, target_index].unsqueeze(-1)\n",
    "                  out = model(features)\n",
    "                  val_losses.append(criterion(out, targets).cpu().detach().numpy())\n",
    "            print(f\"Epoch: {epoch:02d} | Train Loss: {np.mean(losses):.5f} | Validation Loss: {np.mean(val_losses):.5f}\")\n",
    "            \n",
    "            lr_scheduler.step() # Decay to learning rate\n",
    "                  \n",
    "            if epoch == n_epochs:         # calculate results of training: test on all data\n",
    "                  test_start = time.time()\n",
    "                  r2_temp_train, loss_temp_train = test(train_loader, target_index) \n",
    "                  train_r2.append(r2_temp_train)\n",
    "                  train_loss.append(loss_temp_train)\n",
    "                  \n",
    "                  r2_temp_val, loss_temp_val = test(val_loader, target_index)\n",
    "                  val_r2.append(r2_temp_val)\n",
    "                  val_loss.append(loss_temp_val)\n",
    "                  \n",
    "                  r2_temp_test, loss_temp_test = test(test_loader, target_index)\n",
    "                  test_r2.append(r2_temp_test)\n",
    "                  test_loss.append(loss_temp_test)\n",
    "\n",
    "                  print(\"====================================================\")\n",
    "                  print(\"Final training R2:\", train_r2[-1])\n",
    "                  print(\"Average final training R2: \", np.mean(train_r2[-1]))\n",
    "                  print(\"Final training loss:\", train_loss[-1])\n",
    "                  print()\n",
    "                  print(\"Final validation R2:\", val_r2[-1])\n",
    "                  print(\"Average validation test R2: \", np.mean(val_r2[-1]))\n",
    "                  print(\"Final validation loss:\", val_loss[-1])\n",
    "                  print()\n",
    "                  print(\"Final test R2:\", test_r2[-1])\n",
    "                  print(\"Average final test R2: \", np.mean(test_r2[-1]))\n",
    "                  print(\"Final test loss:\", test_loss[-1])\n",
    "                  print(\"====================================================\")\n",
    "                                    \n",
    "print(\"...Done...\")\n",
    "end_time = time.time()\n",
    "print(f\"Time taken: {(end_time - start_time)/60} minutes\")\n",
    "print(f\"Average time per target: {(end_time - start_time)/(num_targets*60)} minutes\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
