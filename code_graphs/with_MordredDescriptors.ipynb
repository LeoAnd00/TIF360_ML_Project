{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TIF360 Project"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main source: https://www.kaggle.com/code/rmonge/predicting-molecule-properties-based-on-its-smiles/notebook"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time as time\n",
    "\n",
    "import torch\n",
    "import torch_geometric\n",
    "from torch_geometric.loader import DataLoader\n",
    "import torch_geometric.utils as utils\n",
    "import networkx as nx\n",
    "from torch.nn import Linear, LeakyReLU\n",
    "from torch_geometric.nn import global_mean_pool, GATConv, BatchNorm, GraphNorm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available: True\n",
      "device: cuda\n",
      "NVIDIA GeForce RTX 3080\n"
     ]
    }
   ],
   "source": [
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"device:\", \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(132820, 21)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"../data/smiles_and_targets.csv\")\n",
    "print(np.shape(df))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(132820, 985)\n"
     ]
    }
   ],
   "source": [
    "mol_descriptor = np.load(\"../data/Mordred_mol_descriptors.npy\")\n",
    "print(mol_descriptor.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigate Neural Networks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Neural Networks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert data to graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from graph_dataset_functions import create_graph_dataset_from_smiles\n",
    "\n",
    "properties_names = ['A', 'B', 'C', 'mu', 'alfa', 'homo', 'lumo', 'gap', 'RÂ²', 'zpve', 'U0', 'U', 'H', 'G', 'Cv']\n",
    "\n",
    "x_smiles = df.smiles.values\n",
    "y = df.loc[:, properties_names].values  # shape = (n_samples, n_properties)\n",
    "\n",
    "dataset = create_graph_dataset_from_smiles(x_smiles, y[0:len(x_smiles), :])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information of the graph dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of graphs (molecules): 132820\n",
      "=================================================================================\n",
      "Properties of graph 50, molecule smiles: CC1=CNC=C1\n",
      "Number of nodes: 6\n",
      "Number of edges: 12\n",
      "Number of node features: 78\n",
      "Number of edge features: 10\n",
      "Number of target properties: 15\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of graphs (molecules): {len(dataset)}')\n",
    "graph = dataset[50]\n",
    "print('=================================================================================')\n",
    "print(f'Properties of graph {50}, molecule smiles: {df.smiles.values[50]}')\n",
    "print(f'Number of nodes: {dataset[50].x.shape[0]}')\n",
    "print(f'Number of edges: {dataset[50].edge_index.shape[1]}')\n",
    "print(f'Number of node features: {dataset[50].x.shape[1]}')\n",
    "print(f'Number of edge features: {dataset[50].edge_attr.shape[1]}')\n",
    "print(f'Number of target properties: {dataset[50].y.shape[1]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filterout data with no edge features defined (Like ex: CH4) (These causes problems down the line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of none edge feature molecules:  0\n",
      "Before:  132820\n",
      "After:  132820\n"
     ]
    }
   ],
   "source": [
    "indexes_to_delete = []\n",
    "for item in range(0,len(dataset)):\n",
    "    if dataset[item].edge_attr.shape[1] == 0:\n",
    "        indexes_to_delete.append(item)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "indexes_to_delete.sort()\n",
    "\n",
    "print(\"Number of none edge feature molecules: \", len(indexes_to_delete))\n",
    "\n",
    "print(\"Before: \", len(dataset))\n",
    "\n",
    "for item in range(0,len(indexes_to_delete)):\n",
    "    print(\"Molecule to delete: \", df.smiles.values[indexes_to_delete[item]])\n",
    "    #del dataset[indexes_to_delete[item] - item] \n",
    "    dataset.pop((indexes_to_delete[item] - item)) # -item since all future data points will have its index reduced by 1 for each deleted previous data point\n",
    "print(\"After: \", len(dataset))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create functions to load and pre-process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "\n",
    "def scale_and_split_data(dataset, device, input_scaler):\n",
    "    # split the dataset into test and validation:\n",
    "    num_samples = len(dataset)\n",
    "\n",
    "    # Want to divide data randomly\n",
    "    random.seed(42)\n",
    "    random_indexes = np.array(random.sample(range(num_samples), num_samples)) # random.sample ensures no duplicates\n",
    "\n",
    "    train_data = [dataset[index] for index in random_indexes[int(.2 * num_samples ) :]] # 80%\n",
    "    test_data = [dataset[index] for index in random_indexes[: int(.2 * num_samples )]] # 20%\n",
    "\n",
    "    print(\"Example train data target before scaling\", train_data[0].y)\n",
    "    train_data_targets = [data.y for data in train_data]\n",
    "    train_data_targets = torch.concatenate(train_data_targets, axis=0)\n",
    "    test_data_targets = [data.y for data in test_data]\n",
    "    test_data_targets = torch.concatenate(test_data_targets, axis=0)\n",
    "\n",
    "    input_scaler = StandardScaler()\n",
    "    train_data_targets = input_scaler.fit_transform(train_data_targets)\n",
    "    test_data_targets = input_scaler.transform(test_data_targets)\n",
    "\n",
    "    # print(\"Example train data target\", train_data_targets[0].reshape(1,-1).shape)\n",
    "    train_data_targets = torch.tensor(train_data_targets, dtype=torch.float)\n",
    "    test_data_targets = torch.tensor(test_data_targets, dtype=torch.float)\n",
    "\n",
    "    train_data = [Data(x=data.x.to(device), edge_index=data.edge_index.to(device), edge_attr=data.edge_attr.to(device), \n",
    "                       y=train_data_targets[index].reshape(1,-1).to(device)) for index, data in enumerate(train_data)]\n",
    "    \n",
    "    test_data = [Data(x=data.x.to(device), edge_index=data.edge_index.to(device), edge_attr=data.edge_attr.to(device), \n",
    "                      y=test_data_targets[index].reshape(1,-1).to(device)) for index, data in enumerate(test_data)]\n",
    "    print(\"Example train data target after scaling:\", train_data[0].y)\n",
    "    \n",
    "    # Divide descriptors and fingerprints into train and test\n",
    "    train_data_descriptor = [mol_descriptor[index,:] for index in random_indexes[int(.2 * num_samples ) :]] # 80%\n",
    "    test_data_desriptors = [mol_descriptor[index,:] for index in random_indexes[: int(.2 * num_samples )]] # 20%\n",
    "\n",
    "    # Normalize descriptors and fingerprints using minmax scaler\n",
    "    minmax_scaler = MinMaxScaler()\n",
    "    train_data_descriptor = minmax_scaler.fit_transform(train_data_descriptor)\n",
    "    test_data_desriptors = minmax_scaler.transform(test_data_desriptors)\n",
    "        \n",
    "    for index in range(0,len(train_data)):\n",
    "        train_data[index].descriptors = torch.FloatTensor([train_data_descriptor[index,:]]).to(device)\n",
    "\n",
    "    for index in range(0,len(test_data)):\n",
    "        test_data[index].descriptors = torch.FloatTensor([test_data_desriptors[index,:]]).to(device)\n",
    "        \n",
    "    return train_data, test_data, input_scaler\n",
    "\n",
    "def create_data_loaders(train_data, test_data, batch_size): \n",
    "    train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine descriptors temporarely with in train and test data for graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Testing stuff\n",
    "\n",
    "# temp = train_data[1000]\n",
    "# temp.descriptors = train_data_descriptor[1000,:]\n",
    "\n",
    "# print(temp)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main GNN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model for all targets at once"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GNN function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN_all(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, feature_dim, target_dim, descriptor_dim):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GATConv(feature_dim, hidden_channels)\n",
    "        self.conv1_norm = GraphNorm(hidden_channels)\n",
    "        self.conv2 = GATConv(hidden_channels, hidden_channels)\n",
    "        self.conv2_norm = GraphNorm(hidden_channels)\n",
    "        self.conv3 = GATConv(hidden_channels, hidden_channels)\n",
    "        self.conv3_norm = GraphNorm(hidden_channels)\n",
    "        self.conv4 = GATConv(hidden_channels, hidden_channels)\n",
    "        self.conv4_norm = GraphNorm(hidden_channels)\n",
    "        self.conv5 = GATConv(hidden_channels, hidden_channels)\n",
    "        self.conv5_norm = GraphNorm(hidden_channels)\n",
    "        \n",
    "        self.input_norm = BatchNorm(hidden_channels+descriptor_dim)\n",
    "        self.lin1 = Linear(hidden_channels+descriptor_dim, 1024)\n",
    "        self.lin2 = Linear(1024, 512)\n",
    "        self.lin3 = Linear(512, target_dim)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch, descriptors): \n",
    "        x = self.conv1(x, edge_index, edge_attr)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.conv1_norm(x)\n",
    "        x = self.conv2(x, edge_index, edge_attr)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.conv2_norm(x)\n",
    "        x = self.conv3(x, edge_index, edge_attr)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.conv3_norm(x)\n",
    "        x = self.conv4(x, edge_index, edge_attr)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.conv4_norm(x)\n",
    "        x = self.conv5(x, edge_index, edge_attr)\n",
    "        x = self.conv5_norm(x)\n",
    "\n",
    "        #Returns batch-wise graph-level-outputs by averaging node features across the node dimension, so that for a single graph G\n",
    "        #its output is computed by\n",
    "        x = global_mean_pool(x, batch) \n",
    "        x2 = descriptors\n",
    "        x = torch.cat((x,x2),1)\n",
    "        \n",
    "        x = self.input_norm(x)\n",
    "        x = self.lin1(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        \n",
    "        x = self.lin2(x)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "\n",
    "        x = self.lin3(x)\n",
    " \n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With attention before dense layer (Doesn't work as it is now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import MultiheadAttention\n",
    "\n",
    "class GNN_attn(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, feature_dim, target_dim, descriptor_dim):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GATConv(feature_dim, hidden_channels)\n",
    "        self.conv2 = GATConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GATConv(hidden_channels, hidden_channels)\n",
    "        self.conv4 = GATConv(hidden_channels, hidden_channels)\n",
    "        self.conv5 = GATConv(hidden_channels, hidden_channels)\n",
    "        self.attention = MultiheadAttention(hidden_channels, 1) # input_size, hidden_size, num_heads(number of parallel self-attention heads to use)\n",
    "        self.attention2 = MultiheadAttention(descriptor_dim, 1) \n",
    "        self.lin = Linear(hidden_channels+descriptor_dim, 512)\n",
    "        self.lin2 = Linear(512, 256)\n",
    "        self.lin3 = Linear(256, target_dim)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch, descriptors, fingerprints): \n",
    "        x = self.conv1(x, edge_index, edge_attr)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.conv2(x, edge_index, edge_attr)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.conv3(x, edge_index, edge_attr)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.conv4(x, edge_index, edge_attr)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.conv5(x, edge_index, edge_attr)\n",
    "\n",
    "        #Returns batch-wise graph-level-outputs by averaging node features across the node dimension, so that for a single graph G\n",
    "        #its output is computed by\n",
    "        x = global_mean_pool(x, batch) \n",
    "        x,_ = self.attention(x,x,x)\n",
    "        x2 = descriptors\n",
    "        x2,_ = self.attention2(x2,x2,x2)\n",
    "        x3 = fingerprints\n",
    "        #x3,_ = self.attention3(x3,x3,x3)\n",
    "        x = torch.cat((x,x2,x3),1)\n",
    "        #x,_ = self.attention(x,x,x)\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.lin2(x)\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.lin3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Loading data...\n",
      "Example train data target before scaling tensor([[ 2.9693e+00,  1.9777e+00,  1.8423e+00,  1.2093e+00,  8.4060e+01,\n",
      "         -1.8970e-01, -7.0400e-02,  1.1930e-01,  8.7339e+02,  1.5988e-01,\n",
      "         -3.4861e+02, -3.4860e+02, -3.4860e+02, -3.4864e+02,  2.9189e+01]])\n",
      "Example train data target after scaling: tensor([[-0.3350,  1.3300,  2.2841, -1.0264,  1.0990,  2.2975, -1.7445, -2.7966,\n",
      "         -1.1527,  0.3357,  1.6155,  1.6155,  1.6155,  1.6156, -0.6067]],\n",
      "       device='cuda:0')\n",
      "...Data loading done...\n"
     ]
    }
   ],
   "source": [
    "print(\"...Loading data...\")\n",
    "train_data, test_data, scaler = scale_and_split_data(dataset, device, input_scaler=StandardScaler())\n",
    "train_loader, test_loader = create_data_loaders(train_data, test_data, batch_size=64)\n",
    "print(\"...Data loading done...\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "GNN_all.__init__() missing 1 required positional argument: 'fingerprint_dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m target_dim \u001b[39m=\u001b[39m train_data[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39my\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[0;32m      5\u001b[0m descriptor_dim \u001b[39m=\u001b[39m train_data[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mdescriptors\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m]\n\u001b[1;32m----> 7\u001b[0m model \u001b[39m=\u001b[39m GNN_all(hidden_channels\u001b[39m=\u001b[39;49m\u001b[39m128\u001b[39;49m, feature_dim\u001b[39m=\u001b[39;49mfeature_dim, target_dim\u001b[39m=\u001b[39;49mtarget_dim, \n\u001b[0;32m      8\u001b[0m                 descriptor_dim\u001b[39m=\u001b[39;49mdescriptor_dim)\u001b[39m.\u001b[39mto(device) \n\u001b[0;32m      9\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.0005\u001b[39m, weight_decay\u001b[39m=\u001b[39m\u001b[39m5e-4\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[39m# Decay for learning rate\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: GNN_all.__init__() missing 1 required positional argument: 'fingerprint_dim'"
     ]
    }
   ],
   "source": [
    "from utility_functions import get_num_parameters\n",
    "\n",
    "feature_dim = train_data[0].x.shape[1]\n",
    "target_dim = train_data[0].y.shape[1]\n",
    "descriptor_dim = train_data[0].descriptors.shape[1]\n",
    "\n",
    "model = GNN_all(hidden_channels=128, feature_dim=feature_dim, target_dim=target_dim, \n",
    "                descriptor_dim=descriptor_dim).to(device) \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0005, weight_decay=5e-4)\n",
    "\n",
    "# Decay for learning rate\n",
    "decayRate = 0.92\n",
    "lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=decayRate)\n",
    "\n",
    "criterion = torch.nn.MSELoss().to(device)\n",
    "\n",
    "train_params, tot_params = get_num_parameters(model)\n",
    "print(f\"Total number of parameters: {tot_params}\")\n",
    "print(f\"Trainable parameters: {train_params}\")\n",
    "\n",
    "def train(data_in):\n",
    "      model.train()\n",
    "      optimizer.zero_grad()  # Clear gradients.\n",
    "      out = model(data_in.x, data_in.edge_index, data_in.edge_attr, data_in.batch, \n",
    "                  data_in.descriptors).to(device)  # Perform a single forward pass.\n",
    "\n",
    "      targets = data_in.y\n",
    "      loss = criterion(out, targets) \n",
    "\n",
    "      loss.backward()  # Derive gradients.\n",
    "      optimizer.step()  # Update parameters based on gradients.\n",
    "      return loss\n",
    "\n",
    "def test(data):\n",
    "      all_r2 = []\n",
    "      all_loss = []\n",
    "      counter = -1    \n",
    "      for data_in in data:\n",
    "            counter += 1\n",
    "            model.eval()\n",
    "            out = model(data_in.x, data_in.edge_index, data_in.edge_attr, data_in.batch,\n",
    "                        data_in.descriptors, data_in.fingerprints).cpu()\n",
    "            targets = data_in.y.cpu()\n",
    "            \n",
    "            # Caculate R2\n",
    "            r2_score_var = []\n",
    "            for item in range(target_dim):\n",
    "                  if item == 0:\n",
    "                        r2_score_var = r2_score(targets[:,item].detach().numpy(), out[:,item].detach().numpy())\n",
    "                  else:\n",
    "                        new_score = r2_score(targets[:,item].detach().numpy(), out[:,item].detach().numpy())\n",
    "                        r2_score_var = np.vstack((r2_score_var, new_score))\n",
    "\n",
    "            if counter == 0:\n",
    "                  all_r2 = r2_score_var\n",
    "            else:\n",
    "                  all_r2 = np.hstack((all_r2, r2_score_var))\n",
    "                  \n",
    "            loss = float(criterion(out, targets).detach().numpy())\n",
    "            all_loss = np.hstack((all_loss, loss))\n",
    "\n",
    "      average_test_r2 = np.mean(all_r2, axis=1)\n",
    "      average_test_loss = np.mean(all_loss)\n",
    "      \n",
    "      return average_test_r2, average_test_loss\n",
    "\n",
    "# Vectors to append accuracy to:\n",
    "train_r2 = []\n",
    "train_loss = []\n",
    "test_r2 = []\n",
    "test_loss = []\n",
    "\n",
    "n_epochs = 20\n",
    "print_every_N_epochs = False\n",
    "N = 5 # print R2 every N epochs\n",
    "\n",
    "epoch_times = []\n",
    "train_times = []\n",
    "test_times = []\n",
    "print()\n",
    "print(\"...Starting training...\")\n",
    "print(\"Device used:\", device)\n",
    "for epoch in np.arange(1, n_epochs+1):\n",
    "      epoch_start = time.time()\n",
    "      losses = []\n",
    "      train_start = time.time()\n",
    "      for data in train_loader:\n",
    "            loss = train(data)\n",
    "            losses.append(loss.cpu().detach().numpy())\n",
    "      print(f'Epoch: {epoch:03d}, Loss: {np.mean(losses):.5f}')\n",
    "      train_end = time.time()\n",
    "      train_times.append(train_end - train_start)\n",
    "      lr_scheduler.step() # Decay to learning rate\n",
    "      \n",
    "      if print_every_N_epochs and (epoch % N == 0 or epoch == 1) and epoch != n_epochs:\n",
    "            test_start = time.time()\n",
    "            r2_temp_train, loss_temp_train = test(train_loader) \n",
    "            train_r2.append(r2_temp_train)\n",
    "            train_loss.append(loss_temp_train)         \n",
    "            r2_temp_test, loss_temp_test = test(test_loader)\n",
    "            test_r2.append(r2_temp_test)\n",
    "            test_loss.append(loss_temp_test)\n",
    "            print(f'Average Train R2: {r2_temp_train}')\n",
    "            print(f\"Average Train Loss: {loss_temp_train}\")\n",
    "            print(f'Average Test R2: {r2_temp_test}')\n",
    "            print(f\"Average Test Loss: {loss_temp_test}\")\n",
    "            test_end = time.time()\n",
    "            test_times.append(test_end - test_start)\n",
    "            \n",
    "      if epoch == n_epochs:         # calculate results of training\n",
    "            print(\"...Training done...\")\n",
    "            print(\"...Calculating final results...\")\n",
    "            test_start = time.time()\n",
    "            r2_temp_train, loss_temp_train = test(train_loader) \n",
    "            train_r2.append(r2_temp_train)\n",
    "            train_loss.append(loss_temp_train)\n",
    "            \n",
    "            r2_temp_test, loss_temp_test = test(test_loader)\n",
    "            test_r2.append(r2_temp_test)\n",
    "            test_loss.append(loss_temp_test)\n",
    "\n",
    "            print(\"====================================================\")\n",
    "            print(\"Final training R2:\", train_r2[-1])\n",
    "            print(\"Average final training R2: \", np.mean(train_r2[-1]))\n",
    "            print(\"Final training loss:\", train_loss[-1])\n",
    "\n",
    "            print(\"Final test R2:\", test_r2[-1])\n",
    "            print(\"Average final test R2: \", np.mean(test_r2[-1]))\n",
    "            print(\"Final test loss:\", test_loss[-1])\n",
    "            \n",
    "            test_end = time.time()\n",
    "            test_times.append(test_end - test_start)\n",
    "            \n",
    "      epoch_end = time.time()\n",
    "      epoch_times.append(epoch_end - epoch_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used: cuda\n",
      "\n",
      "Total number of epochs: 20\n",
      "Total training time: 8.78 minutes\n",
      "Total time in training: 8.36 minutes\n",
      "Total time in testing: 0.42 minutes\n",
      "\n",
      "Average epoch time: 26.3 seconds\n",
      "Average time in training: 25.1 seconds\n",
      "Average time in testing: 25.5 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"Device used:\", device)\n",
    "print()\n",
    "print(f\"Total number of epochs: {len(epoch_times)}\")\n",
    "print(f\"Total training time: {np.sum(epoch_times)/60:.2f} minutes\")\n",
    "print(f\"Total time in training: {np.sum(train_times)/60:.2f} minutes\")\n",
    "print(f\"Total time in testing: {np.sum(test_times)/60:.2f} minutes\")\n",
    "print()\n",
    "print(f\"Average epoch time: {np.mean(epoch_times):.1f} seconds\")\n",
    "print(f\"Average time in training: {np.mean(train_times):.1f} seconds\")\n",
    "print(f\"Average time in testing: {np.mean(test_times):.1f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "...Starting training...\n",
      "Device used: cuda\n",
      "Run 1\n",
      "Epoch: 001, Loss: 0.12977\n",
      "Epoch: 002, Loss: 0.10464\n",
      "Epoch: 003, Loss: 0.09764\n",
      "Epoch: 004, Loss: 0.09280\n",
      "Epoch: 005, Loss: 0.08890\n",
      "Epoch: 006, Loss: 0.08645\n",
      "Epoch: 007, Loss: 0.08319\n",
      "Epoch: 008, Loss: 0.08129\n",
      "Epoch: 009, Loss: 0.07932\n",
      "Epoch: 010, Loss: 0.07757\n",
      "Epoch: 011, Loss: 0.07589\n",
      "Epoch: 012, Loss: 0.07442\n",
      "Epoch: 013, Loss: 0.07301\n",
      "Epoch: 014, Loss: 0.07213\n",
      "Epoch: 015, Loss: 0.07104\n",
      "Epoch: 016, Loss: 0.07020\n",
      "Epoch: 017, Loss: 0.06926\n",
      "Epoch: 018, Loss: 0.06836\n",
      "Epoch: 019, Loss: 0.06742\n",
      "Epoch: 020, Loss: 0.06693\n",
      "...Training done...\n",
      "...Calculating final results...\n",
      "Run 2\n",
      "Epoch: 001, Loss: 0.12980\n",
      "Epoch: 002, Loss: 0.10470\n",
      "Epoch: 003, Loss: 0.09774\n",
      "Epoch: 004, Loss: 0.09288\n",
      "Epoch: 005, Loss: 0.08888\n",
      "Epoch: 006, Loss: 0.08650\n",
      "Epoch: 007, Loss: 0.08315\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 80\u001b[0m\n\u001b[0;32m     78\u001b[0m losses \u001b[39m=\u001b[39m []\n\u001b[0;32m     79\u001b[0m \u001b[39mfor\u001b[39;00m data \u001b[39min\u001b[39;00m train_loader:\n\u001b[1;32m---> 80\u001b[0m       loss \u001b[39m=\u001b[39m train(data)\n\u001b[0;32m     81\u001b[0m       losses\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy())\n\u001b[0;32m     82\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m:\u001b[39;00m\u001b[39m03d\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Loss: \u001b[39m\u001b[39m{\u001b[39;00mnp\u001b[39m.\u001b[39mmean(losses)\u001b[39m:\u001b[39;00m\u001b[39m.5f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[36], line 17\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(data_in)\u001b[0m\n\u001b[0;32m     14\u001b[0m loss \u001b[39m=\u001b[39m criterion(out, targets) \n\u001b[0;32m     16\u001b[0m loss\u001b[39m.\u001b[39mbackward()  \u001b[39m# Derive gradients.\u001b[39;00m\n\u001b[1;32m---> 17\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()  \u001b[39m# Update parameters based on gradients.\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\jespe\\anaconda3\\envs\\proj_env\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:69\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m instance\u001b[39m.\u001b[39m_step_count \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     68\u001b[0m wrapped \u001b[39m=\u001b[39m func\u001b[39m.\u001b[39m\u001b[39m__get__\u001b[39m(instance, \u001b[39mcls\u001b[39m)\n\u001b[1;32m---> 69\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\jespe\\anaconda3\\envs\\proj_env\\lib\\site-packages\\torch\\optim\\optimizer.py:280\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    277\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mfunc\u001b[39m}\u001b[39;00m\u001b[39m must return None or a tuple of (new_args, new_kwargs),\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    278\u001b[0m                                \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut got \u001b[39m\u001b[39m{\u001b[39;00mresult\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 280\u001b[0m out \u001b[39m=\u001b[39m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    281\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    283\u001b[0m \u001b[39m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jespe\\anaconda3\\envs\\proj_env\\lib\\site-packages\\torch\\optim\\optimizer.py:33\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     32\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefaults[\u001b[39m'\u001b[39m\u001b[39mdifferentiable\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m---> 33\u001b[0m     ret \u001b[39m=\u001b[39m func(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     34\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m     torch\u001b[39m.\u001b[39mset_grad_enabled(prev_grad)\n",
      "File \u001b[1;32mc:\\Users\\jespe\\anaconda3\\envs\\proj_env\\lib\\site-packages\\torch\\optim\\adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    130\u001b[0m     beta1, beta2 \u001b[39m=\u001b[39m group[\u001b[39m'\u001b[39m\u001b[39mbetas\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m    132\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_group(\n\u001b[0;32m    133\u001b[0m         group,\n\u001b[0;32m    134\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    138\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    139\u001b[0m         state_steps)\n\u001b[1;32m--> 141\u001b[0m     adam(\n\u001b[0;32m    142\u001b[0m         params_with_grad,\n\u001b[0;32m    143\u001b[0m         grads,\n\u001b[0;32m    144\u001b[0m         exp_avgs,\n\u001b[0;32m    145\u001b[0m         exp_avg_sqs,\n\u001b[0;32m    146\u001b[0m         max_exp_avg_sqs,\n\u001b[0;32m    147\u001b[0m         state_steps,\n\u001b[0;32m    148\u001b[0m         amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    149\u001b[0m         beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    150\u001b[0m         beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    151\u001b[0m         lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    152\u001b[0m         weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    153\u001b[0m         eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    154\u001b[0m         maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    155\u001b[0m         foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    156\u001b[0m         capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    157\u001b[0m         differentiable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mdifferentiable\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    158\u001b[0m         fused\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mfused\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    159\u001b[0m         grad_scale\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mgrad_scale\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m    160\u001b[0m         found_inf\u001b[39m=\u001b[39;49m\u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mfound_inf\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m    161\u001b[0m     )\n\u001b[0;32m    163\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\jespe\\anaconda3\\envs\\proj_env\\lib\\site-packages\\torch\\optim\\adam.py:281\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    278\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    279\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 281\u001b[0m func(params,\n\u001b[0;32m    282\u001b[0m      grads,\n\u001b[0;32m    283\u001b[0m      exp_avgs,\n\u001b[0;32m    284\u001b[0m      exp_avg_sqs,\n\u001b[0;32m    285\u001b[0m      max_exp_avg_sqs,\n\u001b[0;32m    286\u001b[0m      state_steps,\n\u001b[0;32m    287\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[0;32m    288\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    289\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    290\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[0;32m    291\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[0;32m    292\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[0;32m    293\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[0;32m    294\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable,\n\u001b[0;32m    295\u001b[0m      differentiable\u001b[39m=\u001b[39;49mdifferentiable,\n\u001b[0;32m    296\u001b[0m      grad_scale\u001b[39m=\u001b[39;49mgrad_scale,\n\u001b[0;32m    297\u001b[0m      found_inf\u001b[39m=\u001b[39;49mfound_inf)\n",
      "File \u001b[1;32mc:\\Users\\jespe\\anaconda3\\envs\\proj_env\\lib\\site-packages\\torch\\optim\\adam.py:449\u001b[0m, in \u001b[0;36m_multi_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    446\u001b[0m torch\u001b[39m.\u001b[39m_foreach_add_(device_exp_avgs, device_grads, alpha\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta1)\n\u001b[0;32m    448\u001b[0m torch\u001b[39m.\u001b[39m_foreach_mul_(device_exp_avg_sqs, beta2)\n\u001b[1;32m--> 449\u001b[0m torch\u001b[39m.\u001b[39;49m_foreach_addcmul_(device_exp_avg_sqs, device_grads, device_grads, \u001b[39m1\u001b[39;49m \u001b[39m-\u001b[39;49m beta2)\n\u001b[0;32m    451\u001b[0m \u001b[39mif\u001b[39;00m capturable:\n\u001b[0;32m    452\u001b[0m     \u001b[39m# TODO: use foreach_pow if/when foreach_pow is added\u001b[39;00m\n\u001b[0;32m    453\u001b[0m     bias_correction1 \u001b[39m=\u001b[39m [torch\u001b[39m.\u001b[39mpow(beta1, step) \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m device_state_steps]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from utility_functions import get_num_parameters\n",
    "\n",
    "feature_dim = train_data[0].x.shape[1]\n",
    "target_dim = train_data[0].y.shape[1]\n",
    "descriptor_dim = train_data[0].descriptors.shape[1]\n",
    "\n",
    "def train(data_in):\n",
    "      model.train()\n",
    "      optimizer.zero_grad()  # Clear gradients.\n",
    "      out = model(data_in.x, data_in.edge_index, data_in.edge_attr, data_in.batch, \n",
    "                  data_in.descriptors).to(device)  # Perform a single forward pass.\n",
    "\n",
    "      targets = data_in.y\n",
    "      loss = criterion(out, targets) \n",
    "\n",
    "      loss.backward()  # Derive gradients.\n",
    "      optimizer.step()  # Update parameters based on gradients.\n",
    "      return loss\n",
    "\n",
    "def test(data):\n",
    "      all_r2 = []\n",
    "      all_loss = []\n",
    "      counter = -1    \n",
    "      for data_in in data:\n",
    "            counter += 1\n",
    "            model.eval()\n",
    "            out = model(data_in.x, data_in.edge_index, data_in.edge_attr, data_in.batch,\n",
    "                        data_in.descriptors).cpu()\n",
    "            targets = data_in.y.cpu()\n",
    "            \n",
    "            # Caculate R2\n",
    "            r2_score_var = []\n",
    "            for item in range(target_dim):\n",
    "                  if item == 0:\n",
    "                        r2_score_var = r2_score(targets[:,item].detach().numpy(), out[:,item].detach().numpy())\n",
    "                  else:\n",
    "                        new_score = r2_score(targets[:,item].detach().numpy(), out[:,item].detach().numpy())\n",
    "                        r2_score_var = np.vstack((r2_score_var, new_score))\n",
    "\n",
    "            if counter == 0:\n",
    "                  all_r2 = r2_score_var\n",
    "            else:\n",
    "                  all_r2 = np.hstack((all_r2, r2_score_var))\n",
    "                  \n",
    "            loss = float(criterion(out, targets).detach().numpy())\n",
    "            all_loss = np.hstack((all_loss, loss))\n",
    "\n",
    "      average_test_r2 = np.mean(all_r2, axis=1)\n",
    "      average_test_loss = np.mean(all_loss)\n",
    "      \n",
    "      return average_test_r2, average_test_loss\n",
    "\n",
    "n_epochs = 20\n",
    "\n",
    "print()\n",
    "print(\"...Starting training...\")\n",
    "print(\"Device used:\", device)\n",
    "\n",
    "n_runs = 3\n",
    "for run in range(n_runs):\n",
    "      print(\"Run\", run+1)\n",
    "      \n",
    "      model = GNN_all(hidden_channels=128, feature_dim=feature_dim, target_dim=target_dim, \n",
    "                descriptor_dim=descriptor_dim).to(device) \n",
    "      optimizer = torch.optim.Adam(model.parameters(), lr=0.0005, weight_decay=5e-4)\n",
    "      criterion = torch.nn.MSELoss().to(device)\n",
    "\n",
    "      # Decay for learning rate\n",
    "      decayRate = 0.9\n",
    "      lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer=optimizer, gamma=decayRate)\n",
    "      \n",
    "      train_r2 = []\n",
    "      train_loss = []\n",
    "      test_r2 = []\n",
    "      test_loss = []\n",
    "\n",
    "      for epoch in np.arange(1, n_epochs+1):     \n",
    "            losses = []\n",
    "            for data in train_loader:\n",
    "                  loss = train(data)\n",
    "                  losses.append(loss.cpu().detach().numpy())\n",
    "            print(f'Epoch: {epoch:03d}, Loss: {np.mean(losses):.5f}')\n",
    "            lr_scheduler.step() # Decay to learning rate\n",
    "                  \n",
    "            if epoch == n_epochs:         # calculate results of training\n",
    "                  print(\"...Training done...\")\n",
    "                  print(\"...Calculating final results...\")\n",
    "                  r2_temp_train, loss_temp_train = test(train_loader) \n",
    "                  train_r2.append(r2_temp_train)\n",
    "                  train_loss.append(loss_temp_train)\n",
    "                  \n",
    "                  r2_temp_test, loss_temp_test = test(test_loader)\n",
    "                  test_r2.append(r2_temp_test)\n",
    "                  test_loss.append(loss_temp_test)\n",
    "\n",
    "                  # print(\"====================================================\")\n",
    "                  # print(\"Final training R2:\", train_r2[-1])\n",
    "                  # print(\"Average final training R2: \", np.mean(train_r2[-1]))\n",
    "                  # print(\"Final training loss:\", train_loss[-1])\n",
    "\n",
    "                  # print(\"Final test R2:\", test_r2[-1])\n",
    "                  # print(\"Average final test R2: \", np.mean(test_r2[-1]))\n",
    "                  # print(\"Final test loss:\", test_loss[-1])\n",
    "      train_r2_hist = np.array(train_r2) if run == 0 else np.vstack((train_r2_hist, np.array(train_r2)))\n",
    "      train_loss_hist = np.array(train_loss) if run == 0 else np.vstack((train_loss_hist, np.array(train_loss)))\n",
    "      test_r2_hist = np.array(test_r2) if run == 0 else np.vstack((test_r2_hist, np.array(test_r2)))\n",
    "      test_loss_hist = np.array(test_loss) if run == 0 else np.vstack((test_loss_hist, np.array(test_loss)))                              \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================\n",
      "Average final training R2:  [0.88114662 0.93164    0.94876117 0.74532656 0.98489218 0.91070654\n",
      " 0.97367664 0.95806713 0.96945231 0.9937125  0.99723402 0.997234\n",
      " 0.99723578 0.99722721 0.98869512]\n",
      "Average final training R2 (avg. over all runs and targets):  0.9516671857634194\n",
      "Average final training loss: 0.046037392058308264\n",
      "\n",
      "Average final test R2:  [0.88114662 0.93164    0.94876117 0.74532656 0.98489218 0.91070654\n",
      " 0.97367664 0.95806713 0.96945231 0.9937125  0.99723402 0.997234\n",
      " 0.99723578 0.99722721 0.98869512]\n",
      "Average final test R2 (avg. over all runs and targets):  0.9516671857634194\n",
      "Average final test loss: 0.05030429085943466\n"
     ]
    }
   ],
   "source": [
    "print(\"====================================================\")\n",
    "print(\"Average final training R2: \", np.mean(train_r2_hist, axis=0))\n",
    "print(\"Average final training R2 (avg. over all runs and targets): \", np.mean(train_r2_hist))\n",
    "print(\"Average final training loss:\", np.mean(train_loss_hist))\n",
    "print()\n",
    "# print(np.mean(test_r2_hist, axis=1))\n",
    "print(\"Average final test R2: \", np.mean(test_r2_hist, axis=0))\n",
    "print(\"Average final test R2 (avg. over all runs and targets): \", np.mean(test_r2_hist))\n",
    "print(\"Average final test loss:\", np.mean(test_loss_hist))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model for just one target"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GNN function"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GATConv: https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GATConv.html#torch_geometric.nn.conv.GATConv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GENConv: https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GENConv.html#torch_geometric.nn.conv.GENConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, feature_dim, target_dim, descriptor_dim, fingerprint_dim):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GATConv(feature_dim, hidden_channels)\n",
    "        self.conv1_norm = BatchNorm(hidden_channels)\n",
    "        self.conv2 = GATConv(hidden_channels, hidden_channels)\n",
    "        self.conv2_norm = BatchNorm(hidden_channels)\n",
    "        self.conv3 = GATConv(hidden_channels, hidden_channels)\n",
    "        self.conv3_norm = BatchNorm(hidden_channels)\n",
    "        self.conv4 = GATConv(hidden_channels, hidden_channels)\n",
    "        self.conv4_norm = BatchNorm(hidden_channels)\n",
    "        self.conv5 = GATConv(hidden_channels, hidden_channels)\n",
    "        self.conv5_norm = BatchNorm(hidden_channels)\n",
    "\n",
    "        self.input_norm = BatchNorm(hidden_channels+descriptor_dim+fingerprint_dim)        \n",
    "        self.lin1 = Linear(hidden_channels+descriptor_dim+fingerprint_dim, 512)\n",
    "        self.lin2 = Linear(512, 256)\n",
    "        self.lin3 = Linear(256, 128)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch, descriptors, fingerprints): \n",
    "        x = self.conv1(x, edge_index, edge_attr)\n",
    "        x = self.conv1_norm(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index, edge_attr)\n",
    "        x = self.conv2_norm(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3(x, edge_index, edge_attr)\n",
    "        x = self.conv3_norm(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv4(x, edge_index, edge_attr)\n",
    "        x = self.conv4_norm(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv5(x, edge_index, edge_attr)\n",
    "        x = self.conv5_norm(x)\n",
    "\n",
    "        #Returns batch-wise graph-level-outputs by averaging node features across the node dimension, so that for a single graph G\n",
    "        #its output is computed by\n",
    "        x = global_mean_pool(x, batch) \n",
    "        x2 = descriptors\n",
    "        x3 = fingerprints\n",
    "        x = torch.cat((x,x2,x3),1)\n",
    "        \n",
    "        x = self.input_norm(x)\n",
    "        x = self.lin1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        \n",
    "        x = self.lin2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.2, training=self.training)\n",
    "        \n",
    "        x = self.lin3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Loading data...\n",
      "Example train data target before scaling tensor([[ 2.9693e+00,  1.9777e+00,  1.8423e+00,  1.2093e+00,  8.4060e+01,\n",
      "         -1.8970e-01, -7.0400e-02,  1.1930e-01,  8.7339e+02,  1.5988e-01,\n",
      "         -3.4861e+02, -3.4860e+02, -3.4860e+02, -3.4864e+02,  2.9189e+01]])\n",
      "Example train data target after scaling: tensor([[-0.3350,  1.3300,  2.2841, -1.0264,  1.0990,  2.2975, -1.7445, -2.7966,\n",
      "         -1.1527,  0.3357,  1.6155,  1.6155,  1.6155,  1.6156, -0.6067]],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m...Loading data...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m train_data, test_data, scaler \u001b[39m=\u001b[39m scale_and_split_data(dataset, device, input_scaler\u001b[39m=\u001b[39;49mStandardScaler())\n\u001b[0;32m      3\u001b[0m train_loader, test_loader \u001b[39m=\u001b[39m create_data_loaders(train_data, test_data, batch_size\u001b[39m=\u001b[39m\u001b[39m64\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m...Data loading done...\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[10], line 58\u001b[0m, in \u001b[0;36mscale_and_split_data\u001b[1;34m(dataset, device, input_scaler)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[39mfor\u001b[39;00m index \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m,\u001b[39mlen\u001b[39m(test_data)):\n\u001b[0;32m     57\u001b[0m     test_data[index]\u001b[39m.\u001b[39mdescriptors \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mFloatTensor([test_data_desriptors[index,:]])\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m---> 58\u001b[0m     test_data[index]\u001b[39m.\u001b[39mfingerprints \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mFloatTensor([test_data_fingerprints[index,:]])\u001b[39m.\u001b[39;49mto(device)\n\u001b[0;32m     60\u001b[0m \u001b[39mreturn\u001b[39;00m train_data, test_data, input_scaler\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "feature_dim = train_data[0].x.shape[1]\n",
    "target_dim = 1\n",
    "descriptor_dim = train_data[0].descriptors.shape[1]\n",
    "fingerprint_dim = train_data[0].fingerprints.shape[1]\n",
    "\n",
    "def train(data_in, target):\n",
    "      model.train()\n",
    "      optimizer.zero_grad()  # Clear gradients.\n",
    "      out = model(data_in.x, data_in.edge_index, data_in.edge_attr, data_in.batch, \n",
    "                  data_in.descriptors, data_in.fingerprints).to(device)\n",
    "      targets = data_in.y[:,target].reshape(-1,1)\n",
    "      \n",
    "      loss = criterion(out, targets)   \n",
    "\n",
    "      loss.backward()  # Derive gradients.\n",
    "      optimizer.step()  # Update parameters based on gradients.\n",
    "      return loss\n",
    "\n",
    "def test(data, target):\n",
    "      all_r2 = []\n",
    "      all_loss = []\n",
    "      counter = -1    \n",
    "      for data_in in data:\n",
    "            counter += 1\n",
    "            model.eval()\n",
    "            out = model(data_in.x, data_in.edge_index, data_in.edge_attr, data_in.batch, \n",
    "                        data_in.descriptors, data_in.fingerprints).cpu()\n",
    "            targets = data_in.y[:,target].cpu().reshape(-1,1)\n",
    "            \n",
    "            # Caculate R2\n",
    "            r2_score_var = r2_score(targets.detach().numpy(), out.detach().numpy())\n",
    "            all_r2.append(r2_score_var)\n",
    "            \n",
    "            loss = float(criterion(out, targets).detach().numpy())\n",
    "            all_loss.append(loss)\n",
    "\n",
    "      average_test_r2 = np.mean(all_r2)\n",
    "      average_test_loss = np.mean(all_loss)\n",
    "\n",
    "      return average_test_r2, average_test_loss\n",
    "\n",
    "num_targets = train_data[0].y.shape[1]\n",
    "start_time = time.time()\n",
    "for target_index in range(num_targets):\n",
    "      print(\"Target index:\", target_index)\n",
    "\n",
    "      model = GNN(hidden_channels=64, feature_dim=feature_dim, target_dim=target_dim,\n",
    "                  descriptor_dim=descriptor_dim, fingerprint_dim=fingerprint_dim).to(device) \n",
    "      optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=5e-4)\n",
    "      criterion = torch.nn.MSELoss().to(device)\n",
    "\n",
    "      # Vectors to append accuracy to:\n",
    "      train_r2 = []\n",
    "      test_r2 = []\n",
    "      train_loss = []\n",
    "      test_loss = []\n",
    "\n",
    "      # Calculate accuracy and loss before training \n",
    "      r2_temp, loss_temp = test(train_loader, target_index)\n",
    "      train_r2.append(r2_temp)\n",
    "      train_loss.append(loss_temp)\n",
    "      r2_temp, loss_temp = test(test_loader, target_index)\n",
    "      test_r2.append(r2_temp)\n",
    "      test_loss.append(loss_temp)\n",
    "      \n",
    "      print(\"Initial training R2: \", train_r2[0])\n",
    "      print(\"Initial test R2: \", test_r2[0])\n",
    "\n",
    "      print_r2_option = True\n",
    "      counter = 0\n",
    "      for epoch in range(1, 21):\n",
    "            counter += 1\n",
    "            losses = []\n",
    "            for data in train_loader:\n",
    "                  loss = train(data, target_index)\n",
    "                  losses.append(loss.cpu().detach().numpy())\n",
    "            print(f'Epoch: {epoch:03d}, Loss: {np.mean(losses):.5f}')\n",
    "\n",
    "            if print_r2_option & epoch == 20:\n",
    "                  temp_train_r2, temp_train_loss = test(train_loader, target_index)\n",
    "                  train_r2.append(temp_train_r2)\n",
    "                  train_loss.append(temp_train_loss)\n",
    "\n",
    "                  temp_test_r2, temp_test_loss = test(test_loader, target_index)\n",
    "                  test_r2.append(temp_test_r2)\n",
    "                  test_loss.append(temp_test_loss)\n",
    "      print(f\"Best training R2 for target {target_index}: {np.max(train_r2)}\")\n",
    "      print(f\"Best test R2 for target {target_index}: {np.max(test_r2)}\")\n",
    "print(\"...Done...\")\n",
    "end_time = time.time()\n",
    "print(f\"Time taken: {(end_time - start_time)/60} minutes\")\n",
    "print(f\"Average time per target: {(end_time - start_time)/(num_targets*60)} minutes\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
